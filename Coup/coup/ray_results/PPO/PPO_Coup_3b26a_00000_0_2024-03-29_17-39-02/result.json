{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.36, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.32989690721649484, "random": -0.24, "player_2": 0.1553398058252427}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [22, 17, 13, 14, 11, 12, 30, 22, 14, 18, 13, 10, 34, 17, 11, 20, 10, 30, 27, 12, 16, 21, 14, 15, 30, 22, 14, 25, 7, 46, 12, 25, 17, 11, 23, 14, 21, 14, 30, 10, 16, 20, 20, 15, 19, 10, 16, 15, 8, 10, 28, 20, 9, 23, 12, 19, 15, 21, 9, 21, 10, 15, 17, 19, 21, 26, 21, 22, 15, 16, 26, 8, 22, 20, 15, 18, 16, 17, 31, 10, 19, 28, 11, 15, 24, 11, 14, 16, 13, 15, 13, 11, 17, 20, 11, 10, 27, 15, 15, 20, 17, 22, 21, 8, 25, 32, 10, 9, 16, 10, 10, 17, 8, 19, 19, 18, 18, 12, 10, 15, 20, 20, 13, 11, 25, 13, 15, 26, 29, 13, 37, 11, 10, 13, 11, 12, 9, 9, 38, 35, 25, 40, 14, 21, 13, 30, 10, 23, 12, 24, 24, 14, 10, 25, 16, 16, 17, 10, 13, 27, 8, 12, 37, 16, 24, 12, 9, 22, 9, 14, 9, 17, 19, 8, 31, 10, 15, 15, 12, 22, 10, 23, 10, 11, 21, 11, 17, 18, 18, 12, 12, 24, 26, 10, 16, 13, 12, 10, 20, 27], "policy_player_1_reward": [2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0], "policy_random_reward": [-2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6464997229185846, "mean_inference_ms": 1.0926984236430215, "mean_action_processing_ms": 0.16023699279036488, "mean_env_wait_ms": 0.11419980786795424, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014149520132276747, "ViewRequirementAgentConnector_ms": 0.25592284543173655}, "player_1_winrate": 0.5979381443298969, "player_2_winrate": 0.5339805825242718, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.7361251545449097, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.974573739618063, "policy_loss": -0.014735317416489124, "vf_loss": 1.9874095718065898, "vf_explained_var": 0.08879506861170133, "kl": 0.009497465695186978, "entropy": 0.7934868826220433, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.75, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9753701065100877, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.1098133966363237, "policy_loss": -0.035874137370938475, "vf_loss": 2.1433890074432154, "vf_explained_var": 0.02600227532287439, "kl": 0.011492592672770936, "entropy": 0.8337745144086726, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.52941176470588, "num_grad_updates_lifetime": 255.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.84263959390863, "episode_media": {}, "episodes_this_iter": 197, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.025380710659898477, "player_2": 0.025380710659898477}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [28, 23, 25, 25, 18, 8, 37, 15, 10, 31, 17, 16, 10, 9, 37, 11, 16, 14, 10, 26, 12, 15, 29, 12, 29, 19, 24, 18, 28, 26, 14, 21, 27, 17, 21, 24, 29, 22, 24, 15, 20, 17, 24, 22, 13, 20, 23, 21, 18, 19, 8, 31, 21, 22, 7, 11, 32, 18, 15, 29, 16, 16, 25, 8, 52, 25, 33, 19, 15, 25, 18, 12, 12, 16, 33, 15, 32, 22, 40, 11, 22, 19, 63, 30, 17, 10, 11, 28, 54, 39, 9, 33, 27, 34, 14, 9, 13, 10, 6, 14, 30, 38, 20, 9, 45, 15, 12, 12, 8, 20, 10, 17, 18, 17, 34, 10, 22, 12, 13, 26, 18, 8, 16, 26, 29, 9, 23, 13, 11, 10, 8, 19, 46, 11, 44, 9, 29, 11, 21, 9, 16, 23, 11, 19, 19, 19, 22, 9, 11, 23, 15, 22, 18, 36, 34, 24, 16, 15, 35, 14, 27, 30, 11, 55, 10, 10, 13, 19, 24, 26, 26, 15, 13, 9, 12, 9, 16, 10, 8, 27, 21, 11, 16, 20, 13, 16, 11, 20, 18, 15, 9, 26, 9, 19, 15, 24, 7], "policy_player_1_reward": [2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0], "policy_player_2_reward": [-2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6677105407714521, "mean_inference_ms": 2.0652534944002086, "mean_action_processing_ms": 0.19069583341471244, "mean_env_wait_ms": 0.1265866269346436, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011402489570191668, "ViewRequirementAgentConnector_ms": 0.23495143224053666}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.84263959390863, "episodes_this_iter": 197, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.025380710659898477, "player_2": 0.025380710659898477}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [28, 23, 25, 25, 18, 8, 37, 15, 10, 31, 17, 16, 10, 9, 37, 11, 16, 14, 10, 26, 12, 15, 29, 12, 29, 19, 24, 18, 28, 26, 14, 21, 27, 17, 21, 24, 29, 22, 24, 15, 20, 17, 24, 22, 13, 20, 23, 21, 18, 19, 8, 31, 21, 22, 7, 11, 32, 18, 15, 29, 16, 16, 25, 8, 52, 25, 33, 19, 15, 25, 18, 12, 12, 16, 33, 15, 32, 22, 40, 11, 22, 19, 63, 30, 17, 10, 11, 28, 54, 39, 9, 33, 27, 34, 14, 9, 13, 10, 6, 14, 30, 38, 20, 9, 45, 15, 12, 12, 8, 20, 10, 17, 18, 17, 34, 10, 22, 12, 13, 26, 18, 8, 16, 26, 29, 9, 23, 13, 11, 10, 8, 19, 46, 11, 44, 9, 29, 11, 21, 9, 16, 23, 11, 19, 19, 19, 22, 9, 11, 23, 15, 22, 18, 36, 34, 24, 16, 15, 35, 14, 27, 30, 11, 55, 10, 10, 13, 19, 24, 26, 26, 15, 13, 9, 12, 9, 16, 10, 8, 27, 21, 11, 16, 20, 13, 16, 11, 20, 18, 15, 9, 26, 9, 19, 15, 24, 7], "policy_player_1_reward": [2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0], "policy_player_2_reward": [-2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6677105407714521, "mean_inference_ms": 2.0652534944002086, "mean_action_processing_ms": 0.19069583341471244, "mean_env_wait_ms": 0.1265866269346436, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011402489570191668, "ViewRequirementAgentConnector_ms": 0.23495143224053666}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 228.87337172477717, "num_env_steps_trained_throughput_per_sec": 228.87337172477717, "timesteps_total": 4000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 3997, "timers": {"training_iteration_time_ms": 17476.913, "sample_time_ms": 4158.563, "learn_time_ms": 13307.98, "learn_throughput": 300.572, "synch_weights_time_ms": 9.363}, "counters": {"num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997}, "done": false, "episodes_total": 197, "training_iteration": 1, "trial_id": "3b26a_00000", "date": "2024-03-29_17-39-51", "timestamp": 1711733991, "time_this_iter_s": 22.36518144607544, "time_total_s": 22.36518144607544, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D1220E0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D1223B0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D121AB0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 22.36518144607544, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 13.23125, "ram_util_percent": 94.6}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.935, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.21818181818181817, "random": -0.29, "player_2": 0.37777777777777777}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [21, 28, 10, 32, 21, 28, 8, 17, 31, 24, 10, 33, 31, 29, 35, 17, 8, 12, 18, 12, 30, 15, 22, 17, 7, 45, 24, 11, 19, 11, 14, 16, 34, 18, 15, 13, 10, 21, 22, 25, 19, 18, 16, 26, 10, 25, 7, 10, 13, 25, 21, 28, 14, 27, 13, 14, 34, 14, 10, 7, 31, 15, 10, 12, 9, 11, 23, 8, 17, 13, 31, 23, 13, 23, 11, 12, 15, 19, 24, 28, 13, 18, 26, 20, 40, 40, 25, 16, 22, 7, 22, 11, 7, 11, 16, 14, 24, 26, 9, 14, 11, 27, 15, 18, 12, 9, 7, 17, 48, 18, 21, 40, 19, 14, 23, 16, 17, 33, 15, 22, 16, 7, 28, 26, 18, 19, 9, 19, 12, 25, 19, 13, 27, 13, 32, 10, 18, 14, 20, 27, 16, 17, 17, 8, 12, 21, 13, 40, 21, 19, 22, 15, 22, 27, 20, 10, 19, 15, 8, 11, 8, 13, 10, 9, 21, 16, 13, 18, 9, 16, 9, 13, 13, 23, 16, 23, 12, 19, 9, 8, 25, 12, 12, 15, 10, 10, 11, 16, 8, 17, 9, 16, 24, 17, 16, 17, 11, 13, 12, 17], "policy_player_1_reward": [-2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0], "policy_random_reward": [2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0], "policy_player_2_reward": [-1.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6548764932715155, "mean_inference_ms": 1.1066663162507953, "mean_action_processing_ms": 0.15893243661791373, "mean_env_wait_ms": 0.11318220850035143, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013820409774780273, "ViewRequirementAgentConnector_ms": 0.27664655447006226}, "player_1_winrate": 0.5454545454545454, "player_2_winrate": 0.6111111111111112, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.762714666376511, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7986724520723025, "policy_loss": -0.03352690781854714, "vf_loss": 1.8290231920778752, "vf_explained_var": 0.17589781222244102, "kl": 0.01588082636967491, "entropy": 0.7466138459742069, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.6875, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.951641078556285, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.99080576487616, "policy_loss": -0.011084037999092949, "vf_loss": 2.000028733996784, "vf_explained_var": 0.0993084802347071, "kl": 0.009305330125829136, "entropy": 0.7966636893503806, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.76470588235294, "num_grad_updates_lifetime": 765.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.732718894009217, "episode_media": {}, "episodes_this_iter": 217, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.018433179723502304, "player_2": 0.018433179723502304}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 12, 18, 11, 9, 7, 16, 20, 9, 19, 15, 21, 13, 9, 17, 13, 9, 12, 10, 13, 27, 24, 19, 11, 17, 54, 7, 10, 16, 32, 21, 7, 20, 18, 24, 17, 10, 27, 24, 8, 19, 7, 20, 13, 10, 11, 10, 10, 21, 19, 14, 14, 26, 18, 30, 11, 11, 13, 14, 14, 18, 13, 8, 10, 36, 20, 18, 25, 28, 14, 32, 16, 22, 29, 9, 10, 27, 13, 26, 35, 24, 37, 13, 16, 10, 34, 14, 20, 17, 35, 28, 13, 14, 12, 24, 13, 26, 14, 9, 15, 19, 18, 22, 12, 24, 20, 21, 33, 24, 15, 19, 13, 36, 22, 13, 31, 17, 20, 27, 7, 13, 15, 16, 17, 22, 12, 9, 19, 40, 18, 19, 28, 23, 14, 28, 37, 15, 26, 11, 7, 25, 16, 9, 10, 18, 16, 13, 10, 61, 10, 21, 18, 11, 19, 18, 22, 12, 11, 28, 17, 22, 17, 19, 21, 29, 33, 17, 21, 16, 13, 12, 23, 11, 13, 27, 10, 15, 17, 15, 22, 17, 24, 31, 28, 26, 11, 16, 19, 24, 30, 10, 21, 14, 31, 15, 33, 17, 10, 14, 26, 28, 24, 30, 8, 14, 40, 8, 26, 14, 23, 22, 22, 13, 17, 13, 20, 17], "policy_player_1_reward": [-1.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6562873168932928, "mean_inference_ms": 1.9627292686083022, "mean_action_processing_ms": 0.1832107677112873, "mean_env_wait_ms": 0.12263895523669668, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013348364060924899, "ViewRequirementAgentConnector_ms": 0.2174731223813949}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.732718894009217, "episodes_this_iter": 217, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.018433179723502304, "player_2": 0.018433179723502304}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 12, 18, 11, 9, 7, 16, 20, 9, 19, 15, 21, 13, 9, 17, 13, 9, 12, 10, 13, 27, 24, 19, 11, 17, 54, 7, 10, 16, 32, 21, 7, 20, 18, 24, 17, 10, 27, 24, 8, 19, 7, 20, 13, 10, 11, 10, 10, 21, 19, 14, 14, 26, 18, 30, 11, 11, 13, 14, 14, 18, 13, 8, 10, 36, 20, 18, 25, 28, 14, 32, 16, 22, 29, 9, 10, 27, 13, 26, 35, 24, 37, 13, 16, 10, 34, 14, 20, 17, 35, 28, 13, 14, 12, 24, 13, 26, 14, 9, 15, 19, 18, 22, 12, 24, 20, 21, 33, 24, 15, 19, 13, 36, 22, 13, 31, 17, 20, 27, 7, 13, 15, 16, 17, 22, 12, 9, 19, 40, 18, 19, 28, 23, 14, 28, 37, 15, 26, 11, 7, 25, 16, 9, 10, 18, 16, 13, 10, 61, 10, 21, 18, 11, 19, 18, 22, 12, 11, 28, 17, 22, 17, 19, 21, 29, 33, 17, 21, 16, 13, 12, 23, 11, 13, 27, 10, 15, 17, 15, 22, 17, 24, 31, 28, 26, 11, 16, 19, 24, 30, 10, 21, 14, 31, 15, 33, 17, 10, 14, 26, 28, 24, 30, 8, 14, 40, 8, 26, 14, 23, 22, 22, 13, 17, 13, 20, 17], "policy_player_1_reward": [-1.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6562873168932928, "mean_inference_ms": 1.9627292686083022, "mean_action_processing_ms": 0.1832107677112873, "mean_env_wait_ms": 0.12263895523669668, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013348364060924899, "ViewRequirementAgentConnector_ms": 0.2174731223813949}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 228.87679378530495, "num_env_steps_trained_throughput_per_sec": 228.87679378530495, "timesteps_total": 8000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 7997, "timers": {"training_iteration_time_ms": 17476.782, "sample_time_ms": 3987.97, "learn_time_ms": 13478.618, "learn_throughput": 296.766, "synch_weights_time_ms": 9.194}, "counters": {"num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997}, "done": false, "episodes_total": 414, "training_iteration": 2, "trial_id": "3b26a_00000", "date": "2024-03-29_17-40-14", "timestamp": 1711734014, "time_this_iter_s": 22.597036123275757, "time_total_s": 44.962217569351196, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D122CB0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D122D40>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D121870>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 44.962217569351196, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 12.365625000000001, "ram_util_percent": 95.57187499999999}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.085, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.545, "player_2": 0.5930232558139535, "player_1": 0.5087719298245614}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [24, 19, 24, 28, 10, 28, 12, 16, 22, 14, 8, 38, 8, 22, 31, 14, 23, 19, 14, 7, 29, 7, 23, 8, 12, 29, 18, 18, 16, 14, 9, 27, 14, 19, 15, 6, 13, 14, 13, 7, 19, 35, 20, 14, 19, 15, 10, 12, 28, 18, 6, 7, 12, 29, 8, 18, 45, 28, 19, 8, 18, 9, 20, 25, 33, 9, 15, 8, 14, 23, 11, 27, 12, 17, 12, 13, 9, 7, 21, 24, 23, 21, 13, 9, 27, 12, 33, 9, 34, 20, 11, 9, 9, 33, 16, 10, 22, 16, 25, 18, 9, 19, 7, 24, 14, 11, 7, 16, 13, 20, 23, 22, 10, 24, 24, 16, 19, 8, 21, 16, 10, 27, 19, 16, 19, 33, 22, 16, 18, 43, 16, 12, 10, 18, 11, 8, 15, 20, 22, 16, 14, 12, 8, 19, 15, 23, 24, 14, 11, 11, 17, 17, 17, 16, 38, 15, 19, 18, 9, 8, 10, 17, 10, 13, 9, 18, 33, 21, 13, 12, 23, 14, 21, 10, 8, 13, 13, 13, 11, 10, 9, 8, 22, 20, 22, 19, 21, 36, 35, 23, 9, 19, 13, 16, 6, 20, 12, 10, 18, 9], "policy_random_reward": [1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0], "policy_player_2_reward": [-1.0, -1.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, 2.0, 1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -1.0], "policy_player_1_reward": [-1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6383361492778539, "mean_inference_ms": 1.043080478886404, "mean_action_processing_ms": 0.1508075759064557, "mean_env_wait_ms": 0.10965148307419659, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012316644191741943, "ViewRequirementAgentConnector_ms": 0.24639463424682617}, "player_1_winrate": 0.6403508771929824, "player_2_winrate": 0.6511627906976745, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.952588956058025, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.90467679699262, "policy_loss": -0.02176500531495549, "vf_loss": 1.9243890079359214, "vf_explained_var": 0.1518350925296545, "kl": 0.010263954492199377, "entropy": 0.7051980680475632, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.1875, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.659515514794518, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.9503621519780627, "policy_loss": -0.018282245537813973, "vf_loss": 1.9664004646095574, "vf_explained_var": 0.12537961251595442, "kl": 0.011219674171862216, "entropy": 0.7324959914473926, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.29411764705883, "num_grad_updates_lifetime": 1275.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 11998, "num_agent_steps_trained": 11998}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.42608695652174, "episode_media": {}, "episodes_this_iter": 230, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.09130434782608696, "player_2": 0.09130434782608696}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [14, 23, 14, 14, 22, 17, 17, 33, 11, 14, 17, 15, 11, 13, 20, 13, 13, 10, 23, 17, 10, 37, 23, 27, 14, 17, 20, 9, 13, 8, 16, 17, 35, 13, 13, 10, 20, 17, 14, 27, 19, 12, 15, 22, 8, 41, 22, 13, 14, 11, 7, 16, 32, 16, 33, 27, 13, 10, 23, 19, 12, 15, 11, 6, 25, 36, 13, 20, 13, 9, 33, 8, 19, 8, 10, 9, 11, 7, 26, 18, 16, 19, 36, 10, 17, 17, 23, 41, 10, 20, 10, 13, 13, 35, 17, 15, 16, 11, 22, 19, 24, 20, 8, 21, 15, 19, 8, 13, 16, 10, 14, 17, 13, 14, 16, 9, 17, 13, 8, 13, 13, 20, 17, 21, 11, 21, 16, 18, 30, 19, 23, 17, 23, 30, 23, 10, 13, 19, 30, 16, 15, 10, 15, 28, 23, 7, 22, 19, 36, 13, 17, 18, 25, 28, 16, 29, 31, 16, 16, 10, 17, 21, 7, 16, 23, 10, 18, 15, 39, 26, 27, 13, 12, 11, 12, 10, 23, 24, 24, 10, 13, 10, 14, 17, 16, 27, 12, 13, 31, 14, 13, 15, 28, 17, 14, 15, 25, 12, 15, 25, 25, 19, 10, 18, 16, 24, 22, 16, 16, 20, 15, 17, 12, 10, 17, 12, 13, 14, 16, 27, 32, 13, 10, 7, 14, 15, 20, 9, 8, 12], "policy_player_1_reward": [2.0, -1.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0], "policy_player_2_reward": [-2.0, 1.0, -2.0, -1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6497850184340579, "mean_inference_ms": 1.9326548599936197, "mean_action_processing_ms": 0.1833449997514673, "mean_env_wait_ms": 0.12283872262880413, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01075837923132855, "ViewRequirementAgentConnector_ms": 0.2193053390668786}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.42608695652174, "episodes_this_iter": 230, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.09130434782608696, "player_2": 0.09130434782608696}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [14, 23, 14, 14, 22, 17, 17, 33, 11, 14, 17, 15, 11, 13, 20, 13, 13, 10, 23, 17, 10, 37, 23, 27, 14, 17, 20, 9, 13, 8, 16, 17, 35, 13, 13, 10, 20, 17, 14, 27, 19, 12, 15, 22, 8, 41, 22, 13, 14, 11, 7, 16, 32, 16, 33, 27, 13, 10, 23, 19, 12, 15, 11, 6, 25, 36, 13, 20, 13, 9, 33, 8, 19, 8, 10, 9, 11, 7, 26, 18, 16, 19, 36, 10, 17, 17, 23, 41, 10, 20, 10, 13, 13, 35, 17, 15, 16, 11, 22, 19, 24, 20, 8, 21, 15, 19, 8, 13, 16, 10, 14, 17, 13, 14, 16, 9, 17, 13, 8, 13, 13, 20, 17, 21, 11, 21, 16, 18, 30, 19, 23, 17, 23, 30, 23, 10, 13, 19, 30, 16, 15, 10, 15, 28, 23, 7, 22, 19, 36, 13, 17, 18, 25, 28, 16, 29, 31, 16, 16, 10, 17, 21, 7, 16, 23, 10, 18, 15, 39, 26, 27, 13, 12, 11, 12, 10, 23, 24, 24, 10, 13, 10, 14, 17, 16, 27, 12, 13, 31, 14, 13, 15, 28, 17, 14, 15, 25, 12, 15, 25, 25, 19, 10, 18, 16, 24, 22, 16, 16, 20, 15, 17, 12, 10, 17, 12, 13, 14, 16, 27, 32, 13, 10, 7, 14, 15, 20, 9, 8, 12], "policy_player_1_reward": [2.0, -1.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0], "policy_player_2_reward": [-2.0, 1.0, -2.0, -1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6497850184340579, "mean_inference_ms": 1.9326548599936197, "mean_action_processing_ms": 0.1833449997514673, "mean_env_wait_ms": 0.12283872262880413, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01075837923132855, "ViewRequirementAgentConnector_ms": 0.2193053390668786}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 11998, "num_agent_steps_trained": 11998, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 224.48825278681576, "num_env_steps_trained_throughput_per_sec": 224.48825278681576, "timesteps_total": 12000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 11998, "timers": {"training_iteration_time_ms": 17590.623, "sample_time_ms": 3935.383, "learn_time_ms": 13645.303, "learn_throughput": 293.141, "synch_weights_time_ms": 8.939}, "counters": {"num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 11998, "num_agent_steps_trained": 11998}, "done": false, "episodes_total": 644, "training_iteration": 3, "trial_id": "3b26a_00000", "date": "2024-03-29_17-40-36", "timestamp": 1711734036, "time_this_iter_s": 22.112633228302002, "time_total_s": 67.0748507976532, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D05ACB0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D0AB400>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D123490>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 67.0748507976532, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 10.596774193548388, "ram_util_percent": 95.76774193548387}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.81, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.6770833333333334, "random": -0.675, "player_2": 0.6730769230769231}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [10, 22, 13, 10, 16, 20, 34, 10, 17, 18, 17, 14, 20, 12, 10, 9, 16, 22, 14, 12, 11, 17, 13, 24, 17, 32, 13, 22, 7, 13, 13, 14, 15, 10, 24, 11, 15, 22, 24, 20, 22, 9, 14, 10, 7, 12, 11, 17, 18, 21, 9, 35, 15, 16, 11, 9, 23, 11, 21, 17, 7, 11, 17, 10, 11, 25, 37, 10, 14, 8, 11, 15, 15, 22, 20, 15, 10, 17, 16, 22, 12, 9, 19, 22, 10, 21, 15, 23, 13, 19, 15, 9, 8, 24, 15, 17, 8, 15, 18, 11, 16, 20, 13, 7, 18, 19, 15, 13, 13, 14, 23, 12, 15, 17, 10, 28, 18, 18, 16, 14, 24, 19, 30, 13, 18, 10, 8, 15, 14, 8, 10, 8, 13, 39, 15, 12, 11, 23, 13, 34, 14, 21, 31, 14, 13, 13, 18, 14, 24, 19, 13, 10, 29, 10, 17, 9, 8, 28, 6, 14, 8, 32, 15, 22, 13, 16, 16, 25, 21, 29, 20, 9, 27, 9, 19, 6, 12, 7, 18, 6, 10, 9, 16, 10, 7, 33, 22, 15, 12, 19, 10, 7, 9, 15, 9, 17, 22, 17, 8, 7], "policy_player_1_reward": [2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0], "policy_random_reward": [-2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6404165427693705, "mean_inference_ms": 1.028671024860989, "mean_action_processing_ms": 0.1518554827343305, "mean_env_wait_ms": 0.10603577035828664, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01209568977355957, "ViewRequirementAgentConnector_ms": 0.2617218494415283}, "player_1_winrate": 0.7083333333333334, "player_2_winrate": 0.7115384615384616, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.097480866064628, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.817334058135748, "policy_loss": -0.022685950108765004, "vf_loss": 1.8372990806897482, "vf_explained_var": 0.14512369533379874, "kl": 0.013604649597922909, "entropy": 0.6427764983847737, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.9375, "num_grad_updates_lifetime": 1680.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.5956136525845994, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8323943411602694, "policy_loss": -0.012420976999234043, "vf_loss": 1.8429180745984994, "vf_explained_var": 0.14053978978418835, "kl": 0.009486218659591249, "entropy": 0.6902777641427283, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.41176470588235, "num_grad_updates_lifetime": 1785.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.95299145299145, "episode_media": {}, "episodes_this_iter": 234, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.1794871794871795, "player_2": 0.1794871794871795}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [24, 12, 21, 14, 12, 9, 11, 9, 15, 17, 8, 14, 28, 15, 8, 14, 14, 18, 24, 22, 19, 13, 16, 11, 17, 14, 8, 19, 16, 8, 21, 14, 15, 24, 26, 18, 16, 13, 29, 11, 22, 17, 13, 11, 13, 28, 31, 10, 22, 14, 18, 10, 17, 18, 25, 27, 25, 7, 13, 9, 13, 7, 10, 8, 12, 46, 8, 18, 9, 8, 10, 24, 8, 26, 10, 11, 28, 28, 25, 17, 12, 13, 17, 21, 16, 20, 13, 21, 22, 33, 14, 19, 16, 13, 10, 22, 11, 17, 8, 22, 19, 16, 14, 13, 19, 10, 7, 21, 31, 19, 14, 19, 22, 20, 33, 10, 15, 15, 12, 11, 16, 9, 21, 22, 14, 28, 34, 17, 9, 15, 31, 22, 19, 34, 13, 9, 23, 11, 22, 24, 25, 23, 13, 9, 15, 16, 21, 20, 26, 13, 11, 11, 24, 10, 10, 24, 19, 13, 20, 13, 7, 14, 19, 23, 11, 21, 21, 16, 20, 13, 12, 6, 13, 14, 21, 26, 11, 39, 14, 11, 12, 6, 13, 18, 15, 11, 14, 13, 10, 11, 13, 25, 24, 10, 23, 33, 24, 22, 17, 9, 23, 12, 33, 17, 11, 9, 22, 19, 11, 16, 13, 34, 13, 19, 13, 9, 29, 9, 7, 32, 17, 15, 15, 13, 18, 15, 16, 19, 12, 27, 15, 21, 20, 16], "policy_player_1_reward": [1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0], "policy_player_2_reward": [-1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6322986556994723, "mean_inference_ms": 1.8640827042035024, "mean_action_processing_ms": 0.17748977005385677, "mean_env_wait_ms": 0.12038875703165935, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012857893593290932, "ViewRequirementAgentConnector_ms": 0.19045874603793153}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.95299145299145, "episodes_this_iter": 234, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.1794871794871795, "player_2": 0.1794871794871795}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [24, 12, 21, 14, 12, 9, 11, 9, 15, 17, 8, 14, 28, 15, 8, 14, 14, 18, 24, 22, 19, 13, 16, 11, 17, 14, 8, 19, 16, 8, 21, 14, 15, 24, 26, 18, 16, 13, 29, 11, 22, 17, 13, 11, 13, 28, 31, 10, 22, 14, 18, 10, 17, 18, 25, 27, 25, 7, 13, 9, 13, 7, 10, 8, 12, 46, 8, 18, 9, 8, 10, 24, 8, 26, 10, 11, 28, 28, 25, 17, 12, 13, 17, 21, 16, 20, 13, 21, 22, 33, 14, 19, 16, 13, 10, 22, 11, 17, 8, 22, 19, 16, 14, 13, 19, 10, 7, 21, 31, 19, 14, 19, 22, 20, 33, 10, 15, 15, 12, 11, 16, 9, 21, 22, 14, 28, 34, 17, 9, 15, 31, 22, 19, 34, 13, 9, 23, 11, 22, 24, 25, 23, 13, 9, 15, 16, 21, 20, 26, 13, 11, 11, 24, 10, 10, 24, 19, 13, 20, 13, 7, 14, 19, 23, 11, 21, 21, 16, 20, 13, 12, 6, 13, 14, 21, 26, 11, 39, 14, 11, 12, 6, 13, 18, 15, 11, 14, 13, 10, 11, 13, 25, 24, 10, 23, 33, 24, 22, 17, 9, 23, 12, 33, 17, 11, 9, 22, 19, 11, 16, 13, 34, 13, 19, 13, 9, 29, 9, 7, 32, 17, 15, 15, 13, 18, 15, 16, 19, 12, 27, 15, 21, 20, 16], "policy_player_1_reward": [1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0], "policy_player_2_reward": [-1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6322986556994723, "mean_inference_ms": 1.8640827042035024, "mean_action_processing_ms": 0.17748977005385677, "mean_env_wait_ms": 0.12038875703165935, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012857893593290932, "ViewRequirementAgentConnector_ms": 0.19045874603793153}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 243.40739620491885, "num_env_steps_trained_throughput_per_sec": 243.40739620491885, "timesteps_total": 16000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 15997, "timers": {"training_iteration_time_ms": 17301.306, "sample_time_ms": 3804.063, "learn_time_ms": 13487.05, "learn_throughput": 296.581, "synch_weights_time_ms": 9.187}, "counters": {"num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997}, "done": false, "episodes_total": 878, "training_iteration": 4, "trial_id": "3b26a_00000", "date": "2024-03-29_17-40-57", "timestamp": 1711734057, "time_this_iter_s": 20.679561853408813, "time_total_s": 87.75441265106201, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25D5A0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25D480>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D121CF0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 87.75441265106201, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 11.762068965517244, "ram_util_percent": 95.24827586206898}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.925, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.59, "player_2": 0.6346153846153846, "player_1": 0.5416666666666666}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [22, 10, 21, 15, 19, 19, 11, 22, 10, 15, 13, 16, 19, 15, 16, 7, 15, 18, 21, 10, 12, 13, 13, 8, 9, 10, 22, 20, 26, 11, 15, 13, 17, 16, 15, 13, 15, 9, 9, 14, 14, 17, 12, 7, 21, 14, 16, 19, 12, 30, 23, 20, 7, 13, 16, 20, 19, 16, 19, 19, 22, 13, 17, 10, 19, 18, 48, 14, 18, 17, 15, 26, 19, 8, 12, 22, 15, 9, 13, 11, 15, 16, 19, 9, 14, 13, 19, 14, 26, 18, 22, 14, 10, 10, 8, 19, 26, 22, 40, 18, 22, 15, 17, 6, 14, 13, 9, 9, 12, 8, 10, 9, 18, 21, 19, 22, 15, 19, 14, 7, 21, 14, 8, 10, 8, 8, 11, 19, 14, 8, 18, 17, 37, 17, 8, 12, 16, 15, 15, 15, 15, 19, 20, 18, 25, 8, 16, 24, 15, 17, 23, 12, 22, 8, 17, 16, 18, 14, 21, 16, 13, 17, 10, 20, 9, 12, 14, 15, 12, 21, 14, 13, 23, 15, 17, 19, 9, 13, 15, 10, 11, 13, 8, 9, 9, 28, 21, 15, 20, 14, 22, 19, 9, 23, 24, 21, 29, 20, 13, 26], "policy_random_reward": [1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, -1.0], "policy_player_2_reward": [-1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, 1.0], "policy_player_1_reward": [2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6354452932442984, "mean_inference_ms": 1.0059300293024407, "mean_action_processing_ms": 0.1483145055725378, "mean_env_wait_ms": 0.10290677077384405, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012524247169494629, "ViewRequirementAgentConnector_ms": 0.250102698802948}, "player_1_winrate": 0.6666666666666666, "player_2_winrate": 0.6826923076923077, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.0385222507019836, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.93849208081762, "policy_loss": -0.01874454712087754, "vf_loss": 1.9558387940128645, "vf_explained_var": 0.16403684330483279, "kl": 0.006989186873365294, "entropy": 0.606913676050802, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.5, "num_grad_updates_lifetime": 2160.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.093467719414655, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.9293910360803792, "policy_loss": -0.016579554208974334, "vf_loss": 1.9442188624073478, "vf_explained_var": 0.16700268679974126, "kl": 0.008758639822574939, "entropy": 0.6233049147853664, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.94117647058823, "num_grad_updates_lifetime": 2295.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.649122807017545, "episode_media": {}, "episodes_this_iter": 228, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.039473684210526314, "player_2": -0.039473684210526314}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 11, 16, 33, 39, 9, 13, 16, 11, 24, 23, 6, 7, 7, 30, 17, 13, 14, 14, 9, 22, 14, 10, 12, 27, 10, 29, 32, 13, 25, 14, 23, 14, 30, 33, 37, 23, 14, 17, 22, 11, 21, 16, 18, 17, 17, 13, 22, 19, 38, 27, 16, 17, 16, 13, 23, 9, 22, 11, 25, 14, 10, 19, 19, 22, 28, 24, 10, 25, 16, 15, 25, 13, 10, 9, 8, 8, 23, 13, 17, 12, 22, 18, 17, 18, 19, 13, 20, 22, 17, 34, 9, 10, 19, 20, 22, 25, 15, 12, 27, 19, 16, 13, 18, 16, 21, 15, 19, 28, 15, 14, 27, 15, 16, 15, 25, 22, 18, 26, 25, 19, 22, 16, 11, 20, 10, 11, 19, 36, 17, 16, 18, 12, 15, 18, 13, 8, 18, 18, 25, 25, 16, 9, 10, 16, 16, 20, 37, 22, 14, 10, 9, 13, 29, 21, 15, 13, 16, 8, 13, 18, 9, 10, 17, 12, 21, 16, 8, 13, 19, 8, 21, 20, 16, 8, 9, 17, 14, 18, 13, 48, 16, 19, 14, 19, 15, 18, 25, 11, 16, 10, 21, 12, 20, 10, 8, 17, 7, 24, 12, 20, 32, 10, 20, 14, 22, 13, 16, 25, 31, 41, 15, 23, 14, 10, 15, 23, 23, 22, 10, 9, 26, 18, 16, 12, 8, 15, 11], "policy_player_1_reward": [-1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6379906350645894, "mean_inference_ms": 1.906519633227607, "mean_action_processing_ms": 0.1778612212301145, "mean_env_wait_ms": 0.11815199608158289, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013927984655949107, "ViewRequirementAgentConnector_ms": 0.22268169804623253}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.649122807017545, "episodes_this_iter": 228, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.039473684210526314, "player_2": -0.039473684210526314}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 11, 16, 33, 39, 9, 13, 16, 11, 24, 23, 6, 7, 7, 30, 17, 13, 14, 14, 9, 22, 14, 10, 12, 27, 10, 29, 32, 13, 25, 14, 23, 14, 30, 33, 37, 23, 14, 17, 22, 11, 21, 16, 18, 17, 17, 13, 22, 19, 38, 27, 16, 17, 16, 13, 23, 9, 22, 11, 25, 14, 10, 19, 19, 22, 28, 24, 10, 25, 16, 15, 25, 13, 10, 9, 8, 8, 23, 13, 17, 12, 22, 18, 17, 18, 19, 13, 20, 22, 17, 34, 9, 10, 19, 20, 22, 25, 15, 12, 27, 19, 16, 13, 18, 16, 21, 15, 19, 28, 15, 14, 27, 15, 16, 15, 25, 22, 18, 26, 25, 19, 22, 16, 11, 20, 10, 11, 19, 36, 17, 16, 18, 12, 15, 18, 13, 8, 18, 18, 25, 25, 16, 9, 10, 16, 16, 20, 37, 22, 14, 10, 9, 13, 29, 21, 15, 13, 16, 8, 13, 18, 9, 10, 17, 12, 21, 16, 8, 13, 19, 8, 21, 20, 16, 8, 9, 17, 14, 18, 13, 48, 16, 19, 14, 19, 15, 18, 25, 11, 16, 10, 21, 12, 20, 10, 8, 17, 7, 24, 12, 20, 32, 10, 20, 14, 22, 13, 16, 25, 31, 41, 15, 23, 14, 10, 15, 23, 23, 22, 10, 9, 26, 18, 16, 12, 8, 15, 11], "policy_player_1_reward": [-1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6379906350645894, "mean_inference_ms": 1.906519633227607, "mean_action_processing_ms": 0.1778612212301145, "mean_env_wait_ms": 0.11815199608158289, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013927984655949107, "ViewRequirementAgentConnector_ms": 0.22268169804623253}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 232.94709858522802, "num_env_steps_trained_throughput_per_sec": 232.94709858522802, "timesteps_total": 20000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 19997, "timers": {"training_iteration_time_ms": 17275.301, "sample_time_ms": 3864.133, "learn_time_ms": 13401.419, "learn_throughput": 298.476, "synch_weights_time_ms": 8.642}, "counters": {"num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997}, "done": false, "episodes_total": 1106, "training_iteration": 5, "trial_id": "3b26a_00000", "date": "2024-03-29_17-41-18", "timestamp": 1711734078, "time_this_iter_s": 20.970324993133545, "time_total_s": 108.72473764419556, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25E3B0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25E560>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D122B00>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 108.72473764419556, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 10.620689655172416, "ram_util_percent": 94.82758620689656}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.375, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.815, "player_2": 0.8585858585858586, "player_1": 0.7722772277227723}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [29, 19, 26, 16, 21, 18, 15, 10, 12, 8, 15, 17, 15, 26, 29, 15, 18, 7, 12, 12, 35, 35, 21, 21, 22, 14, 9, 17, 15, 22, 20, 7, 19, 8, 19, 13, 15, 7, 12, 14, 21, 12, 15, 18, 17, 11, 18, 30, 23, 14, 10, 16, 27, 14, 23, 7, 25, 12, 19, 12, 7, 19, 12, 26, 26, 12, 22, 16, 15, 7, 19, 26, 32, 12, 10, 17, 9, 12, 14, 21, 17, 17, 10, 25, 26, 26, 10, 9, 10, 12, 15, 19, 20, 11, 15, 19, 17, 17, 16, 11, 17, 18, 11, 34, 19, 25, 7, 9, 20, 26, 31, 35, 10, 9, 34, 15, 7, 24, 23, 18, 8, 12, 25, 7, 15, 10, 34, 12, 14, 23, 32, 11, 13, 10, 8, 20, 8, 8, 11, 13, 18, 18, 19, 33, 7, 27, 7, 20, 27, 28, 27, 18, 25, 7, 22, 12, 17, 10, 17, 17, 13, 10, 28, 9, 8, 24, 9, 40, 10, 11, 12, 12, 15, 38, 36, 18, 24, 11, 16, 20, 18, 21, 14, 12, 12, 22, 17, 23, 13, 16, 9, 31, 29, 12, 17, 8, 25, 15, 19, 31], "policy_random_reward": [-1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0], "policy_player_2_reward": [1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0], "policy_player_1_reward": [-1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6292952755822434, "mean_inference_ms": 0.9952268399902576, "mean_action_processing_ms": 0.14888234833265693, "mean_env_wait_ms": 0.10046978642894128, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013601183891296387, "ViewRequirementAgentConnector_ms": 0.2310914397239685}, "player_1_winrate": 0.7227722772277227, "player_2_winrate": 0.7575757575757576, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.2640470589200654, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8811128231386343, "policy_loss": -0.016996504364457602, "vf_loss": 1.8969294441243012, "vf_explained_var": 0.1453007907917102, "kl": 0.0058994723264255465, "entropy": 0.5607533206542333, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.375, "num_grad_updates_lifetime": 2640.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.068658279905132, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8942729199633879, "policy_loss": -0.01840675449108376, "vf_loss": 1.9111326292449353, "vf_explained_var": 0.13901598511957655, "kl": 0.007735210300023518, "entropy": 0.5663406109693003, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.05882352941177, "num_grad_updates_lifetime": 2805.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.95964125560538, "episode_media": {}, "episodes_this_iter": 223, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.13452914798206278, "player_2": 0.13452914798206278}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 18, 22, 10, 16, 13, 20, 8, 15, 29, 12, 19, 7, 13, 23, 14, 9, 21, 13, 17, 23, 22, 17, 16, 12, 25, 16, 27, 21, 11, 9, 36, 28, 27, 22, 16, 34, 20, 30, 11, 16, 13, 22, 19, 24, 21, 19, 32, 15, 22, 18, 40, 18, 37, 16, 10, 27, 22, 13, 19, 13, 21, 19, 10, 14, 17, 17, 18, 12, 9, 15, 16, 19, 16, 13, 8, 12, 18, 9, 15, 16, 19, 16, 22, 19, 22, 15, 16, 19, 21, 19, 26, 13, 13, 20, 19, 60, 20, 13, 32, 19, 13, 13, 13, 26, 17, 37, 38, 17, 16, 23, 8, 10, 16, 14, 15, 13, 17, 7, 24, 16, 14, 15, 14, 40, 13, 22, 17, 10, 7, 21, 11, 12, 19, 20, 10, 25, 10, 22, 11, 9, 16, 24, 13, 16, 15, 15, 10, 25, 12, 22, 14, 11, 16, 12, 32, 19, 11, 32, 15, 34, 23, 9, 28, 13, 16, 19, 13, 27, 24, 13, 14, 12, 19, 21, 15, 21, 16, 20, 27, 15, 13, 22, 12, 23, 17, 23, 22, 19, 17, 16, 31, 17, 10, 13, 21, 15, 11, 17, 20, 20, 12, 10, 13, 24, 21, 14, 7, 18, 13, 10, 16, 17, 13, 16, 16, 8, 15, 24, 14, 31, 15, 19], "policy_player_1_reward": [-2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0], "policy_player_2_reward": [2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6327063294941033, "mean_inference_ms": 1.870357931908473, "mean_action_processing_ms": 0.17663293193622476, "mean_env_wait_ms": 0.11813791890771062, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0120651027012299, "ViewRequirementAgentConnector_ms": 0.21012096661623283}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.95964125560538, "episodes_this_iter": 223, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.13452914798206278, "player_2": 0.13452914798206278}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 18, 22, 10, 16, 13, 20, 8, 15, 29, 12, 19, 7, 13, 23, 14, 9, 21, 13, 17, 23, 22, 17, 16, 12, 25, 16, 27, 21, 11, 9, 36, 28, 27, 22, 16, 34, 20, 30, 11, 16, 13, 22, 19, 24, 21, 19, 32, 15, 22, 18, 40, 18, 37, 16, 10, 27, 22, 13, 19, 13, 21, 19, 10, 14, 17, 17, 18, 12, 9, 15, 16, 19, 16, 13, 8, 12, 18, 9, 15, 16, 19, 16, 22, 19, 22, 15, 16, 19, 21, 19, 26, 13, 13, 20, 19, 60, 20, 13, 32, 19, 13, 13, 13, 26, 17, 37, 38, 17, 16, 23, 8, 10, 16, 14, 15, 13, 17, 7, 24, 16, 14, 15, 14, 40, 13, 22, 17, 10, 7, 21, 11, 12, 19, 20, 10, 25, 10, 22, 11, 9, 16, 24, 13, 16, 15, 15, 10, 25, 12, 22, 14, 11, 16, 12, 32, 19, 11, 32, 15, 34, 23, 9, 28, 13, 16, 19, 13, 27, 24, 13, 14, 12, 19, 21, 15, 21, 16, 20, 27, 15, 13, 22, 12, 23, 17, 23, 22, 19, 17, 16, 31, 17, 10, 13, 21, 15, 11, 17, 20, 20, 12, 10, 13, 24, 21, 14, 7, 18, 13, 10, 16, 17, 13, 16, 16, 8, 15, 24, 14, 31, 15, 19], "policy_player_1_reward": [-2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0], "policy_player_2_reward": [2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6327063294941033, "mean_inference_ms": 1.870357931908473, "mean_action_processing_ms": 0.17663293193622476, "mean_env_wait_ms": 0.11813791890771062, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0120651027012299, "ViewRequirementAgentConnector_ms": 0.21012096661623283}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 242.09922098937002, "num_env_steps_trained_throughput_per_sec": 242.09922098937002, "timesteps_total": 24000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 23997, "timers": {"training_iteration_time_ms": 17149.776, "sample_time_ms": 3809.702, "learn_time_ms": 13330.599, "learn_throughput": 300.062, "synch_weights_time_ms": 8.553}, "counters": {"num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997}, "done": false, "episodes_total": 1329, "training_iteration": 6, "trial_id": "3b26a_00000", "date": "2024-03-29_17-41-40", "timestamp": 1711734100, "time_this_iter_s": 20.86064887046814, "time_total_s": 129.5853865146637, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25DA20>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25DCF0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D123C70>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 129.5853865146637, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 9.946666666666667, "ram_util_percent": 89.53999999999999}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.435, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.56, "player_2": 0.5567010309278351, "player_1": 0.5631067961165048}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [8, 8, 12, 23, 19, 12, 10, 26, 14, 10, 13, 14, 13, 30, 24, 17, 19, 22, 13, 8, 12, 16, 18, 21, 18, 19, 23, 8, 7, 15, 18, 14, 23, 14, 9, 19, 20, 13, 9, 9, 8, 22, 39, 19, 7, 11, 23, 24, 16, 15, 21, 10, 9, 11, 12, 16, 23, 20, 13, 12, 10, 8, 22, 13, 10, 37, 13, 9, 10, 9, 10, 13, 11, 17, 12, 19, 10, 25, 13, 11, 25, 15, 24, 13, 12, 22, 10, 14, 19, 13, 14, 15, 14, 16, 18, 10, 20, 15, 16, 9, 14, 19, 8, 10, 9, 17, 16, 21, 12, 9, 15, 35, 7, 8, 13, 10, 13, 9, 13, 9, 8, 25, 8, 7, 10, 14, 10, 26, 20, 9, 15, 25, 13, 18, 13, 18, 9, 17, 18, 20, 8, 15, 29, 22, 19, 14, 14, 25, 11, 16, 17, 8, 15, 20, 22, 19, 23, 13, 15, 26, 16, 14, 7, 16, 12, 14, 11, 12, 13, 8, 12, 17, 19, 22, 22, 16, 23, 18, 18, 11, 10, 30, 9, 7, 19, 22, 8, 19, 25, 9, 9, 9, 19, 22, 28, 9, 12, 12, 28, 12], "policy_random_reward": [2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [-2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0], "policy_player_1_reward": [2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6273801290672929, "mean_inference_ms": 1.001812184969509, "mean_action_processing_ms": 0.14897506757018764, "mean_env_wait_ms": 0.09983882097914087, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01225811243057251, "ViewRequirementAgentConnector_ms": 0.24046385288238525}, "player_1_winrate": 0.6601941747572816, "player_2_winrate": 0.6597938144329897, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.5296676908930142, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.975414732346932, "policy_loss": -0.015230603990494274, "vf_loss": 1.9892735006908575, "vf_explained_var": 0.17396875508129597, "kl": 0.00685921654491467, "entropy": 0.537536138917009, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.375, "num_grad_updates_lifetime": 3120.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.334306311607361, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.0008558620424832, "policy_loss": -0.010825058499205054, "vf_loss": 2.0106071481517716, "vf_explained_var": 0.16192054549853008, "kl": 0.005368863035634705, "entropy": 0.5341668365632787, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.05882352941177, "num_grad_updates_lifetime": 3315.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.510917030567686, "episode_media": {}, "episodes_this_iter": 229, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.034934497816593885, "player_2": 0.034934497816593885}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 25, 16, 14, 27, 19, 17, 15, 16, 13, 13, 19, 25, 27, 28, 28, 22, 9, 18, 21, 11, 7, 11, 16, 15, 24, 17, 21, 28, 13, 10, 27, 26, 19, 22, 28, 13, 19, 15, 35, 16, 17, 16, 17, 16, 17, 23, 15, 22, 16, 13, 7, 19, 16, 24, 8, 11, 17, 16, 16, 17, 15, 22, 30, 23, 12, 17, 21, 16, 21, 17, 10, 10, 13, 13, 18, 13, 15, 10, 22, 9, 14, 16, 15, 26, 19, 20, 25, 19, 16, 21, 45, 18, 20, 15, 10, 16, 7, 10, 20, 17, 10, 17, 16, 14, 24, 16, 27, 11, 16, 19, 27, 24, 13, 13, 16, 19, 30, 14, 10, 16, 15, 13, 32, 13, 19, 10, 20, 14, 16, 13, 16, 10, 18, 23, 19, 16, 21, 34, 16, 13, 19, 12, 19, 16, 8, 19, 18, 21, 16, 16, 11, 24, 9, 13, 13, 19, 10, 10, 36, 8, 18, 22, 17, 16, 25, 8, 7, 20, 12, 24, 16, 16, 16, 10, 13, 15, 22, 21, 16, 25, 29, 15, 21, 18, 13, 13, 25, 20, 7, 26, 10, 13, 22, 16, 7, 13, 12, 13, 36, 41, 34, 11, 14, 13, 23, 16, 23, 15, 13, 8, 19, 15, 13, 19, 22, 23, 8, 20, 21, 21, 18, 16, 14, 10, 14, 30, 17, 11], "policy_player_1_reward": [1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6266087928994922, "mean_inference_ms": 1.8352604320419106, "mean_action_processing_ms": 0.1755650633187625, "mean_env_wait_ms": 0.11601709026286826, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01238777127328398, "ViewRequirementAgentConnector_ms": 0.1969145895612292}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.510917030567686, "episodes_this_iter": 229, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.034934497816593885, "player_2": 0.034934497816593885}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 25, 16, 14, 27, 19, 17, 15, 16, 13, 13, 19, 25, 27, 28, 28, 22, 9, 18, 21, 11, 7, 11, 16, 15, 24, 17, 21, 28, 13, 10, 27, 26, 19, 22, 28, 13, 19, 15, 35, 16, 17, 16, 17, 16, 17, 23, 15, 22, 16, 13, 7, 19, 16, 24, 8, 11, 17, 16, 16, 17, 15, 22, 30, 23, 12, 17, 21, 16, 21, 17, 10, 10, 13, 13, 18, 13, 15, 10, 22, 9, 14, 16, 15, 26, 19, 20, 25, 19, 16, 21, 45, 18, 20, 15, 10, 16, 7, 10, 20, 17, 10, 17, 16, 14, 24, 16, 27, 11, 16, 19, 27, 24, 13, 13, 16, 19, 30, 14, 10, 16, 15, 13, 32, 13, 19, 10, 20, 14, 16, 13, 16, 10, 18, 23, 19, 16, 21, 34, 16, 13, 19, 12, 19, 16, 8, 19, 18, 21, 16, 16, 11, 24, 9, 13, 13, 19, 10, 10, 36, 8, 18, 22, 17, 16, 25, 8, 7, 20, 12, 24, 16, 16, 16, 10, 13, 15, 22, 21, 16, 25, 29, 15, 21, 18, 13, 13, 25, 20, 7, 26, 10, 13, 22, 16, 7, 13, 12, 13, 36, 41, 34, 11, 14, 13, 23, 16, 23, 15, 13, 8, 19, 15, 13, 19, 22, 23, 8, 20, 21, 21, 18, 16, 14, 10, 14, 30, 17, 11], "policy_player_1_reward": [1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6266087928994922, "mean_inference_ms": 1.8352604320419106, "mean_action_processing_ms": 0.1755650633187625, "mean_env_wait_ms": 0.11601709026286826, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01238777127328398, "ViewRequirementAgentConnector_ms": 0.1969145895612292}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 232.6426508734796, "num_env_steps_trained_throughput_per_sec": 232.6426508734796, "timesteps_total": 28000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 27997, "timers": {"training_iteration_time_ms": 17156.058, "sample_time_ms": 3754.899, "learn_time_ms": 13391.823, "learn_throughput": 298.69, "synch_weights_time_ms": 8.547}, "counters": {"num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997}, "done": false, "episodes_total": 1558, "training_iteration": 7, "trial_id": "3b26a_00000", "date": "2024-03-29_17-42-01", "timestamp": 1711734121, "time_this_iter_s": 21.468051433563232, "time_total_s": 151.05343794822693, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25F130>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25F2E0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D121990>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 151.05343794822693, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 11.129999999999999, "ram_util_percent": 91.31333333333332}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.4, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.78, "player_2": 0.63, "player_1": 0.93}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [10, 10, 12, 8, 21, 16, 24, 32, 7, 23, 14, 8, 31, 15, 11, 18, 13, 21, 12, 13, 13, 8, 16, 12, 27, 19, 12, 15, 26, 8, 32, 20, 13, 16, 12, 15, 18, 43, 15, 12, 16, 11, 36, 16, 14, 17, 8, 8, 27, 15, 15, 20, 14, 16, 12, 10, 9, 25, 8, 31, 12, 11, 26, 15, 21, 10, 21, 27, 10, 12, 15, 13, 14, 32, 13, 18, 24, 10, 9, 16, 17, 14, 23, 8, 14, 16, 13, 12, 22, 10, 16, 13, 15, 13, 12, 17, 21, 9, 16, 14, 25, 20, 16, 17, 21, 8, 19, 16, 19, 21, 22, 31, 12, 18, 12, 13, 10, 17, 10, 30, 13, 16, 8, 15, 23, 10, 15, 18, 11, 11, 7, 14, 18, 25, 17, 9, 21, 17, 11, 6, 26, 17, 22, 9, 18, 14, 24, 15, 19, 7, 21, 17, 27, 29, 25, 23, 19, 13, 13, 20, 26, 20, 12, 7, 10, 9, 17, 11, 15, 29, 8, 18, 10, 15, 9, 14, 24, 9, 9, 22, 16, 17, 16, 20, 23, 8, 10, 22, 22, 18, 14, 20, 32, 19, 12, 10, 23, 13, 12, 16], "policy_random_reward": [2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [-2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, 2.0], "policy_player_1_reward": [2.0, 1.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -2.0, 1.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6379459874112032, "mean_inference_ms": 1.0211603396231437, "mean_action_processing_ms": 0.15234453770933573, "mean_env_wait_ms": 0.10291066615110363, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01608264446258545, "ViewRequirementAgentConnector_ms": 0.28991061449050903}, "player_1_winrate": 0.78, "player_2_winrate": 0.72, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.8121289988358815, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8288403106232485, "policy_loss": -0.015223535062007916, "vf_loss": 1.843153492361307, "vf_explained_var": 0.1998023021966219, "kl": 0.004551794545714849, "entropy": 0.520182993200918, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.9375, "num_grad_updates_lifetime": 3600.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.1205033052201365, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8994089883916518, "policy_loss": -0.010841027067462896, "vf_loss": 1.9089372845257029, "vf_explained_var": 0.15239270668403776, "kl": 0.0065636414541002394, "entropy": 0.4743953029314677, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.47058823529412, "num_grad_updates_lifetime": 3825.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.462962962962962, "episode_media": {}, "episodes_this_iter": 216, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.35648148148148145, "player_2": 0.35648148148148145}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 10, 44, 25, 8, 28, 19, 14, 16, 7, 19, 13, 13, 13, 20, 13, 22, 18, 17, 29, 17, 14, 19, 13, 13, 32, 13, 17, 19, 19, 15, 16, 23, 23, 19, 16, 22, 19, 25, 17, 19, 16, 11, 19, 10, 24, 12, 17, 21, 29, 11, 25, 10, 32, 34, 21, 23, 21, 18, 14, 14, 24, 21, 13, 13, 23, 34, 13, 17, 17, 19, 19, 14, 22, 19, 28, 16, 29, 20, 9, 21, 19, 8, 29, 19, 27, 16, 18, 10, 13, 23, 16, 19, 27, 22, 27, 39, 19, 18, 12, 15, 22, 30, 29, 17, 11, 34, 21, 22, 13, 18, 19, 15, 22, 38, 18, 19, 17, 25, 25, 30, 14, 16, 22, 15, 11, 17, 17, 13, 14, 15, 10, 28, 19, 12, 22, 31, 22, 13, 13, 13, 10, 23, 19, 15, 11, 11, 16, 13, 43, 13, 16, 19, 21, 22, 19, 19, 14, 10, 10, 16, 10, 10, 11, 29, 17, 13, 16, 23, 24, 23, 29, 13, 10, 10, 19, 23, 27, 30, 15, 13, 22, 11, 15, 22, 10, 8, 15, 15, 19, 21, 26, 13, 20, 23, 31, 20, 13, 7, 13, 13, 16, 16, 13, 16, 8, 13, 21, 19, 17, 17, 26, 17, 10, 22, 13], "policy_player_1_reward": [-2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0], "policy_player_2_reward": [2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6200259409045125, "mean_inference_ms": 1.8118743438839058, "mean_action_processing_ms": 0.1742817415940614, "mean_env_wait_ms": 0.11535001926778052, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.017869030987774884, "ViewRequirementAgentConnector_ms": 0.19036542486261437}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.462962962962962, "episodes_this_iter": 216, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.35648148148148145, "player_2": 0.35648148148148145}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 10, 44, 25, 8, 28, 19, 14, 16, 7, 19, 13, 13, 13, 20, 13, 22, 18, 17, 29, 17, 14, 19, 13, 13, 32, 13, 17, 19, 19, 15, 16, 23, 23, 19, 16, 22, 19, 25, 17, 19, 16, 11, 19, 10, 24, 12, 17, 21, 29, 11, 25, 10, 32, 34, 21, 23, 21, 18, 14, 14, 24, 21, 13, 13, 23, 34, 13, 17, 17, 19, 19, 14, 22, 19, 28, 16, 29, 20, 9, 21, 19, 8, 29, 19, 27, 16, 18, 10, 13, 23, 16, 19, 27, 22, 27, 39, 19, 18, 12, 15, 22, 30, 29, 17, 11, 34, 21, 22, 13, 18, 19, 15, 22, 38, 18, 19, 17, 25, 25, 30, 14, 16, 22, 15, 11, 17, 17, 13, 14, 15, 10, 28, 19, 12, 22, 31, 22, 13, 13, 13, 10, 23, 19, 15, 11, 11, 16, 13, 43, 13, 16, 19, 21, 22, 19, 19, 14, 10, 10, 16, 10, 10, 11, 29, 17, 13, 16, 23, 24, 23, 29, 13, 10, 10, 19, 23, 27, 30, 15, 13, 22, 11, 15, 22, 10, 8, 15, 15, 19, 21, 26, 13, 20, 23, 31, 20, 13, 7, 13, 13, 16, 16, 13, 16, 8, 13, 21, 19, 17, 17, 26, 17, 10, 22, 13], "policy_player_1_reward": [-2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0], "policy_player_2_reward": [2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6200259409045125, "mean_inference_ms": 1.8118743438839058, "mean_action_processing_ms": 0.1742817415940614, "mean_env_wait_ms": 0.11535001926778052, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.017869030987774884, "ViewRequirementAgentConnector_ms": 0.19036542486261437}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 240.78947273998875, "num_env_steps_trained_throughput_per_sec": 240.78947273998875, "timesteps_total": 32000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 31997, "timers": {"training_iteration_time_ms": 17088.054, "sample_time_ms": 3713.353, "learn_time_ms": 13364.884, "learn_throughput": 299.292, "synch_weights_time_ms": 8.997}, "counters": {"num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997}, "done": false, "episodes_total": 1774, "training_iteration": 8, "trial_id": "3b26a_00000", "date": "2024-03-29_17-42-23", "timestamp": 1711734143, "time_this_iter_s": 21.61887264251709, "time_total_s": 172.67231059074402, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25D3F0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25DD80>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D121CF0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 172.67231059074402, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 12.796774193548385, "ram_util_percent": 92.39032258064513}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 14.7, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.625, "player_2": 0.5463917525773195, "player_1": 0.6990291262135923}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [19, 11, 13, 11, 28, 9, 16, 25, 10, 7, 8, 14, 12, 17, 6, 20, 12, 16, 16, 8, 15, 10, 24, 19, 14, 10, 14, 16, 9, 9, 8, 13, 10, 28, 18, 20, 15, 23, 8, 12, 14, 15, 11, 11, 14, 21, 9, 10, 20, 17, 19, 21, 16, 10, 8, 9, 14, 9, 38, 13, 12, 9, 15, 18, 17, 13, 9, 7, 20, 15, 21, 29, 12, 15, 12, 14, 10, 35, 22, 15, 17, 17, 8, 8, 8, 16, 13, 24, 9, 12, 9, 7, 9, 8, 26, 20, 20, 19, 14, 24, 11, 12, 12, 24, 18, 13, 16, 14, 13, 8, 25, 9, 8, 9, 18, 17, 13, 21, 20, 13, 6, 19, 14, 11, 13, 15, 15, 14, 12, 15, 24, 19, 7, 17, 9, 11, 26, 31, 10, 10, 6, 9, 21, 22, 18, 17, 10, 12, 44, 12, 11, 8, 9, 27, 19, 18, 14, 14, 13, 8, 14, 11, 8, 14, 8, 7, 11, 10, 18, 8, 25, 37, 9, 16, 20, 11, 10, 12, 12, 14, 8, 10, 17, 10, 13, 13, 10, 9, 16, 8, 13, 11, 15, 17, 29, 10, 12, 12, 9, 39], "policy_random_reward": [-1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0], "policy_player_1_reward": [-2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6417571024896066, "mean_inference_ms": 1.0127064325615802, "mean_action_processing_ms": 0.1522759041609742, "mean_env_wait_ms": 0.10253737741393838, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01647895574569702, "ViewRequirementAgentConnector_ms": 0.27374762296676636}, "player_1_winrate": 0.6990291262135923, "player_2_winrate": 0.6494845360824743, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9985450225571793, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.031707976261775, "policy_loss": -0.016961277191876435, "vf_loss": 2.047932411978642, "vf_explained_var": 0.1769038797666629, "kl": 0.007368461883038348, "entropy": 0.5021280073250334, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.125, "num_grad_updates_lifetime": 4080.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.4670646597357355, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.049032456734601, "policy_loss": -0.01007737839530569, "vf_loss": 2.0581958681929344, "vf_explained_var": 0.16365208847849977, "kl": 0.004569852664609141, "entropy": 0.4406268012582087, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.29411764705883, "num_grad_updates_lifetime": 4335.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.68141592920354, "episode_media": {}, "episodes_this_iter": 226, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.18584070796460178, "player_2": 0.18584070796460178}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [27, 13, 19, 6, 23, 17, 14, 13, 24, 23, 14, 13, 28, 14, 20, 19, 21, 17, 16, 15, 22, 20, 16, 13, 17, 21, 23, 13, 27, 10, 28, 14, 20, 15, 16, 8, 13, 32, 8, 10, 27, 22, 31, 24, 28, 13, 19, 18, 10, 20, 14, 10, 19, 22, 21, 20, 28, 15, 9, 15, 10, 25, 25, 16, 16, 20, 19, 20, 15, 13, 22, 13, 33, 12, 37, 21, 43, 11, 10, 9, 25, 33, 13, 21, 16, 21, 24, 14, 13, 13, 17, 10, 24, 15, 23, 30, 25, 17, 7, 12, 13, 10, 17, 30, 13, 9, 22, 19, 28, 23, 16, 8, 19, 10, 28, 8, 16, 13, 16, 13, 19, 21, 24, 12, 21, 13, 26, 32, 21, 22, 11, 13, 13, 16, 17, 17, 13, 8, 13, 18, 12, 18, 19, 36, 13, 27, 13, 9, 23, 16, 22, 10, 15, 17, 19, 19, 18, 8, 17, 19, 14, 13, 10, 13, 21, 10, 13, 19, 16, 13, 10, 16, 13, 32, 19, 24, 21, 20, 22, 13, 10, 18, 7, 21, 21, 11, 18, 10, 13, 21, 10, 16, 25, 18, 19, 19, 16, 9, 11, 13, 19, 13, 25, 10, 10, 22, 19, 26, 24, 21, 16, 16, 23, 19, 12, 16, 15, 19, 21, 18, 24, 13, 14, 10, 23, 29], "policy_player_1_reward": [-1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6143534192269993, "mean_inference_ms": 1.7886711442951222, "mean_action_processing_ms": 0.1732844286669352, "mean_env_wait_ms": 0.11324009499564248, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010675297374218967, "ViewRequirementAgentConnector_ms": 0.19250485749371285}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.68141592920354, "episodes_this_iter": 226, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.18584070796460178, "player_2": 0.18584070796460178}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [27, 13, 19, 6, 23, 17, 14, 13, 24, 23, 14, 13, 28, 14, 20, 19, 21, 17, 16, 15, 22, 20, 16, 13, 17, 21, 23, 13, 27, 10, 28, 14, 20, 15, 16, 8, 13, 32, 8, 10, 27, 22, 31, 24, 28, 13, 19, 18, 10, 20, 14, 10, 19, 22, 21, 20, 28, 15, 9, 15, 10, 25, 25, 16, 16, 20, 19, 20, 15, 13, 22, 13, 33, 12, 37, 21, 43, 11, 10, 9, 25, 33, 13, 21, 16, 21, 24, 14, 13, 13, 17, 10, 24, 15, 23, 30, 25, 17, 7, 12, 13, 10, 17, 30, 13, 9, 22, 19, 28, 23, 16, 8, 19, 10, 28, 8, 16, 13, 16, 13, 19, 21, 24, 12, 21, 13, 26, 32, 21, 22, 11, 13, 13, 16, 17, 17, 13, 8, 13, 18, 12, 18, 19, 36, 13, 27, 13, 9, 23, 16, 22, 10, 15, 17, 19, 19, 18, 8, 17, 19, 14, 13, 10, 13, 21, 10, 13, 19, 16, 13, 10, 16, 13, 32, 19, 24, 21, 20, 22, 13, 10, 18, 7, 21, 21, 11, 18, 10, 13, 21, 10, 16, 25, 18, 19, 19, 16, 9, 11, 13, 19, 13, 25, 10, 10, 22, 19, 26, 24, 21, 16, 16, 23, 19, 12, 16, 15, 19, 21, 18, 24, 13, 14, 10, 23, 29], "policy_player_1_reward": [-1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6143534192269993, "mean_inference_ms": 1.7886711442951222, "mean_action_processing_ms": 0.1732844286669352, "mean_env_wait_ms": 0.11324009499564248, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010675297374218967, "ViewRequirementAgentConnector_ms": 0.19250485749371285}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 247.74817791808474, "num_env_steps_trained_throughput_per_sec": 247.74817791808474, "timesteps_total": 36000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 35997, "timers": {"training_iteration_time_ms": 16983.317, "sample_time_ms": 3667.555, "learn_time_ms": 13306.032, "learn_throughput": 300.616, "synch_weights_time_ms": 8.838}, "counters": {"num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997}, "done": false, "episodes_total": 2000, "training_iteration": 9, "trial_id": "3b26a_00000", "date": "2024-03-29_17-42-43", "timestamp": 1711734163, "time_this_iter_s": 20.04501223564148, "time_total_s": 192.7173228263855, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25E320>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25FAC0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D25FE20>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 192.7173228263855, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 9.539285714285715, "ram_util_percent": 93.8142857142857}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 14.935, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.9696969696969697, "random": -0.85, "player_2": 0.7326732673267327}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [8, 8, 23, 12, 13, 15, 22, 29, 8, 15, 12, 8, 16, 24, 26, 8, 7, 19, 36, 16, 19, 8, 10, 21, 9, 12, 28, 24, 13, 34, 8, 31, 14, 12, 10, 14, 17, 14, 8, 12, 9, 17, 11, 13, 10, 9, 10, 16, 17, 17, 15, 22, 10, 8, 10, 22, 15, 15, 10, 15, 15, 21, 9, 13, 13, 12, 19, 16, 11, 7, 14, 21, 14, 13, 6, 29, 12, 14, 7, 10, 24, 10, 13, 18, 10, 8, 17, 18, 15, 37, 12, 12, 17, 9, 21, 16, 22, 15, 11, 13, 15, 15, 15, 8, 8, 15, 12, 12, 15, 13, 23, 7, 13, 17, 12, 11, 12, 16, 15, 16, 31, 8, 24, 20, 18, 13, 9, 16, 15, 13, 14, 15, 9, 16, 14, 10, 12, 11, 15, 20, 22, 18, 30, 12, 9, 22, 9, 10, 19, 20, 8, 7, 13, 7, 21, 10, 14, 11, 22, 14, 15, 10, 24, 14, 15, 21, 19, 14, 17, 13, 8, 9, 19, 22, 10, 16, 12, 17, 31, 19, 9, 16, 10, 15, 16, 12, 13, 18, 10, 14, 12, 13, 18, 11, 18, 18, 9, 15, 22, 14], "policy_player_1_reward": [2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0], "policy_random_reward": [-2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0], "policy_player_2_reward": [1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6425110899142047, "mean_inference_ms": 1.0073548453726995, "mean_action_processing_ms": 0.1508222642760761, "mean_env_wait_ms": 0.10286664230581581, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011221468448638916, "ViewRequirementAgentConnector_ms": 0.26415008306503296}, "player_1_winrate": 0.7777777777777778, "player_2_winrate": 0.7029702970297029, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9389677132169405, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8605581654856602, "policy_loss": -0.02342365386915238, "vf_loss": 1.8833431884646417, "vf_explained_var": 0.23710224591195583, "kl": 0.006386223342811962, "entropy": 0.46813699894895155, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.25, "num_grad_updates_lifetime": 4560.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.5349781630085966, "cur_kl_coeff": 0.1, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.916774305525948, "policy_loss": -0.007198002435924376, "vf_loss": 1.923297250738331, "vf_explained_var": 0.2109959339394289, "kl": 0.006750592288474612, "entropy": 0.4166616784006942, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.17647058823529, "num_grad_updates_lifetime": 4845.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.56279069767442, "episode_media": {}, "episodes_this_iter": 215, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.24186046511627907, "player_2": 0.24186046511627907}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [35, 34, 33, 14, 25, 10, 19, 37, 8, 41, 19, 19, 13, 19, 21, 15, 13, 19, 16, 15, 16, 19, 18, 14, 13, 21, 28, 13, 15, 11, 21, 10, 13, 20, 28, 9, 17, 13, 33, 15, 10, 25, 16, 28, 26, 10, 22, 16, 13, 18, 29, 16, 13, 13, 21, 17, 41, 16, 10, 30, 26, 15, 13, 27, 16, 23, 28, 22, 19, 16, 10, 10, 13, 10, 23, 25, 13, 11, 17, 13, 10, 21, 13, 22, 24, 30, 22, 16, 19, 43, 10, 21, 13, 28, 14, 13, 13, 19, 25, 17, 19, 18, 17, 12, 15, 10, 13, 17, 38, 17, 13, 22, 13, 19, 22, 18, 19, 22, 19, 32, 22, 10, 16, 34, 23, 19, 25, 29, 19, 13, 13, 10, 8, 25, 25, 15, 13, 16, 30, 13, 14, 17, 14, 12, 16, 27, 13, 21, 13, 43, 18, 13, 10, 17, 17, 16, 10, 14, 18, 21, 14, 16, 27, 21, 8, 19, 10, 24, 10, 27, 23, 36, 21, 13, 13, 15, 8, 25, 9, 22, 21, 19, 13, 19, 16, 19, 31, 24, 13, 14, 26, 21, 10, 22, 15, 10, 21, 13, 11, 29, 12, 17, 27, 14, 13, 19, 24, 28, 10, 25, 8, 10, 10, 22, 22], "policy_player_1_reward": [-1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0], "policy_player_2_reward": [1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6079125717292714, "mean_inference_ms": 1.7652403420427765, "mean_action_processing_ms": 0.17168491321376544, "mean_env_wait_ms": 0.11183773510029875, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009002241977425508, "ViewRequirementAgentConnector_ms": 0.20298381184422692}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.56279069767442, "episodes_this_iter": 215, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.24186046511627907, "player_2": 0.24186046511627907}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [35, 34, 33, 14, 25, 10, 19, 37, 8, 41, 19, 19, 13, 19, 21, 15, 13, 19, 16, 15, 16, 19, 18, 14, 13, 21, 28, 13, 15, 11, 21, 10, 13, 20, 28, 9, 17, 13, 33, 15, 10, 25, 16, 28, 26, 10, 22, 16, 13, 18, 29, 16, 13, 13, 21, 17, 41, 16, 10, 30, 26, 15, 13, 27, 16, 23, 28, 22, 19, 16, 10, 10, 13, 10, 23, 25, 13, 11, 17, 13, 10, 21, 13, 22, 24, 30, 22, 16, 19, 43, 10, 21, 13, 28, 14, 13, 13, 19, 25, 17, 19, 18, 17, 12, 15, 10, 13, 17, 38, 17, 13, 22, 13, 19, 22, 18, 19, 22, 19, 32, 22, 10, 16, 34, 23, 19, 25, 29, 19, 13, 13, 10, 8, 25, 25, 15, 13, 16, 30, 13, 14, 17, 14, 12, 16, 27, 13, 21, 13, 43, 18, 13, 10, 17, 17, 16, 10, 14, 18, 21, 14, 16, 27, 21, 8, 19, 10, 24, 10, 27, 23, 36, 21, 13, 13, 15, 8, 25, 9, 22, 21, 19, 13, 19, 16, 19, 31, 24, 13, 14, 26, 21, 10, 22, 15, 10, 21, 13, 11, 29, 12, 17, 27, 14, 13, 19, 24, 28, 10, 25, 8, 10, 10, 22, 22], "policy_player_1_reward": [-1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0], "policy_player_2_reward": [1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6079125717292714, "mean_inference_ms": 1.7652403420427765, "mean_action_processing_ms": 0.17168491321376544, "mean_env_wait_ms": 0.11183773510029875, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009002241977425508, "ViewRequirementAgentConnector_ms": 0.20298381184422692}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997, "num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 244.08472425670016, "num_env_steps_trained_throughput_per_sec": 244.08472425670016, "timesteps_total": 40000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 39997, "timers": {"training_iteration_time_ms": 16923.761, "sample_time_ms": 3622.406, "learn_time_ms": 13291.503, "learn_throughput": 300.944, "synch_weights_time_ms": 8.926}, "counters": {"num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997}, "done": false, "episodes_total": 2215, "training_iteration": 10, "trial_id": "3b26a_00000", "date": "2024-03-29_17-43-04", "timestamp": 1711734184, "time_this_iter_s": 20.289244890213013, "time_total_s": 213.0065677165985, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D305360>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D305000>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D25D1B0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 213.0065677165985, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 10.237931034482758, "ram_util_percent": 94.51724137931036}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 14.665, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.83, "player_2": 0.6730769230769231, "player_1": 1.0}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [10, 9, 7, 16, 8, 19, 27, 25, 16, 9, 12, 25, 8, 11, 16, 8, 14, 21, 10, 9, 10, 10, 15, 14, 16, 14, 8, 10, 15, 21, 24, 10, 13, 18, 13, 20, 11, 42, 10, 15, 16, 15, 24, 8, 20, 10, 10, 9, 13, 19, 23, 12, 7, 8, 20, 11, 9, 24, 15, 13, 16, 26, 13, 10, 16, 22, 18, 10, 12, 7, 8, 8, 8, 21, 19, 8, 12, 11, 11, 8, 32, 9, 9, 18, 8, 21, 21, 21, 11, 19, 12, 7, 18, 15, 18, 14, 8, 15, 34, 10, 17, 11, 28, 13, 11, 10, 13, 15, 9, 17, 10, 12, 8, 18, 14, 18, 15, 7, 12, 14, 19, 12, 8, 9, 17, 35, 31, 14, 30, 32, 8, 20, 11, 10, 17, 28, 10, 20, 9, 9, 13, 20, 9, 8, 27, 14, 18, 15, 15, 15, 13, 8, 19, 19, 14, 12, 19, 8, 17, 9, 20, 17, 22, 14, 14, 25, 12, 11, 16, 13, 8, 12, 14, 17, 16, 11, 13, 18, 7, 8, 16, 11, 14, 6, 19, 9, 11, 16, 13, 15, 8, 15, 10, 8, 16, 7, 13, 37, 20, 9], "policy_random_reward": [2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [-2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0], "policy_player_1_reward": [2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6457465518538842, "mean_inference_ms": 1.008890252454787, "mean_action_processing_ms": 0.15143329573244413, "mean_env_wait_ms": 0.10212308671679798, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014045476913452148, "ViewRequirementAgentConnector_ms": 0.27943164110183716}, "player_1_winrate": 0.7916666666666666, "player_2_winrate": 0.7019230769230769, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.0122134466966, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8153640794257322, "policy_loss": -0.016326199910933308, "vf_loss": 1.8310587654511135, "vf_explained_var": 0.2734724951287111, "kl": 0.006315126117928841, "entropy": 0.4357886889018118, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.5, "num_grad_updates_lifetime": 5040.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.9655478234384574, "cur_kl_coeff": 0.1, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8636537517987046, "policy_loss": -0.0073838906727877315, "vf_loss": 1.870469579860276, "vf_explained_var": 0.23837922869944106, "kl": 0.005680702706288937, "entropy": 0.41039696215998894, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.94117647058823, "num_grad_updates_lifetime": 5355.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.21153846153846, "episode_media": {}, "episodes_this_iter": 208, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.22596153846153846, "player_2": 0.22596153846153846}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [18, 21, 20, 15, 28, 20, 21, 19, 25, 15, 18, 26, 13, 31, 10, 13, 12, 32, 21, 13, 10, 13, 19, 19, 13, 17, 13, 27, 13, 11, 25, 11, 16, 19, 23, 28, 19, 16, 16, 17, 24, 17, 13, 19, 21, 8, 19, 37, 22, 28, 8, 9, 13, 16, 11, 22, 10, 28, 23, 16, 37, 17, 13, 14, 13, 22, 13, 11, 25, 17, 27, 14, 10, 35, 10, 19, 29, 15, 9, 16, 29, 19, 7, 8, 14, 16, 10, 14, 22, 25, 8, 21, 31, 13, 44, 10, 22, 13, 10, 48, 15, 16, 22, 17, 13, 25, 10, 10, 19, 31, 26, 25, 22, 31, 22, 23, 22, 10, 25, 16, 20, 29, 19, 23, 10, 14, 10, 25, 30, 10, 16, 16, 13, 22, 31, 10, 16, 13, 13, 11, 12, 13, 13, 10, 19, 17, 23, 13, 23, 10, 29, 28, 20, 28, 10, 16, 57, 13, 19, 10, 15, 16, 18, 32, 21, 16, 25, 10, 25, 15, 19, 13, 33, 19, 36, 22, 34, 16, 16, 13, 19, 36, 20, 15, 13, 24, 19, 24, 10, 17, 13, 24, 12, 17, 25, 22, 33, 14, 19, 10, 52, 13, 13, 47, 35, 10, 15, 27], "policy_player_1_reward": [1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6081267136924391, "mean_inference_ms": 1.7650527120759956, "mean_action_processing_ms": 0.17282527516456023, "mean_env_wait_ms": 0.1120956716772359, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009595889311570387, "ViewRequirementAgentConnector_ms": 0.21239387301298288}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.21153846153846, "episodes_this_iter": 208, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.22596153846153846, "player_2": 0.22596153846153846}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [18, 21, 20, 15, 28, 20, 21, 19, 25, 15, 18, 26, 13, 31, 10, 13, 12, 32, 21, 13, 10, 13, 19, 19, 13, 17, 13, 27, 13, 11, 25, 11, 16, 19, 23, 28, 19, 16, 16, 17, 24, 17, 13, 19, 21, 8, 19, 37, 22, 28, 8, 9, 13, 16, 11, 22, 10, 28, 23, 16, 37, 17, 13, 14, 13, 22, 13, 11, 25, 17, 27, 14, 10, 35, 10, 19, 29, 15, 9, 16, 29, 19, 7, 8, 14, 16, 10, 14, 22, 25, 8, 21, 31, 13, 44, 10, 22, 13, 10, 48, 15, 16, 22, 17, 13, 25, 10, 10, 19, 31, 26, 25, 22, 31, 22, 23, 22, 10, 25, 16, 20, 29, 19, 23, 10, 14, 10, 25, 30, 10, 16, 16, 13, 22, 31, 10, 16, 13, 13, 11, 12, 13, 13, 10, 19, 17, 23, 13, 23, 10, 29, 28, 20, 28, 10, 16, 57, 13, 19, 10, 15, 16, 18, 32, 21, 16, 25, 10, 25, 15, 19, 13, 33, 19, 36, 22, 34, 16, 16, 13, 19, 36, 20, 15, 13, 24, 19, 24, 10, 17, 13, 24, 12, 17, 25, 22, 33, 14, 19, 10, 52, 13, 13, 47, 35, 10, 15, 27], "policy_player_1_reward": [1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6081267136924391, "mean_inference_ms": 1.7650527120759956, "mean_action_processing_ms": 0.17282527516456023, "mean_env_wait_ms": 0.1120956716772359, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009595889311570387, "ViewRequirementAgentConnector_ms": 0.21239387301298288}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997, "num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 236.8224780978729, "num_env_steps_trained_throughput_per_sec": 236.8224780978729, "timesteps_total": 44000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 43997, "timers": {"training_iteration_time_ms": 16865.098, "sample_time_ms": 3571.364, "learn_time_ms": 13283.95, "learn_throughput": 301.115, "synch_weights_time_ms": 8.819}, "counters": {"num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997}, "done": false, "episodes_total": 2423, "training_iteration": 11, "trial_id": "3b26a_00000", "date": "2024-03-29_17-43-25", "timestamp": 1711734205, "time_this_iter_s": 20.97157883644104, "time_total_s": 233.97814655303955, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25EEF0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25D3F0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D25F0A0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 233.97814655303955, "iterations_since_restore": 11, "perf": {"cpu_util_percent": 10.941379310344828, "ram_util_percent": 94.64827586206896}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.27, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.675, "player_2": 0.8431372549019608, "player_1": 0.5}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [13, 7, 19, 20, 15, 12, 18, 13, 17, 30, 10, 9, 15, 16, 24, 17, 7, 11, 23, 15, 10, 20, 31, 26, 15, 13, 15, 11, 44, 15, 12, 18, 20, 23, 9, 15, 10, 8, 11, 13, 23, 11, 25, 10, 18, 13, 13, 17, 13, 13, 15, 30, 29, 15, 20, 36, 17, 8, 16, 11, 27, 13, 27, 10, 13, 13, 25, 19, 18, 10, 12, 7, 23, 8, 17, 23, 12, 12, 8, 32, 15, 18, 10, 19, 10, 14, 9, 11, 15, 12, 27, 15, 29, 22, 28, 10, 14, 17, 17, 11, 14, 15, 30, 30, 13, 7, 10, 7, 22, 12, 18, 19, 23, 34, 20, 29, 22, 20, 22, 18, 22, 24, 9, 10, 13, 21, 7, 8, 21, 16, 13, 16, 15, 12, 25, 19, 12, 9, 28, 11, 15, 9, 35, 22, 13, 28, 19, 10, 11, 17, 8, 12, 10, 17, 9, 26, 9, 11, 12, 13, 18, 10, 11, 8, 11, 7, 10, 9, 11, 9, 19, 13, 10, 10, 15, 17, 15, 25, 10, 19, 9, 35, 19, 8, 15, 14, 30, 12, 8, 11, 10, 28, 16, 19, 15, 21, 20, 20, 21, 9], "policy_random_reward": [-2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0], "policy_player_1_reward": [1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, 1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6479196220029501, "mean_inference_ms": 1.0160430126744442, "mean_action_processing_ms": 0.15252936088923277, "mean_env_wait_ms": 0.10357722789719452, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014254510402679443, "ViewRequirementAgentConnector_ms": 0.26791107654571533}, "player_1_winrate": 0.6224489795918368, "player_2_winrate": 0.7352941176470589, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.051965791980425, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6533134707560142, "policy_loss": -0.016110726223269013, "vf_loss": 1.6687972987691562, "vf_explained_var": 0.26845428757369516, "kl": 0.0062690019962252114, "entropy": 0.4027797441308697, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.3125, "num_grad_updates_lifetime": 5520.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7718531395874773, "cur_kl_coeff": 0.1, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.747128417445164, "policy_loss": -0.005848767184743694, "vf_loss": 1.7525128425336352, "vf_explained_var": 0.21293292863696228, "kl": 0.004643332492700708, "entropy": 0.3993923782717948, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.11764705882354, "num_grad_updates_lifetime": 5865.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}}, "num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.294416243654823, "episode_media": {}, "episodes_this_iter": 197, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.3401015228426396, "player_2": 0.3401015228426396}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 42, 24, 38, 10, 28, 20, 42, 19, 13, 12, 21, 25, 9, 13, 23, 15, 14, 22, 13, 16, 19, 36, 28, 22, 28, 13, 24, 10, 13, 11, 9, 31, 16, 13, 40, 10, 16, 11, 14, 24, 29, 24, 13, 25, 13, 13, 13, 26, 45, 16, 33, 19, 20, 13, 27, 13, 27, 17, 11, 13, 23, 25, 14, 11, 37, 20, 16, 17, 16, 13, 18, 28, 16, 25, 29, 33, 13, 13, 9, 19, 17, 11, 24, 8, 31, 20, 17, 28, 23, 30, 31, 30, 10, 19, 13, 46, 29, 21, 21, 20, 25, 13, 13, 27, 19, 22, 16, 10, 16, 7, 10, 19, 19, 10, 19, 20, 13, 23, 10, 25, 25, 13, 19, 10, 52, 14, 16, 13, 17, 22, 17, 25, 18, 14, 30, 29, 19, 21, 13, 28, 23, 20, 21, 10, 10, 23, 14, 26, 14, 13, 10, 10, 23, 25, 35, 14, 25, 57, 15, 11, 22, 28, 13, 13, 13, 27, 13, 25, 33, 10, 39, 16, 10, 13, 17, 16, 16, 21, 8, 15, 25, 24, 13, 27, 25, 28, 23, 19, 25, 13, 63, 16, 23, 13, 53, 13], "policy_player_1_reward": [2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0], "policy_player_2_reward": [-2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.613888272156403, "mean_inference_ms": 1.782502660416159, "mean_action_processing_ms": 0.17566556190035204, "mean_env_wait_ms": 0.11282524409729516, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010882718914051346, "ViewRequirementAgentConnector_ms": 0.24647464606967676}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.294416243654823, "episodes_this_iter": 197, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.3401015228426396, "player_2": 0.3401015228426396}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 42, 24, 38, 10, 28, 20, 42, 19, 13, 12, 21, 25, 9, 13, 23, 15, 14, 22, 13, 16, 19, 36, 28, 22, 28, 13, 24, 10, 13, 11, 9, 31, 16, 13, 40, 10, 16, 11, 14, 24, 29, 24, 13, 25, 13, 13, 13, 26, 45, 16, 33, 19, 20, 13, 27, 13, 27, 17, 11, 13, 23, 25, 14, 11, 37, 20, 16, 17, 16, 13, 18, 28, 16, 25, 29, 33, 13, 13, 9, 19, 17, 11, 24, 8, 31, 20, 17, 28, 23, 30, 31, 30, 10, 19, 13, 46, 29, 21, 21, 20, 25, 13, 13, 27, 19, 22, 16, 10, 16, 7, 10, 19, 19, 10, 19, 20, 13, 23, 10, 25, 25, 13, 19, 10, 52, 14, 16, 13, 17, 22, 17, 25, 18, 14, 30, 29, 19, 21, 13, 28, 23, 20, 21, 10, 10, 23, 14, 26, 14, 13, 10, 10, 23, 25, 35, 14, 25, 57, 15, 11, 22, 28, 13, 13, 13, 27, 13, 25, 33, 10, 39, 16, 10, 13, 17, 16, 16, 21, 8, 15, 25, 24, 13, 27, 25, 28, 23, 19, 25, 13, 63, 16, 23, 13, 53, 13], "policy_player_1_reward": [2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0], "policy_player_2_reward": [-2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.613888272156403, "mean_inference_ms": 1.782502660416159, "mean_action_processing_ms": 0.17566556190035204, "mean_env_wait_ms": 0.11282524409729516, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010882718914051346, "ViewRequirementAgentConnector_ms": 0.24647464606967676}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997, "num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 231.75141351416443, "num_env_steps_trained_throughput_per_sec": 231.75141351416443, "timesteps_total": 48000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 47997, "timers": {"training_iteration_time_ms": 16843.421, "sample_time_ms": 3598.168, "learn_time_ms": 13235.243, "learn_throughput": 302.223, "synch_weights_time_ms": 9.008}, "counters": {"num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997}, "done": false, "episodes_total": 2620, "training_iteration": 12, "trial_id": "3b26a_00000", "date": "2024-03-29_17-43-47", "timestamp": 1711734227, "time_this_iter_s": 21.932432889938354, "time_total_s": 255.9105794429779, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D3067A0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D306440>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D305CF0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 255.9105794429779, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 11.64516129032258, "ram_util_percent": 95.36451612903225}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.035, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.715, "player_2": 0.8181818181818182, "player_1": 0.6138613861386139}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [13, 14, 36, 26, 11, 17, 17, 15, 17, 10, 10, 15, 18, 14, 12, 13, 13, 10, 13, 9, 11, 12, 17, 9, 8, 15, 27, 16, 15, 13, 13, 18, 13, 13, 10, 8, 17, 11, 16, 14, 15, 17, 14, 19, 19, 10, 20, 9, 15, 16, 11, 14, 8, 16, 10, 10, 17, 11, 20, 8, 24, 15, 17, 11, 8, 15, 26, 9, 20, 13, 7, 34, 8, 8, 17, 12, 11, 10, 13, 9, 14, 11, 50, 15, 7, 18, 8, 13, 23, 11, 15, 11, 7, 10, 19, 19, 16, 15, 10, 9, 7, 8, 18, 16, 16, 19, 23, 12, 22, 15, 11, 14, 18, 20, 22, 25, 13, 10, 15, 8, 8, 13, 19, 21, 17, 18, 13, 13, 9, 9, 15, 8, 20, 10, 26, 23, 14, 13, 9, 8, 9, 9, 22, 38, 19, 33, 11, 16, 12, 11, 15, 19, 19, 14, 7, 22, 10, 8, 7, 18, 12, 38, 12, 8, 14, 10, 12, 14, 9, 35, 8, 24, 9, 19, 15, 35, 16, 9, 14, 20, 7, 13, 13, 11, 27, 16, 13, 17, 18, 17, 15, 31, 9, 11, 9, 15, 12, 27, 12, 18], "policy_random_reward": [-2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, -1.0, 1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0], "policy_player_2_reward": [2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0], "policy_player_1_reward": [2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6417274930489257, "mean_inference_ms": 1.002731168559344, "mean_action_processing_ms": 0.15124160639882475, "mean_env_wait_ms": 0.1022421203848063, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010537922382354736, "ViewRequirementAgentConnector_ms": 0.21487367153167725}, "player_1_winrate": 0.6732673267326733, "player_2_winrate": 0.7676767676767676, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.002739413082599, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7456418400009472, "policy_loss": -0.01689343882802253, "vf_loss": 1.7619928372402986, "vf_explained_var": 0.21928932803372542, "kl": 0.00542448687639348, "entropy": 0.4100977380449573, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.25, "num_grad_updates_lifetime": 6000.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.343841664741437, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7052420258522034, "policy_loss": -0.012514429474443508, "vf_loss": 1.7174922473728658, "vf_explained_var": 0.2437733132392168, "kl": 0.005284231072064967, "entropy": 0.36251343730837104, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.75, "num_grad_updates_lifetime": 6360.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.36111111111111, "episode_media": {}, "episodes_this_iter": 180, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.022222222222222223, "player_2": 0.022222222222222223}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 23, 14, 55, 13, 23, 24, 24, 28, 16, 16, 16, 17, 22, 13, 26, 22, 28, 18, 13, 24, 13, 13, 12, 10, 13, 10, 10, 24, 19, 15, 25, 10, 33, 34, 34, 25, 46, 19, 10, 16, 27, 16, 10, 22, 10, 48, 13, 44, 13, 20, 10, 13, 36, 14, 23, 44, 13, 15, 18, 15, 28, 10, 33, 31, 56, 25, 16, 31, 19, 22, 14, 29, 14, 58, 19, 19, 27, 13, 13, 31, 15, 30, 44, 24, 26, 33, 13, 19, 13, 22, 47, 30, 16, 25, 13, 24, 23, 22, 25, 13, 17, 58, 43, 11, 13, 18, 19, 40, 27, 10, 16, 29, 17, 24, 19, 27, 19, 10, 41, 10, 17, 36, 14, 24, 13, 17, 25, 16, 43, 10, 19, 23, 13, 30, 10, 24, 13, 15, 22, 35, 32, 19, 32, 20, 16, 13, 13, 33, 25, 14, 27, 12, 26, 25, 9, 9, 52, 30, 29, 25, 38, 19, 26, 24, 19, 10, 19, 16, 21, 26, 40, 36, 13, 10, 32, 22, 23, 25, 10], "policy_player_1_reward": [-2.0, -1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0], "policy_player_2_reward": [2.0, 1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6100869430292466, "mean_inference_ms": 1.7719185823801158, "mean_action_processing_ms": 0.17525696583980568, "mean_env_wait_ms": 0.11282446984654114, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010199546813964844, "ViewRequirementAgentConnector_ms": 0.20240465799967447}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.36111111111111, "episodes_this_iter": 180, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.022222222222222223, "player_2": 0.022222222222222223}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 23, 14, 55, 13, 23, 24, 24, 28, 16, 16, 16, 17, 22, 13, 26, 22, 28, 18, 13, 24, 13, 13, 12, 10, 13, 10, 10, 24, 19, 15, 25, 10, 33, 34, 34, 25, 46, 19, 10, 16, 27, 16, 10, 22, 10, 48, 13, 44, 13, 20, 10, 13, 36, 14, 23, 44, 13, 15, 18, 15, 28, 10, 33, 31, 56, 25, 16, 31, 19, 22, 14, 29, 14, 58, 19, 19, 27, 13, 13, 31, 15, 30, 44, 24, 26, 33, 13, 19, 13, 22, 47, 30, 16, 25, 13, 24, 23, 22, 25, 13, 17, 58, 43, 11, 13, 18, 19, 40, 27, 10, 16, 29, 17, 24, 19, 27, 19, 10, 41, 10, 17, 36, 14, 24, 13, 17, 25, 16, 43, 10, 19, 23, 13, 30, 10, 24, 13, 15, 22, 35, 32, 19, 32, 20, 16, 13, 13, 33, 25, 14, 27, 12, 26, 25, 9, 9, 52, 30, 29, 25, 38, 19, 26, 24, 19, 10, 19, 16, 21, 26, 40, 36, 13, 10, 32, 22, 23, 25, 10], "policy_player_1_reward": [-2.0, -1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0], "policy_player_2_reward": [2.0, 1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6100869430292466, "mean_inference_ms": 1.7719185823801158, "mean_action_processing_ms": 0.17525696583980568, "mean_env_wait_ms": 0.11282446984654114, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010199546813964844, "ViewRequirementAgentConnector_ms": 0.20240465799967447}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997, "num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 228.1218223105219, "num_env_steps_trained_throughput_per_sec": 228.1218223105219, "timesteps_total": 52000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 51997, "timers": {"training_iteration_time_ms": 16815.039, "sample_time_ms": 3563.671, "learn_time_ms": 13241.249, "learn_throughput": 302.086, "synch_weights_time_ms": 9.118}, "counters": {"num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997}, "done": false, "episodes_total": 2800, "training_iteration": 13, "trial_id": "3b26a_00000", "date": "2024-03-29_17-44-08", "timestamp": 1711734248, "time_this_iter_s": 21.027127742767334, "time_total_s": 276.93770718574524, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D25E4D0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D25DFC0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D0AB2E0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 276.93770718574524, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 11.636666666666665, "ram_util_percent": 94.82333333333331}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.01, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.8016528925619835, "random": -0.715, "player_2": 0.5822784810126582}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [16, 9, 12, 9, 15, 10, 14, 17, 10, 37, 26, 10, 13, 17, 21, 10, 13, 8, 7, 10, 9, 18, 15, 32, 11, 14, 23, 22, 19, 11, 10, 28, 9, 10, 13, 13, 26, 13, 14, 28, 8, 7, 13, 12, 10, 9, 8, 13, 23, 8, 16, 29, 8, 12, 10, 19, 14, 10, 14, 8, 9, 8, 10, 22, 13, 7, 14, 13, 25, 18, 12, 16, 15, 11, 9, 17, 8, 11, 22, 14, 9, 15, 21, 16, 12, 28, 8, 28, 12, 22, 16, 8, 7, 14, 14, 8, 18, 28, 10, 20, 10, 7, 18, 23, 11, 17, 12, 16, 25, 21, 16, 9, 10, 12, 12, 15, 10, 14, 9, 15, 21, 16, 9, 17, 16, 16, 18, 27, 13, 16, 8, 10, 15, 20, 13, 13, 14, 10, 10, 10, 14, 11, 19, 15, 11, 11, 8, 19, 15, 13, 19, 19, 20, 10, 17, 15, 28, 14, 13, 15, 15, 29, 16, 10, 24, 12, 9, 8, 12, 16, 14, 22, 13, 24, 23, 24, 14, 14, 34, 14, 15, 24, 12, 10, 14, 16, 26, 10, 13, 9, 26, 13, 14, 27, 15, 18, 9, 19, 12, 12], "policy_player_1_reward": [1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0], "policy_random_reward": [-1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6476942356934253, "mean_inference_ms": 1.0137964440845948, "mean_action_processing_ms": 0.15237655018618734, "mean_env_wait_ms": 0.10314883297210752, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015082061290740967, "ViewRequirementAgentConnector_ms": 0.28918343782424927}, "player_1_winrate": 0.7272727272727273, "player_2_winrate": 0.6708860759493671, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.073399902383486, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6576770449678102, "policy_loss": -0.01897998728575961, "vf_loss": 1.6760226388772328, "vf_explained_var": 0.28014085615674655, "kl": 0.006343953750984719, "entropy": 0.3973533251322806, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.1875, "num_grad_updates_lifetime": 6480.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7002769651512306, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7060234089692434, "policy_loss": -0.010339427726285067, "vf_loss": 1.7161553032696246, "vf_explained_var": 0.26578370841840904, "kl": 0.004150856591854223, "entropy": 0.36271687562887867, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.8125, "num_grad_updates_lifetime": 6840.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.429378531073446, "episode_media": {}, "episodes_this_iter": 177, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.12429378531073447, "player_2": 0.12429378531073447}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [17, 10, 27, 13, 14, 13, 36, 18, 19, 17, 12, 24, 27, 15, 26, 34, 24, 13, 23, 10, 14, 21, 21, 20, 42, 17, 35, 13, 42, 31, 61, 9, 13, 13, 31, 13, 13, 13, 14, 25, 27, 10, 10, 26, 23, 13, 46, 17, 16, 22, 27, 25, 19, 28, 10, 13, 48, 18, 36, 26, 25, 20, 10, 13, 27, 26, 25, 25, 24, 28, 30, 30, 13, 28, 25, 32, 10, 19, 13, 30, 28, 17, 22, 10, 28, 21, 42, 25, 10, 22, 14, 23, 35, 24, 49, 16, 26, 10, 10, 16, 38, 10, 27, 40, 11, 13, 23, 25, 19, 13, 13, 18, 28, 25, 27, 9, 19, 10, 21, 14, 15, 19, 22, 40, 13, 22, 18, 30, 27, 19, 30, 13, 30, 22, 34, 82, 21, 37, 30, 27, 16, 10, 13, 21, 16, 13, 29, 10, 13, 19, 52, 46, 48, 10, 19, 34, 19, 10, 13, 10, 16, 21, 14, 15, 23, 25, 13, 31, 40, 25, 21, 28, 22, 16, 33, 11, 27], "policy_player_1_reward": [-1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6030403749459017, "mean_inference_ms": 1.755389547636602, "mean_action_processing_ms": 0.17379484199657688, "mean_env_wait_ms": 0.11209946954057391, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00736363190042097, "ViewRequirementAgentConnector_ms": 0.178932066017625}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.429378531073446, "episodes_this_iter": 177, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.12429378531073447, "player_2": 0.12429378531073447}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [17, 10, 27, 13, 14, 13, 36, 18, 19, 17, 12, 24, 27, 15, 26, 34, 24, 13, 23, 10, 14, 21, 21, 20, 42, 17, 35, 13, 42, 31, 61, 9, 13, 13, 31, 13, 13, 13, 14, 25, 27, 10, 10, 26, 23, 13, 46, 17, 16, 22, 27, 25, 19, 28, 10, 13, 48, 18, 36, 26, 25, 20, 10, 13, 27, 26, 25, 25, 24, 28, 30, 30, 13, 28, 25, 32, 10, 19, 13, 30, 28, 17, 22, 10, 28, 21, 42, 25, 10, 22, 14, 23, 35, 24, 49, 16, 26, 10, 10, 16, 38, 10, 27, 40, 11, 13, 23, 25, 19, 13, 13, 18, 28, 25, 27, 9, 19, 10, 21, 14, 15, 19, 22, 40, 13, 22, 18, 30, 27, 19, 30, 13, 30, 22, 34, 82, 21, 37, 30, 27, 16, 10, 13, 21, 16, 13, 29, 10, 13, 19, 52, 46, 48, 10, 19, 34, 19, 10, 13, 10, 16, 21, 14, 15, 23, 25, 13, 31, 40, 25, 21, 28, 22, 16, 33, 11, 27], "policy_player_1_reward": [-1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6030403749459017, "mean_inference_ms": 1.755389547636602, "mean_action_processing_ms": 0.17379484199657688, "mean_env_wait_ms": 0.11209946954057391, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00736363190042097, "ViewRequirementAgentConnector_ms": 0.178932066017625}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997, "num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 242.25364347561305, "num_env_steps_trained_throughput_per_sec": 242.25364347561305, "timesteps_total": 56000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 55997, "timers": {"training_iteration_time_ms": 16822.866, "sample_time_ms": 3546.752, "learn_time_ms": 13265.93, "learn_throughput": 301.524, "synch_weights_time_ms": 9.082}, "counters": {"num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997}, "done": false, "episodes_total": 2977, "training_iteration": 14, "trial_id": "3b26a_00000", "date": "2024-03-29_17-44-29", "timestamp": 1711734269, "time_this_iter_s": 21.095048904418945, "time_total_s": 298.0327560901642, "pid": 16464, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x0000020F4D305120>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x0000020F4D307880>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x0000020F4D305900>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 298.0327560901642, "iterations_since_restore": 14, "perf": {"cpu_util_percent": 12.10344827586207, "ram_util_percent": 92.63448275862068}}
