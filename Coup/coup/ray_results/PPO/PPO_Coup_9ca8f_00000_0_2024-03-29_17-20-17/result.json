{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.355, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.43, "player_2": 0.4666666666666667, "player_1": 0.3894736842105263}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [15, 24, 20, 8, 10, 9, 22, 23, 7, 20, 10, 22, 13, 8, 8, 21, 12, 16, 22, 8, 11, 39, 16, 20, 29, 19, 31, 17, 25, 14, 21, 27, 8, 15, 24, 19, 7, 8, 14, 22, 18, 23, 21, 39, 20, 27, 25, 17, 17, 11, 10, 19, 18, 23, 9, 10, 10, 35, 10, 12, 16, 22, 33, 16, 10, 23, 15, 34, 9, 17, 15, 29, 27, 55, 21, 11, 14, 15, 18, 11, 20, 15, 26, 8, 21, 27, 9, 9, 21, 25, 38, 16, 21, 23, 7, 28, 20, 19, 9, 13, 8, 16, 22, 22, 16, 27, 35, 20, 22, 16, 19, 17, 28, 29, 21, 11, 28, 20, 27, 24, 38, 14, 9, 25, 22, 18, 28, 17, 7, 16, 17, 18, 21, 18, 30, 16, 17, 18, 19, 17, 18, 10, 25, 14, 13, 6, 10, 19, 29, 9, 7, 9, 13, 13, 12, 13, 7, 15, 10, 12, 26, 13, 12, 8, 18, 25, 24, 23, 11, 11, 37, 24, 19, 15, 10, 7, 12, 12, 7, 22, 10, 26, 28, 22, 11, 17, 29, 25, 22, 21, 27, 35, 22, 10, 16, 11, 12, 29, 18, 23], "policy_random_reward": [-1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0], "policy_player_2_reward": [1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0], "policy_player_1_reward": [1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5452678889969483, "mean_inference_ms": 0.8234200327493649, "mean_action_processing_ms": 0.12564912381751028, "mean_env_wait_ms": 0.08083948583860164, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00961523585849338, "ViewRequirementAgentConnector_ms": 0.21983468863699174}, "player_1_winrate": 0.631578947368421, "player_2_winrate": 0.6666666666666666, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.9135307648602655, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.181369765365825, "policy_loss": -0.022649372270440355, "vf_loss": 2.201391088728811, "vf_explained_var": 0.04613772303450341, "kl": 0.013140322547374319, "entropy": 0.8221930592083464, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.88235294117646, "num_grad_updates_lifetime": 255.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.8954473843177158, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 2.1206147288282713, "policy_loss": -0.017880540357631012, "vf_loss": 2.1354621852437656, "vf_explained_var": 0.07190866470336914, "kl": 0.01516545831329572, "entropy": 0.7930040242771308, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.375, "num_grad_updates_lifetime": 240.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.72641509433962, "episode_media": {}, "episodes_this_iter": 212, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.04245283018867924, "player_2": 0.04245283018867924}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 7, 14, 27, 10, 10, 15, 26, 14, 15, 44, 10, 20, 26, 43, 17, 21, 39, 13, 10, 19, 7, 22, 16, 13, 25, 9, 19, 21, 13, 16, 19, 38, 37, 13, 35, 21, 16, 27, 17, 26, 17, 28, 12, 17, 7, 49, 9, 18, 23, 29, 25, 8, 20, 28, 10, 29, 14, 20, 18, 20, 18, 20, 14, 26, 13, 8, 9, 25, 14, 9, 10, 14, 9, 19, 10, 9, 23, 19, 21, 31, 19, 17, 23, 16, 34, 26, 14, 42, 23, 8, 7, 16, 22, 29, 10, 18, 23, 17, 8, 28, 18, 18, 35, 17, 23, 21, 34, 31, 7, 23, 23, 9, 20, 15, 43, 21, 26, 12, 26, 24, 14, 22, 11, 28, 8, 9, 7, 9, 13, 13, 17, 17, 9, 13, 17, 12, 26, 13, 22, 25, 28, 21, 28, 18, 14, 10, 10, 18, 31, 14, 14, 23, 24, 7, 16, 24, 21, 19, 9, 13, 28, 9, 9, 9, 22, 20, 30, 11, 18, 14, 27, 15, 16, 7, 13, 25, 10, 11, 30, 9, 23, 14, 18, 23, 18, 19, 30, 34, 20, 21, 10, 11, 17, 39, 7, 16, 21, 22, 13, 14, 16, 13, 18, 28, 20, 16, 12, 21, 11, 18, 16], "policy_player_1_reward": [-1.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0], "policy_player_2_reward": [1.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5305725045386278, "mean_inference_ms": 1.5074627356934125, "mean_action_processing_ms": 0.1494694040504311, "mean_env_wait_ms": 0.10807640019605272, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011376164994149838, "ViewRequirementAgentConnector_ms": 0.17657541642291846}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.72641509433962, "episodes_this_iter": 212, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.04245283018867924, "player_2": 0.04245283018867924}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 7, 14, 27, 10, 10, 15, 26, 14, 15, 44, 10, 20, 26, 43, 17, 21, 39, 13, 10, 19, 7, 22, 16, 13, 25, 9, 19, 21, 13, 16, 19, 38, 37, 13, 35, 21, 16, 27, 17, 26, 17, 28, 12, 17, 7, 49, 9, 18, 23, 29, 25, 8, 20, 28, 10, 29, 14, 20, 18, 20, 18, 20, 14, 26, 13, 8, 9, 25, 14, 9, 10, 14, 9, 19, 10, 9, 23, 19, 21, 31, 19, 17, 23, 16, 34, 26, 14, 42, 23, 8, 7, 16, 22, 29, 10, 18, 23, 17, 8, 28, 18, 18, 35, 17, 23, 21, 34, 31, 7, 23, 23, 9, 20, 15, 43, 21, 26, 12, 26, 24, 14, 22, 11, 28, 8, 9, 7, 9, 13, 13, 17, 17, 9, 13, 17, 12, 26, 13, 22, 25, 28, 21, 28, 18, 14, 10, 10, 18, 31, 14, 14, 23, 24, 7, 16, 24, 21, 19, 9, 13, 28, 9, 9, 9, 22, 20, 30, 11, 18, 14, 27, 15, 16, 7, 13, 25, 10, 11, 30, 9, 23, 14, 18, 23, 18, 19, 30, 34, 20, 21, 10, 11, 17, 39, 7, 16, 21, 22, 13, 14, 16, 13, 18, 28, 20, 16, 12, 21, 11, 18, 16], "policy_player_1_reward": [-1.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0], "policy_player_2_reward": [1.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5305725045386278, "mean_inference_ms": 1.5074627356934125, "mean_action_processing_ms": 0.1494694040504311, "mean_env_wait_ms": 0.10807640019605272, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011376164994149838, "ViewRequirementAgentConnector_ms": 0.17657541642291846}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997, "num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 256.3930752785104, "num_env_steps_trained_throughput_per_sec": 256.3930752785104, "timesteps_total": 4000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 3997, "timers": {"training_iteration_time_ms": 15601.045, "sample_time_ms": 3142.642, "learn_time_ms": 12446.22, "learn_throughput": 321.383, "synch_weights_time_ms": 10.143}, "counters": {"num_env_steps_sampled": 4000, "num_env_steps_trained": 4000, "num_agent_steps_sampled": 3997, "num_agent_steps_trained": 3997}, "done": false, "episodes_total": 212, "training_iteration": 1, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-21-02", "timestamp": 1711732862, "time_this_iter_s": 19.677799463272095, "time_total_s": 19.677799463272095, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C187E3B0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C187E4D0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187E170>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 19.677799463272095, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 8.767857142857142, "ram_util_percent": 90.54642857142858}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.63, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.37, "player_2": 0.12745098039215685, "player_1": 0.6224489795918368}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [25, 9, 16, 36, 18, 29, 25, 11, 12, 12, 21, 27, 10, 11, 17, 15, 14, 55, 11, 26, 10, 15, 16, 34, 17, 20, 24, 16, 24, 12, 19, 19, 17, 17, 8, 12, 18, 20, 25, 20, 10, 30, 13, 28, 9, 19, 8, 21, 16, 20, 9, 23, 6, 14, 21, 19, 19, 27, 9, 8, 25, 9, 16, 18, 36, 16, 18, 35, 26, 23, 24, 13, 8, 19, 7, 30, 12, 40, 19, 18, 20, 24, 28, 26, 16, 10, 18, 8, 36, 12, 10, 13, 10, 10, 19, 9, 14, 11, 16, 7, 29, 10, 15, 12, 26, 20, 21, 10, 31, 23, 25, 26, 20, 13, 16, 8, 25, 13, 26, 28, 26, 19, 10, 56, 16, 19, 30, 7, 12, 20, 10, 20, 16, 16, 31, 44, 12, 15, 12, 15, 14, 13, 6, 14, 16, 18, 8, 13, 15, 10, 13, 9, 10, 16, 18, 25, 24, 40, 22, 18, 21, 14, 10, 24, 7, 13, 11, 20, 13, 19, 39, 8, 10, 8, 25, 21, 15, 30, 23, 57, 19, 19, 37, 22, 13, 18, 14, 14, 11, 18, 18, 12, 8, 23, 18, 15, 13, 49, 32, 19], "policy_random_reward": [-2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 1.0, -1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0], "policy_player_1_reward": [1.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.550571979910275, "mean_inference_ms": 0.8598160228077407, "mean_action_processing_ms": 0.13112128416238855, "mean_env_wait_ms": 0.08865231920446642, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008314371109008789, "ViewRequirementAgentConnector_ms": 0.23841077089309692}, "player_1_winrate": 0.7040816326530612, "player_2_winrate": 0.5588235294117647, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.787072085866741, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8238079995501275, "policy_loss": -0.027970450734902248, "vf_loss": 1.8489998169973785, "vf_explained_var": 0.20126204093297323, "kl": 0.013893196858505943, "entropy": 0.7837420137489544, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.05882352941177, "num_grad_updates_lifetime": 765.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.738107967376709, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8215978388985, "policy_loss": -0.020356762083247305, "vf_loss": 1.839408162732919, "vf_explained_var": 0.20968645413716633, "kl": 0.012732184754685297, "entropy": 0.7408244764432311, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.375, "num_grad_updates_lifetime": 720.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.307339449541285, "episode_media": {}, "episodes_this_iter": 218, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.16055045871559634, "player_2": 0.16055045871559634}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [7, 12, 19, 10, 16, 35, 22, 25, 22, 10, 17, 10, 8, 28, 19, 27, 13, 12, 20, 15, 20, 21, 13, 53, 13, 12, 14, 17, 10, 11, 10, 11, 15, 7, 12, 22, 42, 8, 25, 19, 8, 24, 27, 9, 32, 19, 10, 28, 32, 35, 25, 22, 25, 19, 11, 26, 13, 17, 16, 15, 16, 9, 7, 16, 15, 15, 21, 17, 14, 25, 11, 14, 15, 13, 17, 17, 17, 8, 19, 13, 15, 23, 25, 15, 18, 16, 22, 21, 13, 10, 20, 30, 18, 37, 20, 30, 32, 15, 29, 17, 17, 12, 14, 22, 15, 30, 10, 20, 20, 19, 13, 6, 21, 25, 27, 29, 23, 20, 18, 26, 40, 14, 33, 35, 31, 20, 23, 9, 11, 15, 22, 19, 8, 17, 22, 14, 24, 11, 10, 7, 9, 19, 12, 20, 22, 7, 27, 16, 18, 13, 13, 16, 7, 17, 33, 21, 27, 22, 13, 7, 8, 24, 20, 18, 31, 26, 16, 17, 14, 27, 22, 20, 21, 9, 23, 15, 17, 14, 8, 15, 22, 31, 22, 8, 30, 16, 19, 8, 11, 14, 19, 17, 14, 23, 9, 15, 10, 13, 26, 17, 32, 21, 16, 21, 15, 12, 11, 10, 9, 15, 22, 9, 10, 28, 28, 9, 34, 29], "policy_player_1_reward": [-2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5448943418545777, "mean_inference_ms": 1.5229557707270742, "mean_action_processing_ms": 0.14781806021539792, "mean_env_wait_ms": 0.10887982337921084, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.007301459618664663, "ViewRequirementAgentConnector_ms": 0.1905736026413944}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.307339449541285, "episodes_this_iter": 218, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.16055045871559634, "player_2": 0.16055045871559634}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [7, 12, 19, 10, 16, 35, 22, 25, 22, 10, 17, 10, 8, 28, 19, 27, 13, 12, 20, 15, 20, 21, 13, 53, 13, 12, 14, 17, 10, 11, 10, 11, 15, 7, 12, 22, 42, 8, 25, 19, 8, 24, 27, 9, 32, 19, 10, 28, 32, 35, 25, 22, 25, 19, 11, 26, 13, 17, 16, 15, 16, 9, 7, 16, 15, 15, 21, 17, 14, 25, 11, 14, 15, 13, 17, 17, 17, 8, 19, 13, 15, 23, 25, 15, 18, 16, 22, 21, 13, 10, 20, 30, 18, 37, 20, 30, 32, 15, 29, 17, 17, 12, 14, 22, 15, 30, 10, 20, 20, 19, 13, 6, 21, 25, 27, 29, 23, 20, 18, 26, 40, 14, 33, 35, 31, 20, 23, 9, 11, 15, 22, 19, 8, 17, 22, 14, 24, 11, 10, 7, 9, 19, 12, 20, 22, 7, 27, 16, 18, 13, 13, 16, 7, 17, 33, 21, 27, 22, 13, 7, 8, 24, 20, 18, 31, 26, 16, 17, 14, 27, 22, 20, 21, 9, 23, 15, 17, 14, 8, 15, 22, 31, 22, 8, 30, 16, 19, 8, 11, 14, 19, 17, 14, 23, 9, 15, 10, 13, 26, 17, 32, 21, 16, 21, 15, 12, 11, 10, 9, 15, 22, 9, 10, 28, 28, 9, 34, 29], "policy_player_1_reward": [-2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5448943418545777, "mean_inference_ms": 1.5229557707270742, "mean_action_processing_ms": 0.14781806021539792, "mean_env_wait_ms": 0.10887982337921084, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.007301459618664663, "ViewRequirementAgentConnector_ms": 0.1905736026413944}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997, "num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 195.6520692106115, "num_env_steps_trained_throughput_per_sec": 195.6520692106115, "timesteps_total": 8000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 7997, "timers": {"training_iteration_time_ms": 18022.75, "sample_time_ms": 3172.218, "learn_time_ms": 14840.205, "learn_throughput": 269.538, "synch_weights_time_ms": 9.308}, "counters": {"num_env_steps_sampled": 8000, "num_env_steps_trained": 8000, "num_agent_steps_sampled": 7997, "num_agent_steps_trained": 7997}, "done": false, "episodes_total": 430, "training_iteration": 2, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-21-27", "timestamp": 1711732887, "time_this_iter_s": 24.89843988418579, "time_total_s": 44.576239347457886, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C187EB00>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C187E830>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C180B0A0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 44.576239347457886, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 15.057142857142857, "ram_util_percent": 86.61142857142858}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.135, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.385, "player_2": 0.3584905660377358, "player_1": 0.4148936170212766}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [28, 14, 20, 12, 13, 31, 21, 22, 13, 14, 9, 20, 13, 12, 14, 9, 7, 7, 9, 24, 15, 17, 12, 34, 16, 15, 16, 10, 18, 24, 14, 7, 8, 14, 9, 19, 8, 15, 16, 18, 14, 11, 16, 11, 23, 11, 20, 14, 26, 13, 16, 25, 8, 24, 18, 24, 15, 13, 9, 12, 12, 13, 19, 13, 9, 29, 19, 8, 27, 8, 14, 8, 11, 37, 11, 7, 16, 16, 18, 10, 9, 19, 15, 10, 9, 20, 15, 21, 14, 18, 25, 16, 8, 18, 7, 12, 18, 15, 7, 19, 32, 12, 9, 10, 11, 11, 10, 12, 13, 11, 34, 27, 23, 14, 29, 8, 11, 11, 22, 7, 12, 20, 18, 10, 9, 20, 14, 19, 12, 11, 16, 12, 20, 22, 24, 9, 20, 23, 7, 27, 32, 27, 15, 26, 18, 33, 22, 10, 23, 21, 16, 16, 19, 17, 13, 31, 26, 15, 18, 20, 14, 7, 17, 20, 17, 21, 20, 15, 18, 13, 16, 9, 9, 16, 15, 9, 31, 21, 16, 13, 9, 13, 18, 23, 11, 27, 8, 10, 16, 7, 16, 25, 12, 12, 29, 13, 17, 14, 9, 21], "policy_random_reward": [1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [-1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0], "policy_player_1_reward": [-2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5695137980618297, "mean_inference_ms": 0.8726877916482348, "mean_action_processing_ms": 0.1318782562781395, "mean_env_wait_ms": 0.08650707164112355, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010952889919281006, "ViewRequirementAgentConnector_ms": 0.25618481636047363}, "player_1_winrate": 0.6276595744680851, "player_2_winrate": 0.6226415094339622, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.935953840788673, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.9388837047651701, "policy_loss": -0.025840514555902167, "vf_loss": 1.9624577822638494, "vf_explained_var": 0.15260027413274727, "kl": 0.011332207173028373, "entropy": 0.7273044986467735, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.05882352941177, "num_grad_updates_lifetime": 1275.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.951455473403136, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8129284689823786, "policy_loss": -0.025928404788525466, "vf_loss": 1.8364207131167254, "vf_explained_var": 0.20438073985278607, "kl": 0.012180764701561483, "entropy": 0.6996539700155456, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.375, "num_grad_updates_lifetime": 1200.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 11997, "num_agent_steps_trained": 11997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.054054054054053, "episode_media": {}, "episodes_this_iter": 222, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.07207207207207207, "player_2": 0.07207207207207207}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [26, 11, 30, 15, 26, 10, 7, 9, 40, 17, 23, 10, 10, 8, 23, 21, 14, 11, 16, 17, 23, 14, 43, 9, 16, 22, 17, 18, 20, 8, 19, 16, 18, 23, 16, 13, 19, 16, 7, 8, 31, 22, 16, 19, 17, 15, 31, 14, 18, 11, 22, 16, 20, 21, 8, 16, 13, 20, 23, 21, 16, 7, 31, 20, 14, 8, 27, 25, 10, 7, 19, 10, 9, 14, 14, 40, 26, 15, 15, 30, 12, 12, 16, 28, 9, 10, 12, 7, 19, 19, 18, 7, 14, 10, 22, 50, 6, 12, 6, 31, 15, 25, 31, 18, 8, 13, 34, 9, 9, 9, 8, 19, 16, 29, 21, 19, 37, 11, 17, 17, 26, 8, 14, 13, 23, 13, 13, 9, 21, 13, 19, 21, 29, 19, 11, 23, 38, 9, 28, 37, 21, 24, 17, 9, 19, 24, 19, 7, 11, 7, 26, 28, 21, 8, 19, 40, 34, 22, 16, 30, 35, 26, 15, 16, 20, 24, 16, 16, 26, 11, 11, 7, 27, 26, 17, 31, 9, 15, 12, 12, 15, 11, 10, 18, 24, 9, 24, 19, 25, 14, 22, 39, 20, 31, 11, 9, 15, 14, 13, 11, 14, 16, 12, 15, 40, 19, 11, 10, 15, 18, 25, 30, 25, 10, 9, 18, 33, 9, 21, 11, 18, 9], "policy_player_1_reward": [1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6072412971192167, "mean_inference_ms": 1.720848656491758, "mean_action_processing_ms": 0.1678555089401926, "mean_env_wait_ms": 0.1207989737453614, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.016799112697979353, "ViewRequirementAgentConnector_ms": 0.26682953576783874}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.054054054054053, "episodes_this_iter": 222, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.07207207207207207, "player_2": 0.07207207207207207}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [26, 11, 30, 15, 26, 10, 7, 9, 40, 17, 23, 10, 10, 8, 23, 21, 14, 11, 16, 17, 23, 14, 43, 9, 16, 22, 17, 18, 20, 8, 19, 16, 18, 23, 16, 13, 19, 16, 7, 8, 31, 22, 16, 19, 17, 15, 31, 14, 18, 11, 22, 16, 20, 21, 8, 16, 13, 20, 23, 21, 16, 7, 31, 20, 14, 8, 27, 25, 10, 7, 19, 10, 9, 14, 14, 40, 26, 15, 15, 30, 12, 12, 16, 28, 9, 10, 12, 7, 19, 19, 18, 7, 14, 10, 22, 50, 6, 12, 6, 31, 15, 25, 31, 18, 8, 13, 34, 9, 9, 9, 8, 19, 16, 29, 21, 19, 37, 11, 17, 17, 26, 8, 14, 13, 23, 13, 13, 9, 21, 13, 19, 21, 29, 19, 11, 23, 38, 9, 28, 37, 21, 24, 17, 9, 19, 24, 19, 7, 11, 7, 26, 28, 21, 8, 19, 40, 34, 22, 16, 30, 35, 26, 15, 16, 20, 24, 16, 16, 26, 11, 11, 7, 27, 26, 17, 31, 9, 15, 12, 12, 15, 11, 10, 18, 24, 9, 24, 19, 25, 14, 22, 39, 20, 31, 11, 9, 15, 14, 13, 11, 14, 16, 12, 15, 40, 19, 11, 10, 15, 18, 25, 30, 25, 10, 9, 18, 33, 9, 21, 11, 18, 9], "policy_player_1_reward": [1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6072412971192167, "mean_inference_ms": 1.720848656491758, "mean_action_processing_ms": 0.1678555089401926, "mean_env_wait_ms": 0.1207989737453614, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.016799112697979353, "ViewRequirementAgentConnector_ms": 0.26682953576783874}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 11997, "num_agent_steps_trained": 11997, "num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 206.53155226485885, "num_env_steps_trained_throughput_per_sec": 206.53155226485885, "timesteps_total": 12000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 11997, "timers": {"training_iteration_time_ms": 18471.001, "sample_time_ms": 3560.318, "learn_time_ms": 14901.194, "learn_throughput": 268.435, "synch_weights_time_ms": 8.809}, "counters": {"num_env_steps_sampled": 12000, "num_env_steps_trained": 12000, "num_agent_steps_sampled": 11997, "num_agent_steps_trained": 11997}, "done": false, "episodes_total": 652, "training_iteration": 3, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-21-51", "timestamp": 1711732911, "time_this_iter_s": 23.221555948257446, "time_total_s": 67.79779529571533, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C187FD90>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C187FBE0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C180A7A0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 67.79779529571533, "iterations_since_restore": 3, "perf": {"cpu_util_percent": 13.003030303030302, "ram_util_percent": 84.43030303030302}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.68, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.31, "player_2": 0.4158415841584158, "player_1": 0.20202020202020202}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [13, 13, 14, 31, 7, 13, 19, 9, 9, 37, 9, 20, 14, 8, 22, 13, 16, 25, 17, 12, 13, 31, 20, 12, 36, 20, 11, 9, 9, 40, 26, 20, 19, 9, 15, 7, 17, 18, 31, 12, 14, 28, 19, 13, 22, 25, 9, 13, 28, 10, 15, 15, 19, 8, 25, 14, 24, 12, 12, 15, 23, 24, 18, 14, 26, 20, 9, 19, 19, 17, 17, 20, 25, 14, 9, 23, 37, 22, 18, 9, 17, 9, 8, 15, 37, 20, 12, 12, 8, 18, 17, 7, 8, 29, 15, 20, 29, 21, 8, 7, 38, 14, 17, 21, 16, 13, 17, 16, 11, 17, 21, 21, 10, 9, 14, 12, 21, 38, 13, 19, 19, 12, 19, 11, 24, 12, 13, 15, 7, 23, 8, 10, 17, 32, 14, 17, 24, 15, 22, 11, 22, 21, 12, 32, 14, 16, 21, 18, 17, 16, 23, 15, 27, 32, 17, 14, 25, 13, 25, 27, 21, 18, 18, 18, 16, 24, 8, 17, 9, 10, 11, 20, 17, 21, 24, 9, 12, 13, 17, 14, 13, 13, 26, 38, 34, 19, 26, 7, 22, 20, 16, 7, 33, 24, 14, 19, 12, 18, 23, 9], "policy_random_reward": [-1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0], "policy_player_2_reward": [1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0], "policy_player_1_reward": [-2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5736893649482684, "mean_inference_ms": 0.8733493598053366, "mean_action_processing_ms": 0.13278543306145513, "mean_env_wait_ms": 0.08461372104708559, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013139486312866211, "ViewRequirementAgentConnector_ms": 0.23497223854064941}, "player_1_winrate": 0.5656565656565656, "player_2_winrate": 0.6435643564356436, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.1450928938155083, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.84389234828014, "policy_loss": -0.017655205135872844, "vf_loss": 1.8586979227907516, "vf_explained_var": 0.17221199052006592, "kl": 0.01424815989765095, "entropy": 0.6737769175686088, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.82352941176471, "num_grad_updates_lifetime": 1785.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.990451194345951, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8364776141941548, "policy_loss": -0.013448270518953602, "vf_loss": 1.8482437933484712, "vf_explained_var": 0.17555763882895312, "kl": 0.008410486353091074, "entropy": 0.6552545369913181, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.625, "num_grad_updates_lifetime": 1680.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.20909090909091, "episode_media": {}, "episodes_this_iter": 220, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.09545454545454546, "player_2": -0.09545454545454546}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [26, 20, 8, 17, 21, 18, 10, 12, 7, 13, 13, 38, 8, 16, 13, 25, 11, 30, 10, 14, 41, 18, 12, 15, 8, 26, 9, 11, 15, 14, 7, 16, 13, 24, 19, 28, 58, 14, 14, 10, 13, 33, 14, 22, 25, 16, 18, 15, 13, 22, 13, 8, 7, 13, 11, 15, 8, 19, 13, 20, 21, 32, 31, 13, 15, 27, 12, 22, 29, 13, 12, 21, 16, 17, 15, 27, 13, 8, 10, 21, 26, 8, 49, 15, 10, 16, 23, 16, 20, 22, 14, 16, 11, 13, 26, 15, 21, 16, 32, 29, 33, 8, 9, 15, 17, 12, 7, 15, 12, 26, 17, 16, 30, 20, 10, 16, 17, 13, 19, 19, 24, 14, 17, 33, 39, 24, 16, 10, 14, 17, 15, 16, 30, 19, 20, 15, 11, 26, 14, 28, 22, 12, 22, 23, 13, 16, 20, 29, 9, 10, 21, 14, 27, 25, 16, 12, 17, 26, 19, 20, 8, 20, 9, 13, 8, 15, 17, 17, 13, 16, 12, 15, 9, 32, 24, 22, 25, 23, 26, 26, 7, 10, 11, 28, 34, 22, 13, 15, 18, 6, 14, 27, 17, 24, 19, 21, 10, 25, 22, 18, 17, 18, 24, 28, 14, 13, 38, 13, 25, 15, 26, 19, 19, 18, 17, 9, 24, 10, 27, 22], "policy_player_1_reward": [1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0], "policy_player_2_reward": [-1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6149784877792923, "mean_inference_ms": 1.7432739439099096, "mean_action_processing_ms": 0.17079788839381546, "mean_env_wait_ms": 0.11934900165248134, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015094713731245562, "ViewRequirementAgentConnector_ms": 0.21000130610032516}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.20909090909091, "episodes_this_iter": 220, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.09545454545454546, "player_2": -0.09545454545454546}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [26, 20, 8, 17, 21, 18, 10, 12, 7, 13, 13, 38, 8, 16, 13, 25, 11, 30, 10, 14, 41, 18, 12, 15, 8, 26, 9, 11, 15, 14, 7, 16, 13, 24, 19, 28, 58, 14, 14, 10, 13, 33, 14, 22, 25, 16, 18, 15, 13, 22, 13, 8, 7, 13, 11, 15, 8, 19, 13, 20, 21, 32, 31, 13, 15, 27, 12, 22, 29, 13, 12, 21, 16, 17, 15, 27, 13, 8, 10, 21, 26, 8, 49, 15, 10, 16, 23, 16, 20, 22, 14, 16, 11, 13, 26, 15, 21, 16, 32, 29, 33, 8, 9, 15, 17, 12, 7, 15, 12, 26, 17, 16, 30, 20, 10, 16, 17, 13, 19, 19, 24, 14, 17, 33, 39, 24, 16, 10, 14, 17, 15, 16, 30, 19, 20, 15, 11, 26, 14, 28, 22, 12, 22, 23, 13, 16, 20, 29, 9, 10, 21, 14, 27, 25, 16, 12, 17, 26, 19, 20, 8, 20, 9, 13, 8, 15, 17, 17, 13, 16, 12, 15, 9, 32, 24, 22, 25, 23, 26, 26, 7, 10, 11, 28, 34, 22, 13, 15, 18, 6, 14, 27, 17, 24, 19, 21, 10, 25, 22, 18, 17, 18, 24, 28, 14, 13, 38, 13, 25, 15, 26, 19, 19, 18, 17, 9, 24, 10, 27, 22], "policy_player_1_reward": [1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -1.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0], "policy_player_2_reward": [-1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6149784877792923, "mean_inference_ms": 1.7432739439099096, "mean_action_processing_ms": 0.17079788839381546, "mean_env_wait_ms": 0.11934900165248134, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015094713731245562, "ViewRequirementAgentConnector_ms": 0.21000130610032516}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997, "num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 225.35727674879334, "num_env_steps_trained_throughput_per_sec": 225.35727674879334, "timesteps_total": 16000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 15997, "timers": {"training_iteration_time_ms": 18290.649, "sample_time_ms": 3639.432, "learn_time_ms": 14642.083, "learn_throughput": 273.185, "synch_weights_time_ms": 8.624}, "counters": {"num_env_steps_sampled": 16000, "num_env_steps_trained": 16000, "num_agent_steps_sampled": 15997, "num_agent_steps_trained": 15997}, "done": false, "episodes_total": 872, "training_iteration": 4, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-22-13", "timestamp": 1711732933, "time_this_iter_s": 21.77670693397522, "time_total_s": 89.57450222969055, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C13F0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C1240>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187DA20>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 89.57450222969055, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 11.349999999999998, "ram_util_percent": 84.16333333333334}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.195, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.59, "player_2": 0.5217391304347826, "player_1": 0.6823529411764706}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [19, 27, 19, 8, 20, 22, 9, 16, 13, 21, 16, 11, 13, 14, 17, 14, 12, 10, 10, 7, 20, 31, 18, 7, 8, 9, 11, 17, 8, 13, 9, 18, 27, 16, 12, 10, 15, 15, 13, 23, 21, 12, 21, 23, 29, 14, 52, 18, 18, 16, 23, 8, 14, 29, 25, 18, 31, 22, 20, 8, 12, 15, 28, 15, 11, 9, 14, 10, 21, 13, 19, 10, 15, 12, 9, 26, 15, 8, 19, 11, 7, 10, 13, 21, 19, 9, 10, 21, 15, 14, 10, 18, 15, 18, 8, 25, 9, 15, 26, 21, 8, 37, 16, 15, 27, 17, 11, 28, 32, 14, 23, 26, 30, 20, 13, 12, 7, 18, 17, 19, 21, 32, 13, 9, 11, 16, 14, 15, 12, 14, 16, 22, 32, 14, 20, 7, 9, 19, 16, 9, 14, 8, 28, 27, 19, 11, 7, 9, 19, 9, 15, 24, 17, 10, 26, 23, 25, 8, 8, 12, 20, 21, 9, 21, 22, 11, 22, 14, 14, 17, 40, 10, 7, 16, 32, 51, 21, 14, 18, 8, 16, 14, 20, 20, 21, 27, 11, 19, 25, 18, 18, 10, 49, 17, 26, 8, 31, 18, 20, 17], "policy_random_reward": [-2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -1.0, 2.0], "policy_player_2_reward": [2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -1.0], "policy_player_1_reward": [1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5799508503276691, "mean_inference_ms": 0.8802804885961851, "mean_action_processing_ms": 0.1325680498288303, "mean_env_wait_ms": 0.08524172573986177, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010969936847686768, "ViewRequirementAgentConnector_ms": 0.24200689792633057}, "player_1_winrate": 0.7058823529411765, "player_2_winrate": 0.6608695652173913, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.6403577494124573, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7674113670984903, "policy_loss": -0.015378413829512282, "vf_loss": 1.780794516702493, "vf_explained_var": 0.17953741773962975, "kl": 0.009976335426104651, "entropy": 0.6077092641343673, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.9375, "num_grad_updates_lifetime": 2280.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.020293472955624, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.780139929552873, "policy_loss": -0.02189572868325437, "vf_loss": 1.800154553602139, "vf_explained_var": 0.16368377655744554, "kl": 0.00940549722485297, "entropy": 0.615763182255129, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.0625, "num_grad_updates_lifetime": 2160.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.577319587628867, "episode_media": {}, "episodes_this_iter": 194, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.015463917525773196, "player_2": 0.015463917525773196}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 14, 45, 22, 8, 16, 18, 27, 20, 18, 11, 32, 26, 26, 33, 20, 21, 24, 17, 19, 21, 19, 19, 36, 15, 13, 24, 9, 15, 9, 13, 25, 9, 29, 34, 13, 9, 42, 15, 27, 8, 17, 16, 9, 17, 16, 28, 20, 22, 10, 62, 12, 36, 18, 30, 31, 13, 14, 13, 17, 24, 16, 9, 14, 23, 23, 10, 20, 13, 36, 8, 13, 22, 19, 8, 18, 22, 34, 10, 17, 11, 31, 47, 17, 16, 19, 30, 17, 13, 7, 16, 44, 13, 30, 18, 12, 19, 16, 13, 23, 22, 28, 27, 14, 11, 8, 15, 12, 13, 54, 24, 17, 23, 15, 25, 8, 39, 19, 17, 30, 17, 26, 14, 25, 29, 13, 20, 19, 29, 30, 21, 12, 10, 13, 17, 22, 27, 14, 43, 24, 16, 24, 17, 18, 19, 10, 20, 30, 15, 20, 51, 16, 17, 8, 34, 29, 29, 15, 17, 15, 13, 17, 44, 20, 18, 14, 20, 21, 11, 46, 13, 21, 24, 23, 26, 17, 13, 26, 22, 13, 25, 15, 48, 32, 14, 14, 16, 25, 24, 23, 8, 8, 28, 18], "policy_player_1_reward": [-2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0], "policy_player_2_reward": [2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6083998535822793, "mean_inference_ms": 1.825699836963829, "mean_action_processing_ms": 0.17138798957601942, "mean_env_wait_ms": 0.11701542243857907, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010972662070362838, "ViewRequirementAgentConnector_ms": 0.2037182296674276}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.577319587628867, "episodes_this_iter": 194, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.015463917525773196, "player_2": 0.015463917525773196}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 14, 45, 22, 8, 16, 18, 27, 20, 18, 11, 32, 26, 26, 33, 20, 21, 24, 17, 19, 21, 19, 19, 36, 15, 13, 24, 9, 15, 9, 13, 25, 9, 29, 34, 13, 9, 42, 15, 27, 8, 17, 16, 9, 17, 16, 28, 20, 22, 10, 62, 12, 36, 18, 30, 31, 13, 14, 13, 17, 24, 16, 9, 14, 23, 23, 10, 20, 13, 36, 8, 13, 22, 19, 8, 18, 22, 34, 10, 17, 11, 31, 47, 17, 16, 19, 30, 17, 13, 7, 16, 44, 13, 30, 18, 12, 19, 16, 13, 23, 22, 28, 27, 14, 11, 8, 15, 12, 13, 54, 24, 17, 23, 15, 25, 8, 39, 19, 17, 30, 17, 26, 14, 25, 29, 13, 20, 19, 29, 30, 21, 12, 10, 13, 17, 22, 27, 14, 43, 24, 16, 24, 17, 18, 19, 10, 20, 30, 15, 20, 51, 16, 17, 8, 34, 29, 29, 15, 17, 15, 13, 17, 44, 20, 18, 14, 20, 21, 11, 46, 13, 21, 24, 23, 26, 17, 13, 26, 22, 13, 25, 15, 48, 32, 14, 14, 16, 25, 24, 23, 8, 8, 28, 18], "policy_player_1_reward": [-2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0], "policy_player_2_reward": [2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6083998535822793, "mean_inference_ms": 1.825699836963829, "mean_action_processing_ms": 0.17138798957601942, "mean_env_wait_ms": 0.11701542243857907, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010972662070362838, "ViewRequirementAgentConnector_ms": 0.2037182296674276}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997, "num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 236.10969636011268, "num_env_steps_trained_throughput_per_sec": 236.10969636011268, "timesteps_total": 20000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 19997, "timers": {"training_iteration_time_ms": 18020.775, "sample_time_ms": 3764.022, "learn_time_ms": 14247.413, "learn_throughput": 280.753, "synch_weights_time_ms": 8.565}, "counters": {"num_env_steps_sampled": 20000, "num_env_steps_trained": 20000, "num_agent_steps_sampled": 19997, "num_agent_steps_trained": 19997}, "done": false, "episodes_total": 1066, "training_iteration": 5, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-22-34", "timestamp": 1711732954, "time_this_iter_s": 21.163338899612427, "time_total_s": 110.73784112930298, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C23B0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C1BD0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187E170>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 110.73784112930298, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 10.83, "ram_util_percent": 86.08666666666667}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.81, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.7647058823529411, "random": -0.655, "player_2": 0.5408163265306123}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [13, 19, 35, 10, 19, 15, 9, 10, 20, 50, 18, 11, 10, 12, 18, 13, 10, 13, 26, 7, 26, 12, 17, 19, 16, 16, 8, 14, 25, 16, 13, 17, 13, 12, 22, 16, 22, 18, 27, 12, 15, 13, 29, 16, 16, 21, 15, 7, 9, 16, 13, 20, 18, 9, 16, 14, 11, 19, 18, 20, 20, 16, 13, 12, 8, 23, 16, 18, 11, 16, 12, 9, 10, 12, 10, 19, 21, 18, 28, 20, 8, 10, 14, 20, 15, 41, 23, 26, 14, 13, 20, 16, 15, 20, 24, 10, 7, 11, 9, 40, 17, 22, 10, 11, 7, 15, 20, 15, 8, 28, 9, 11, 18, 15, 14, 24, 14, 8, 14, 22, 29, 17, 25, 20, 14, 20, 9, 18, 20, 13, 14, 16, 32, 31, 18, 18, 17, 14, 10, 18, 17, 15, 14, 13, 15, 14, 27, 20, 23, 25, 25, 18, 20, 12, 15, 20, 13, 20, 12, 10, 24, 11, 15, 13, 21, 11, 19, 10, 23, 15, 33, 23, 16, 7, 25, 11, 38, 11, 15, 10, 15, 36, 24, 11, 10, 13, 14, 47, 7, 19, 19, 7, 13, 12, 15, 8, 20, 13, 7, 16], "policy_player_1_reward": [-2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0], "policy_random_reward": [2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0], "policy_player_2_reward": [1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5896065843338271, "mean_inference_ms": 0.9065426803459473, "mean_action_processing_ms": 0.13711855754960908, "mean_env_wait_ms": 0.08820963647374992, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011361896991729736, "ViewRequirementAgentConnector_ms": 0.27805018424987793}, "player_1_winrate": 0.7156862745098039, "player_2_winrate": 0.6632653061224489, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.689519507189592, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7857642595966656, "policy_loss": -0.010274766303579479, "vf_loss": 1.7948559438188871, "vf_explained_var": 0.17071953328947226, "kl": 0.005915411401841666, "entropy": 0.5846849933887521, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.625, "num_grad_updates_lifetime": 2760.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 2.954574739684661, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.802446971088648, "policy_loss": -0.020413763390873405, "vf_loss": 1.821249578644832, "vf_explained_var": 0.14522088492910068, "kl": 0.008055762881725088, "entropy": 0.5788750547915698, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.375, "num_grad_updates_lifetime": 2640.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.675257731958762, "episode_media": {}, "episodes_this_iter": 194, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.21649484536082475, "player_2": -0.21649484536082475}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 32, 32, 23, 22, 47, 26, 19, 13, 31, 20, 18, 17, 20, 11, 8, 16, 38, 18, 10, 16, 30, 26, 51, 16, 34, 34, 28, 30, 13, 17, 18, 12, 20, 29, 22, 16, 13, 42, 9, 14, 17, 20, 31, 13, 30, 30, 17, 17, 16, 18, 27, 16, 25, 9, 14, 18, 31, 30, 20, 11, 18, 18, 13, 22, 22, 34, 18, 39, 22, 16, 15, 31, 22, 16, 22, 20, 14, 24, 32, 24, 23, 16, 33, 9, 14, 24, 10, 25, 16, 18, 30, 23, 9, 18, 20, 23, 13, 20, 19, 24, 25, 29, 17, 18, 19, 16, 16, 13, 28, 15, 28, 14, 27, 22, 13, 18, 17, 18, 16, 21, 19, 17, 18, 21, 23, 16, 19, 15, 8, 14, 19, 22, 17, 30, 19, 26, 10, 12, 16, 34, 13, 16, 35, 20, 30, 24, 22, 13, 17, 38, 24, 19, 19, 10, 22, 36, 19, 12, 51, 19, 18, 10, 17, 28, 17, 15, 13, 11, 51, 15, 14, 19, 13, 34, 15, 12, 8, 22, 8, 18, 16, 17, 13, 56, 17, 14, 15, 14, 19, 16, 19, 19, 23], "policy_player_1_reward": [-1.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -1.0], "policy_player_2_reward": [1.0, -1.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6052474022647104, "mean_inference_ms": 1.8046063005773758, "mean_action_processing_ms": 0.1707907110588522, "mean_env_wait_ms": 0.11635553803894319, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011815238244754752, "ViewRequirementAgentConnector_ms": 0.20551730677024604}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.675257731958762, "episodes_this_iter": 194, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.21649484536082475, "player_2": -0.21649484536082475}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 32, 32, 23, 22, 47, 26, 19, 13, 31, 20, 18, 17, 20, 11, 8, 16, 38, 18, 10, 16, 30, 26, 51, 16, 34, 34, 28, 30, 13, 17, 18, 12, 20, 29, 22, 16, 13, 42, 9, 14, 17, 20, 31, 13, 30, 30, 17, 17, 16, 18, 27, 16, 25, 9, 14, 18, 31, 30, 20, 11, 18, 18, 13, 22, 22, 34, 18, 39, 22, 16, 15, 31, 22, 16, 22, 20, 14, 24, 32, 24, 23, 16, 33, 9, 14, 24, 10, 25, 16, 18, 30, 23, 9, 18, 20, 23, 13, 20, 19, 24, 25, 29, 17, 18, 19, 16, 16, 13, 28, 15, 28, 14, 27, 22, 13, 18, 17, 18, 16, 21, 19, 17, 18, 21, 23, 16, 19, 15, 8, 14, 19, 22, 17, 30, 19, 26, 10, 12, 16, 34, 13, 16, 35, 20, 30, 24, 22, 13, 17, 38, 24, 19, 19, 10, 22, 36, 19, 12, 51, 19, 18, 10, 17, 28, 17, 15, 13, 11, 51, 15, 14, 19, 13, 34, 15, 12, 8, 22, 8, 18, 16, 17, 13, 56, 17, 14, 15, 14, 19, 16, 19, 19, 23], "policy_player_1_reward": [-1.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -1.0], "policy_player_2_reward": [1.0, -1.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6052474022647104, "mean_inference_ms": 1.8046063005773758, "mean_action_processing_ms": 0.1707907110588522, "mean_env_wait_ms": 0.11635553803894319, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011815238244754752, "ViewRequirementAgentConnector_ms": 0.20551730677024604}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997, "num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 227.0784038563066, "num_env_steps_trained_throughput_per_sec": 227.0784038563066, "timesteps_total": 24000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 23997, "timers": {"training_iteration_time_ms": 17953.156, "sample_time_ms": 3726.481, "learn_time_ms": 14217.013, "learn_throughput": 281.353, "synch_weights_time_ms": 8.807}, "counters": {"num_env_steps_sampled": 24000, "num_env_steps_trained": 24000, "num_agent_steps_sampled": 23997, "num_agent_steps_trained": 23997}, "done": false, "episodes_total": 1260, "training_iteration": 6, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-22-56", "timestamp": 1711732976, "time_this_iter_s": 22.162468433380127, "time_total_s": 132.9003095626831, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C1870>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C2320>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187E3B0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 132.9003095626831, "iterations_since_restore": 6, "perf": {"cpu_util_percent": 10.683870967741937, "ram_util_percent": 87.96129032258064}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.69, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.715, "player_2": 0.6923076923076923, "player_1": 0.7395833333333334}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [18, 22, 13, 17, 15, 28, 11, 8, 15, 13, 14, 13, 10, 10, 20, 14, 11, 8, 20, 12, 17, 9, 8, 25, 20, 10, 30, 9, 24, 14, 10, 9, 18, 7, 14, 7, 8, 9, 11, 13, 18, 15, 15, 26, 12, 13, 15, 21, 10, 9, 14, 14, 9, 14, 14, 7, 24, 31, 27, 22, 8, 25, 25, 11, 13, 7, 18, 18, 12, 22, 23, 15, 9, 26, 13, 13, 16, 9, 26, 7, 9, 18, 12, 13, 8, 13, 17, 13, 9, 32, 9, 13, 12, 28, 11, 28, 9, 16, 12, 15, 11, 13, 12, 31, 16, 22, 19, 13, 9, 12, 14, 31, 16, 19, 8, 14, 10, 21, 15, 12, 6, 12, 17, 9, 18, 18, 12, 14, 17, 13, 18, 15, 22, 20, 19, 10, 13, 12, 18, 8, 11, 27, 13, 20, 19, 12, 23, 20, 12, 8, 11, 19, 43, 13, 9, 13, 15, 10, 8, 15, 12, 10, 12, 16, 30, 25, 14, 11, 19, 27, 9, 29, 17, 12, 15, 34, 10, 11, 43, 9, 10, 12, 20, 9, 8, 13, 11, 16, 16, 17, 12, 12, 41, 15, 13, 22, 18, 8, 42, 21], "policy_random_reward": [2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [-2.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, -1.0, 1.0], "policy_player_1_reward": [-2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5992333342677426, "mean_inference_ms": 0.9242353627825761, "mean_action_processing_ms": 0.1381550818525549, "mean_env_wait_ms": 0.08916781749066689, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013195931911468506, "ViewRequirementAgentConnector_ms": 0.27092140913009644}, "player_1_winrate": 0.71875, "player_2_winrate": 0.7019230769230769, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7165742011631235, "cur_kl_coeff": 0.2, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.9232654571533203, "policy_loss": -0.02110831454560599, "vf_loss": 1.9422687573760165, "vf_explained_var": 0.19579374416201722, "kl": 0.010525030432911666, "entropy": 0.5327487140309577, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.94117647058823, "num_grad_updates_lifetime": 3255.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.378076195716858, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.976912935078144, "policy_loss": -0.01447647701133974, "vf_loss": 1.9901622086763382, "vf_explained_var": 0.18859214757879575, "kl": 0.006136075514951889, "entropy": 0.5511241255948941, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.5, "num_grad_updates_lifetime": 3120.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.16346153846154, "episode_media": {}, "episodes_this_iter": 208, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.14903846153846154, "player_2": 0.14903846153846154}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [20, 30, 14, 7, 17, 21, 34, 28, 41, 14, 12, 16, 18, 20, 20, 25, 25, 19, 23, 18, 33, 24, 10, 14, 19, 17, 16, 40, 15, 16, 20, 17, 10, 12, 18, 15, 22, 19, 16, 18, 13, 24, 24, 8, 21, 20, 16, 11, 22, 17, 25, 14, 9, 19, 22, 20, 18, 16, 21, 8, 41, 13, 14, 15, 16, 6, 12, 23, 10, 19, 14, 34, 28, 19, 20, 13, 13, 20, 23, 23, 23, 62, 24, 17, 13, 27, 28, 9, 37, 15, 27, 16, 22, 13, 19, 14, 15, 23, 18, 13, 8, 19, 13, 10, 9, 10, 27, 18, 18, 16, 15, 19, 13, 17, 36, 21, 31, 26, 31, 9, 13, 16, 13, 16, 25, 16, 17, 21, 23, 28, 11, 13, 22, 13, 16, 19, 33, 19, 37, 26, 17, 21, 9, 25, 21, 13, 19, 24, 13, 13, 35, 28, 10, 11, 16, 11, 15, 8, 17, 23, 29, 23, 16, 13, 15, 24, 35, 10, 14, 26, 29, 10, 12, 18, 28, 21, 17, 24, 13, 18, 8, 22, 14, 19, 25, 9, 8, 17, 13, 29, 18, 37, 13, 23, 25, 13, 17, 30, 13, 16, 33, 18, 28, 16, 21, 8, 21, 13], "policy_player_1_reward": [2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [-2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6116645665097091, "mean_inference_ms": 1.8272321992801728, "mean_action_processing_ms": 0.17313042944928297, "mean_env_wait_ms": 0.11719951727959108, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00958368182182312, "ViewRequirementAgentConnector_ms": 0.23929534050134513}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.16346153846154, "episodes_this_iter": 208, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.14903846153846154, "player_2": 0.14903846153846154}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [20, 30, 14, 7, 17, 21, 34, 28, 41, 14, 12, 16, 18, 20, 20, 25, 25, 19, 23, 18, 33, 24, 10, 14, 19, 17, 16, 40, 15, 16, 20, 17, 10, 12, 18, 15, 22, 19, 16, 18, 13, 24, 24, 8, 21, 20, 16, 11, 22, 17, 25, 14, 9, 19, 22, 20, 18, 16, 21, 8, 41, 13, 14, 15, 16, 6, 12, 23, 10, 19, 14, 34, 28, 19, 20, 13, 13, 20, 23, 23, 23, 62, 24, 17, 13, 27, 28, 9, 37, 15, 27, 16, 22, 13, 19, 14, 15, 23, 18, 13, 8, 19, 13, 10, 9, 10, 27, 18, 18, 16, 15, 19, 13, 17, 36, 21, 31, 26, 31, 9, 13, 16, 13, 16, 25, 16, 17, 21, 23, 28, 11, 13, 22, 13, 16, 19, 33, 19, 37, 26, 17, 21, 9, 25, 21, 13, 19, 24, 13, 13, 35, 28, 10, 11, 16, 11, 15, 8, 17, 23, 29, 23, 16, 13, 15, 24, 35, 10, 14, 26, 29, 10, 12, 18, 28, 21, 17, 24, 13, 18, 8, 22, 14, 19, 25, 9, 8, 17, 13, 29, 18, 37, 13, 23, 25, 13, 17, 30, 13, 16, 33, 18, 28, 16, 21, 8, 21, 13], "policy_player_1_reward": [2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [-2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6116645665097091, "mean_inference_ms": 1.8272321992801728, "mean_action_processing_ms": 0.17313042944928297, "mean_env_wait_ms": 0.11719951727959108, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00958368182182312, "ViewRequirementAgentConnector_ms": 0.23929534050134513}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997, "num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 238.7205256798197, "num_env_steps_trained_throughput_per_sec": 238.7205256798197, "timesteps_total": 28000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 27997, "timers": {"training_iteration_time_ms": 17782.133, "sample_time_ms": 3765.269, "learn_time_ms": 14007.4, "learn_throughput": 285.563, "synch_weights_time_ms": 8.732}, "counters": {"num_env_steps_sampled": 28000, "num_env_steps_trained": 28000, "num_agent_steps_sampled": 27997, "num_agent_steps_trained": 27997}, "done": false, "episodes_total": 1468, "training_iteration": 7, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-23-17", "timestamp": 1711732997, "time_this_iter_s": 21.127545595169067, "time_total_s": 154.02785515785217, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C3400>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C3370>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187F6D0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 154.02785515785217, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 9.956666666666667, "ram_util_percent": 89.03666666666666}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.03, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.660377358490566, "random": -0.69, "player_2": 0.723404255319149}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [32, 11, 24, 11, 9, 8, 44, 15, 28, 22, 12, 11, 8, 8, 11, 15, 19, 25, 9, 13, 20, 19, 12, 19, 10, 27, 9, 12, 23, 15, 12, 7, 13, 12, 13, 7, 29, 23, 16, 10, 12, 14, 22, 34, 38, 8, 20, 19, 14, 12, 11, 17, 14, 18, 19, 18, 10, 32, 9, 7, 31, 16, 13, 7, 12, 32, 10, 19, 14, 20, 9, 14, 11, 13, 11, 15, 11, 7, 13, 13, 15, 10, 20, 11, 19, 21, 20, 38, 8, 16, 13, 14, 13, 24, 15, 10, 9, 36, 18, 25, 33, 11, 9, 15, 20, 13, 19, 8, 23, 6, 14, 14, 9, 21, 12, 9, 19, 12, 11, 26, 14, 14, 8, 20, 22, 8, 20, 8, 11, 26, 8, 21, 14, 20, 8, 15, 19, 23, 14, 8, 28, 30, 22, 12, 7, 9, 38, 13, 12, 19, 20, 23, 8, 28, 14, 17, 11, 11, 9, 19, 8, 15, 7, 21, 15, 22, 16, 13, 12, 18, 9, 10, 8, 12, 11, 30, 13, 23, 14, 14, 33, 15, 9, 14, 14, 15, 21, 11, 12, 35, 28, 10, 8, 9, 10, 14, 21, 7, 24, 13], "policy_player_1_reward": [1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0], "policy_random_reward": [-1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0], "policy_player_2_reward": [1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6023288861093633, "mean_inference_ms": 0.9334066015431562, "mean_action_processing_ms": 0.13885495223573638, "mean_env_wait_ms": 0.09004415687504069, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009365439414978027, "ViewRequirementAgentConnector_ms": 0.2623213529586792}, "player_1_winrate": 0.7169811320754716, "player_2_winrate": 0.723404255319149, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.157188143581152, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8752123485008876, "policy_loss": -0.012427333207112194, "vf_loss": 1.8867584663132826, "vf_explained_var": 0.20243012371162575, "kl": 0.004406050658325319, "entropy": 0.5332236707831423, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.75, "num_grad_updates_lifetime": 3750.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.401200566192468, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8023880630731584, "policy_loss": -0.01991453106068851, "vf_loss": 1.8202121930817763, "vf_explained_var": 0.21739946715533734, "kl": 0.010451985467140039, "entropy": 0.5151064388453961, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.25, "num_grad_updates_lifetime": 3600.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.377551020408163, "episode_media": {}, "episodes_this_iter": 196, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.1326530612244898, "player_2": -0.1326530612244898}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [22, 19, 11, 34, 9, 25, 19, 19, 26, 42, 55, 30, 22, 22, 34, 13, 26, 24, 16, 10, 16, 36, 16, 18, 22, 18, 26, 30, 13, 32, 11, 10, 19, 15, 10, 11, 11, 16, 13, 24, 28, 60, 11, 13, 24, 16, 23, 11, 26, 15, 13, 30, 26, 10, 13, 14, 33, 13, 46, 13, 23, 20, 11, 10, 21, 26, 16, 29, 15, 10, 13, 29, 21, 30, 8, 13, 16, 22, 12, 10, 24, 18, 24, 25, 52, 14, 13, 21, 8, 10, 14, 30, 13, 12, 31, 16, 32, 25, 10, 18, 26, 26, 19, 10, 12, 19, 29, 18, 21, 23, 14, 8, 19, 14, 20, 25, 19, 48, 17, 35, 12, 18, 27, 54, 34, 9, 36, 17, 22, 16, 39, 13, 31, 8, 15, 30, 23, 18, 17, 22, 10, 13, 23, 10, 8, 13, 18, 16, 14, 8, 11, 21, 31, 8, 11, 42, 16, 14, 14, 13, 29, 13, 16, 13, 21, 23, 13, 10, 13, 25, 22, 18, 38, 10, 39, 21, 13, 28, 22, 25, 14, 23, 19, 27, 13, 9, 14, 30, 18, 30, 13, 27, 18, 25, 13, 44], "policy_player_1_reward": [2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0], "policy_player_2_reward": [-2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6064719735525363, "mean_inference_ms": 1.807285254295715, "mean_action_processing_ms": 0.17167657827917893, "mean_env_wait_ms": 0.11695894508665503, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009847234706489407, "ViewRequirementAgentConnector_ms": 0.20276739889261675}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.377551020408163, "episodes_this_iter": 196, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.1326530612244898, "player_2": -0.1326530612244898}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [22, 19, 11, 34, 9, 25, 19, 19, 26, 42, 55, 30, 22, 22, 34, 13, 26, 24, 16, 10, 16, 36, 16, 18, 22, 18, 26, 30, 13, 32, 11, 10, 19, 15, 10, 11, 11, 16, 13, 24, 28, 60, 11, 13, 24, 16, 23, 11, 26, 15, 13, 30, 26, 10, 13, 14, 33, 13, 46, 13, 23, 20, 11, 10, 21, 26, 16, 29, 15, 10, 13, 29, 21, 30, 8, 13, 16, 22, 12, 10, 24, 18, 24, 25, 52, 14, 13, 21, 8, 10, 14, 30, 13, 12, 31, 16, 32, 25, 10, 18, 26, 26, 19, 10, 12, 19, 29, 18, 21, 23, 14, 8, 19, 14, 20, 25, 19, 48, 17, 35, 12, 18, 27, 54, 34, 9, 36, 17, 22, 16, 39, 13, 31, 8, 15, 30, 23, 18, 17, 22, 10, 13, 23, 10, 8, 13, 18, 16, 14, 8, 11, 21, 31, 8, 11, 42, 16, 14, 14, 13, 29, 13, 16, 13, 21, 23, 13, 10, 13, 25, 22, 18, 38, 10, 39, 21, 13, 28, 22, 25, 14, 23, 19, 27, 13, 9, 14, 30, 18, 30, 13, 27, 18, 25, 13, 44], "policy_player_1_reward": [2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0], "policy_player_2_reward": [-2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6064719735525363, "mean_inference_ms": 1.807285254295715, "mean_action_processing_ms": 0.17167657827917893, "mean_env_wait_ms": 0.11695894508665503, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009847234706489407, "ViewRequirementAgentConnector_ms": 0.20276739889261675}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997, "num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 261.9271284302711, "num_env_steps_trained_throughput_per_sec": 261.9271284302711, "timesteps_total": 32000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 31997, "timers": {"training_iteration_time_ms": 17468.294, "sample_time_ms": 3721.168, "learn_time_ms": 13737.787, "learn_throughput": 291.168, "synch_weights_time_ms": 8.698}, "counters": {"num_env_steps_sampled": 32000, "num_env_steps_trained": 32000, "num_agent_steps_sampled": 31997, "num_agent_steps_trained": 31997}, "done": false, "episodes_total": 1664, "training_iteration": 8, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-23-37", "timestamp": 1711733017, "time_this_iter_s": 19.543689489364624, "time_total_s": 173.5715446472168, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C1EA0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C1E10>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187E830>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 173.5715446472168, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 10.792592592592595, "ram_util_percent": 88.31851851851852}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.74, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.3854166666666667, "random": -0.665, "player_2": 0.9230769230769231}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [12, 18, 34, 10, 23, 10, 13, 19, 11, 11, 20, 29, 13, 22, 25, 12, 21, 8, 9, 7, 22, 16, 21, 25, 13, 25, 14, 9, 11, 7, 14, 30, 24, 8, 17, 31, 16, 12, 15, 16, 18, 20, 8, 16, 14, 20, 8, 16, 19, 14, 12, 13, 8, 19, 10, 15, 8, 24, 9, 13, 11, 7, 13, 11, 9, 27, 9, 15, 15, 19, 27, 12, 21, 10, 7, 21, 12, 10, 15, 23, 16, 11, 11, 13, 24, 12, 8, 20, 9, 14, 13, 10, 13, 20, 21, 23, 12, 11, 18, 9, 25, 13, 19, 21, 14, 22, 16, 8, 8, 16, 15, 8, 23, 9, 12, 13, 14, 8, 31, 9, 11, 23, 10, 18, 23, 13, 12, 11, 19, 15, 13, 23, 20, 36, 12, 7, 10, 17, 11, 12, 21, 19, 8, 23, 13, 25, 14, 10, 25, 14, 9, 17, 7, 12, 17, 18, 11, 13, 26, 19, 15, 31, 13, 11, 19, 10, 9, 7, 23, 26, 16, 13, 16, 17, 17, 27, 11, 7, 9, 31, 18, 11, 21, 17, 13, 13, 19, 13, 19, 20, 15, 24, 9, 16, 11, 25, 15, 30, 22, 9], "policy_player_1_reward": [2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0], "policy_random_reward": [-2.0, -1.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0], "policy_player_2_reward": [1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6018114177047873, "mean_inference_ms": 0.9283626740416457, "mean_action_processing_ms": 0.1390143608156908, "mean_env_wait_ms": 0.08880576603557688, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013422966003417969, "ViewRequirementAgentConnector_ms": 0.24690526723861694}, "player_1_winrate": 0.6145833333333334, "player_2_winrate": 0.7788461538461539, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.1241796189663456, "cur_kl_coeff": 0.1, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8292013398134241, "policy_loss": -0.0128549140658887, "vf_loss": 1.841575749189246, "vf_explained_var": 0.19256789906352173, "kl": 0.004805072381059283, "entropy": 0.5272312003607843, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.58823529411765, "num_grad_updates_lifetime": 4245.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.5423656289776164, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8316404489179452, "policy_loss": -0.01928237018970928, "vf_loss": 1.8495035228629908, "vf_explained_var": 0.20745882466435434, "kl": 0.00709648043606929, "entropy": 0.477479146917661, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.875, "num_grad_updates_lifetime": 4080.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.802083333333332, "episode_media": {}, "episodes_this_iter": 192, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.203125, "player_2": 0.203125}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 19, 19, 13, 12, 40, 25, 10, 18, 10, 22, 10, 48, 50, 32, 32, 31, 16, 13, 14, 16, 40, 16, 11, 17, 51, 19, 16, 55, 29, 20, 12, 21, 18, 9, 12, 32, 22, 14, 13, 28, 21, 32, 28, 15, 19, 16, 13, 16, 16, 18, 10, 50, 30, 25, 17, 16, 14, 13, 13, 18, 9, 19, 13, 13, 27, 19, 21, 55, 33, 9, 18, 13, 11, 13, 8, 21, 19, 7, 8, 15, 28, 47, 34, 19, 25, 28, 25, 31, 15, 24, 46, 20, 19, 32, 20, 16, 12, 41, 9, 19, 18, 13, 37, 20, 21, 12, 27, 15, 21, 16, 13, 19, 23, 27, 17, 17, 10, 18, 18, 13, 17, 30, 19, 25, 21, 22, 20, 16, 9, 11, 21, 24, 28, 23, 16, 16, 15, 13, 23, 37, 21, 9, 20, 13, 41, 19, 22, 36, 17, 33, 44, 17, 10, 26, 19, 37, 32, 42, 14, 10, 15, 19, 7, 16, 16, 13, 20, 13, 16, 14, 30, 24, 13, 9, 16, 13, 28, 30, 17, 34, 31, 12, 19, 15, 16, 9, 25, 18, 12, 20, 16], "policy_player_1_reward": [-2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6078410775661323, "mean_inference_ms": 1.8066047652846338, "mean_action_processing_ms": 0.1725043906337104, "mean_env_wait_ms": 0.11706151514746672, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012228575845559439, "ViewRequirementAgentConnector_ms": 0.22720415145158768}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.802083333333332, "episodes_this_iter": 192, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.203125, "player_2": 0.203125}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 19, 19, 13, 12, 40, 25, 10, 18, 10, 22, 10, 48, 50, 32, 32, 31, 16, 13, 14, 16, 40, 16, 11, 17, 51, 19, 16, 55, 29, 20, 12, 21, 18, 9, 12, 32, 22, 14, 13, 28, 21, 32, 28, 15, 19, 16, 13, 16, 16, 18, 10, 50, 30, 25, 17, 16, 14, 13, 13, 18, 9, 19, 13, 13, 27, 19, 21, 55, 33, 9, 18, 13, 11, 13, 8, 21, 19, 7, 8, 15, 28, 47, 34, 19, 25, 28, 25, 31, 15, 24, 46, 20, 19, 32, 20, 16, 12, 41, 9, 19, 18, 13, 37, 20, 21, 12, 27, 15, 21, 16, 13, 19, 23, 27, 17, 17, 10, 18, 18, 13, 17, 30, 19, 25, 21, 22, 20, 16, 9, 11, 21, 24, 28, 23, 16, 16, 15, 13, 23, 37, 21, 9, 20, 13, 41, 19, 22, 36, 17, 33, 44, 17, 10, 26, 19, 37, 32, 42, 14, 10, 15, 19, 7, 16, 16, 13, 20, 13, 16, 14, 30, 24, 13, 9, 16, 13, 28, 30, 17, 34, 31, 12, 19, 15, 16, 9, 25, 18, 12, 20, 16], "policy_player_1_reward": [-2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6078410775661323, "mean_inference_ms": 1.8066047652846338, "mean_action_processing_ms": 0.1725043906337104, "mean_env_wait_ms": 0.11706151514746672, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012228575845559439, "ViewRequirementAgentConnector_ms": 0.22720415145158768}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997, "num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 233.9672378125236, "num_env_steps_trained_throughput_per_sec": 233.9672378125236, "timesteps_total": 36000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 35997, "timers": {"training_iteration_time_ms": 17426.974, "sample_time_ms": 3721.934, "learn_time_ms": 13695.836, "learn_throughput": 292.06, "synch_weights_time_ms": 8.522}, "counters": {"num_env_steps_sampled": 36000, "num_env_steps_trained": 36000, "num_agent_steps_sampled": 35997, "num_agent_steps_trained": 35997}, "done": false, "episodes_total": 1856, "training_iteration": 9, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-23-58", "timestamp": 1711733038, "time_this_iter_s": 20.82273578643799, "time_total_s": 194.39428043365479, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C3D90>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C19C13F0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C19C36D0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 194.39428043365479, "iterations_since_restore": 9, "perf": {"cpu_util_percent": 11.344827586206899, "ram_util_percent": 88.32413793103451}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.065, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.605, "player_2": 0.6818181818181818, "player_1": 0.5111111111111111}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [29, 18, 7, 10, 15, 21, 16, 11, 13, 20, 11, 30, 15, 12, 7, 19, 12, 17, 14, 11, 13, 28, 13, 9, 11, 12, 9, 16, 8, 7, 9, 8, 25, 7, 22, 16, 10, 8, 9, 13, 21, 33, 16, 7, 20, 15, 7, 21, 15, 13, 16, 9, 16, 12, 24, 16, 12, 19, 13, 8, 13, 34, 11, 16, 27, 14, 8, 12, 11, 8, 22, 10, 16, 26, 16, 26, 18, 13, 22, 9, 16, 9, 8, 8, 12, 12, 14, 10, 9, 11, 14, 12, 16, 26, 17, 8, 27, 11, 9, 17, 22, 9, 13, 7, 22, 26, 9, 13, 10, 18, 11, 13, 17, 21, 27, 16, 11, 18, 17, 9, 22, 19, 14, 22, 19, 8, 10, 19, 10, 13, 18, 10, 21, 15, 17, 9, 10, 13, 23, 11, 16, 17, 14, 11, 17, 16, 25, 13, 8, 19, 11, 9, 17, 22, 13, 42, 16, 12, 9, 32, 14, 7, 22, 8, 13, 17, 7, 9, 19, 23, 17, 13, 13, 7, 10, 15, 18, 13, 12, 13, 18, 11, 15, 14, 12, 9, 10, 29, 12, 14, 19, 26, 13, 15, 19, 12, 22, 13, 15, 22], "policy_random_reward": [-1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0], "policy_player_2_reward": [1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, -1.0, 1.0], "policy_player_1_reward": [2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6012473440059957, "mean_inference_ms": 0.9226303470806207, "mean_action_processing_ms": 0.13905141246218986, "mean_env_wait_ms": 0.08835258085829441, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009742498397827148, "ViewRequirementAgentConnector_ms": 0.2316492199897766}, "player_1_winrate": 0.6555555555555556, "player_2_winrate": 0.6909090909090909, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.241230069597562, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5997255966067314, "policy_loss": -0.014964794972062616, "vf_loss": 1.61426479468743, "vf_explained_var": 0.3048016397903363, "kl": 0.008512081171883753, "entropy": 0.45127000684539476, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.6875, "num_grad_updates_lifetime": 4740.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.659355904906988, "cur_kl_coeff": 0.20000000000000004, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5088604929546514, "policy_loss": -0.015956910568638703, "vf_loss": 1.5238365466396013, "vf_explained_var": 0.333381154636542, "kl": 0.004904286174921242, "entropy": 0.4715980619813005, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.3125, "num_grad_updates_lifetime": 4560.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.931428571428572, "episode_media": {}, "episodes_this_iter": 175, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.005714285714285714, "player_2": -0.005714285714285714}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [20, 17, 13, 29, 23, 9, 10, 28, 12, 19, 18, 19, 14, 36, 19, 10, 26, 38, 50, 13, 9, 9, 17, 22, 30, 16, 8, 37, 48, 12, 13, 14, 29, 48, 29, 26, 26, 26, 28, 17, 52, 19, 15, 30, 26, 13, 13, 12, 27, 30, 13, 23, 32, 15, 23, 19, 24, 18, 18, 17, 18, 22, 19, 15, 28, 8, 40, 23, 46, 9, 19, 14, 25, 25, 15, 33, 23, 17, 19, 16, 17, 44, 19, 30, 20, 24, 15, 19, 16, 22, 17, 38, 24, 13, 13, 21, 19, 13, 39, 30, 14, 23, 52, 10, 7, 19, 26, 22, 13, 28, 21, 66, 31, 16, 25, 28, 31, 23, 22, 13, 20, 22, 30, 13, 9, 12, 13, 17, 9, 42, 29, 14, 17, 40, 46, 12, 33, 15, 16, 55, 24, 9, 14, 11, 22, 58, 22, 16, 34, 19, 38, 19, 13, 13, 10, 24, 28, 37, 14, 12, 32, 18, 16, 30, 31, 27, 23, 36, 13, 16, 30, 13, 57, 54, 13], "policy_player_1_reward": [1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6044132955988929, "mean_inference_ms": 1.7927332404524525, "mean_action_processing_ms": 0.1714916566100149, "mean_env_wait_ms": 0.11594969371178224, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013970238821847098, "ViewRequirementAgentConnector_ms": 0.21076692853655135}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.931428571428572, "episodes_this_iter": 175, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.005714285714285714, "player_2": -0.005714285714285714}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [20, 17, 13, 29, 23, 9, 10, 28, 12, 19, 18, 19, 14, 36, 19, 10, 26, 38, 50, 13, 9, 9, 17, 22, 30, 16, 8, 37, 48, 12, 13, 14, 29, 48, 29, 26, 26, 26, 28, 17, 52, 19, 15, 30, 26, 13, 13, 12, 27, 30, 13, 23, 32, 15, 23, 19, 24, 18, 18, 17, 18, 22, 19, 15, 28, 8, 40, 23, 46, 9, 19, 14, 25, 25, 15, 33, 23, 17, 19, 16, 17, 44, 19, 30, 20, 24, 15, 19, 16, 22, 17, 38, 24, 13, 13, 21, 19, 13, 39, 30, 14, 23, 52, 10, 7, 19, 26, 22, 13, 28, 21, 66, 31, 16, 25, 28, 31, 23, 22, 13, 20, 22, 30, 13, 9, 12, 13, 17, 9, 42, 29, 14, 17, 40, 46, 12, 33, 15, 16, 55, 24, 9, 14, 11, 22, 58, 22, 16, 34, 19, 38, 19, 13, 13, 10, 24, 28, 37, 14, 12, 32, 18, 16, 30, 31, 27, 23, 36, 13, 16, 30, 13, 57, 54, 13], "policy_player_1_reward": [1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6044132955988929, "mean_inference_ms": 1.7927332404524525, "mean_action_processing_ms": 0.1714916566100149, "mean_env_wait_ms": 0.11594969371178224, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013970238821847098, "ViewRequirementAgentConnector_ms": 0.21076692853655135}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997, "num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 241.39761390228614, "num_env_steps_trained_throughput_per_sec": 241.39761390228614, "timesteps_total": 40000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 39997, "timers": {"training_iteration_time_ms": 17341.294, "sample_time_ms": 3692.667, "learn_time_ms": 13639.515, "learn_throughput": 293.266, "synch_weights_time_ms": 8.498}, "counters": {"num_env_steps_sampled": 40000, "num_env_steps_trained": 40000, "num_agent_steps_sampled": 39997, "num_agent_steps_trained": 39997}, "done": false, "episodes_total": 2031, "training_iteration": 10, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-24-18", "timestamp": 1711733058, "time_this_iter_s": 20.10471749305725, "time_total_s": 214.49899792671204, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3D480>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3D990>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3CF70>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 214.49899792671204, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 9.920689655172414, "ram_util_percent": 88.58620689655172}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.045, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.77, "player_2": 0.7549019607843137, "player_1": 0.7857142857142857}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [17, 21, 15, 11, 13, 22, 10, 11, 16, 18, 13, 16, 7, 16, 13, 20, 13, 10, 16, 14, 8, 8, 16, 17, 20, 21, 13, 18, 28, 10, 13, 13, 10, 11, 22, 10, 25, 21, 20, 13, 11, 21, 8, 8, 18, 15, 8, 40, 10, 12, 8, 10, 18, 14, 16, 11, 11, 15, 13, 20, 17, 18, 25, 19, 7, 14, 9, 11, 18, 9, 7, 17, 12, 12, 31, 7, 24, 7, 16, 11, 22, 18, 23, 14, 19, 13, 12, 12, 10, 9, 24, 13, 8, 21, 7, 7, 14, 21, 20, 19, 16, 10, 11, 12, 9, 13, 13, 12, 7, 21, 23, 15, 12, 8, 7, 23, 15, 14, 25, 19, 11, 12, 10, 13, 14, 15, 18, 9, 7, 11, 7, 19, 10, 16, 18, 9, 19, 10, 10, 12, 14, 24, 17, 20, 38, 12, 19, 9, 16, 12, 11, 10, 28, 24, 9, 18, 34, 9, 13, 9, 9, 13, 14, 10, 10, 32, 19, 13, 25, 30, 8, 8, 16, 15, 14, 15, 19, 15, 17, 10, 9, 12, 19, 10, 11, 7, 15, 19, 13, 10, 19, 14, 19, 25, 21, 17, 18, 12, 12, 41], "policy_random_reward": [-1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0], "policy_player_2_reward": [1.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 1.0], "policy_player_1_reward": [-1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6013958188918697, "mean_inference_ms": 0.9205679440115901, "mean_action_processing_ms": 0.13887103095692563, "mean_env_wait_ms": 0.08915471900999206, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009192466735839844, "ViewRequirementAgentConnector_ms": 0.24517697095870972}, "player_1_winrate": 0.7040816326530612, "player_2_winrate": 0.7254901960784313, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.4079545232156914, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7519362345337868, "policy_loss": -0.009427274722838774, "vf_loss": 1.761158722639084, "vf_explained_var": 0.3125925066570441, "kl": 0.004095822481152607, "entropy": 0.4245127580439051, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.9375, "num_grad_updates_lifetime": 5220.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.086094628274441, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7034949267903963, "policy_loss": -0.01805135627328127, "vf_loss": 1.7208897486329078, "vf_explained_var": 0.3294919699430466, "kl": 0.006565263980648264, "entropy": 0.4529717804864049, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.0625, "num_grad_updates_lifetime": 5040.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.15606936416185, "episode_media": {}, "episodes_this_iter": 173, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.24277456647398843, "player_2": 0.24277456647398843}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [15, 21, 9, 23, 14, 26, 8, 17, 16, 19, 53, 25, 30, 40, 35, 13, 38, 37, 21, 13, 23, 9, 8, 22, 18, 22, 17, 25, 19, 19, 22, 14, 47, 25, 30, 36, 19, 20, 26, 12, 19, 21, 9, 27, 13, 19, 24, 19, 15, 22, 17, 40, 37, 19, 35, 24, 25, 36, 13, 36, 20, 36, 16, 10, 9, 56, 13, 22, 38, 25, 44, 36, 12, 25, 37, 30, 13, 23, 20, 29, 22, 14, 13, 27, 17, 36, 50, 30, 14, 21, 9, 13, 13, 14, 19, 17, 38, 32, 25, 20, 13, 9, 10, 40, 59, 42, 21, 43, 22, 14, 40, 25, 13, 25, 13, 34, 30, 13, 15, 27, 15, 23, 20, 17, 22, 14, 25, 52, 13, 9, 10, 23, 16, 27, 13, 18, 8, 13, 21, 37, 30, 8, 15, 24, 19, 31, 12, 16, 20, 28, 9, 19, 26, 29, 47, 16, 31, 7, 50, 20, 22, 19, 24, 19, 31, 18, 31, 19, 48, 25, 29, 13, 13], "policy_player_1_reward": [-2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6047861895885646, "mean_inference_ms": 1.7979056875733197, "mean_action_processing_ms": 0.17234728912408315, "mean_env_wait_ms": 0.11585993072226607, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011751072944244208, "ViewRequirementAgentConnector_ms": 0.22057501566892415}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.15606936416185, "episodes_this_iter": 173, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.24277456647398843, "player_2": 0.24277456647398843}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [15, 21, 9, 23, 14, 26, 8, 17, 16, 19, 53, 25, 30, 40, 35, 13, 38, 37, 21, 13, 23, 9, 8, 22, 18, 22, 17, 25, 19, 19, 22, 14, 47, 25, 30, 36, 19, 20, 26, 12, 19, 21, 9, 27, 13, 19, 24, 19, 15, 22, 17, 40, 37, 19, 35, 24, 25, 36, 13, 36, 20, 36, 16, 10, 9, 56, 13, 22, 38, 25, 44, 36, 12, 25, 37, 30, 13, 23, 20, 29, 22, 14, 13, 27, 17, 36, 50, 30, 14, 21, 9, 13, 13, 14, 19, 17, 38, 32, 25, 20, 13, 9, 10, 40, 59, 42, 21, 43, 22, 14, 40, 25, 13, 25, 13, 34, 30, 13, 15, 27, 15, 23, 20, 17, 22, 14, 25, 52, 13, 9, 10, 23, 16, 27, 13, 18, 8, 13, 21, 37, 30, 8, 15, 24, 19, 31, 12, 16, 20, 28, 9, 19, 26, 29, 47, 16, 31, 7, 50, 20, 22, 19, 24, 19, 31, 18, 31, 19, 48, 25, 29, 13, 13], "policy_player_1_reward": [-2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6047861895885646, "mean_inference_ms": 1.7979056875733197, "mean_action_processing_ms": 0.17234728912408315, "mean_env_wait_ms": 0.11585993072226607, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011751072944244208, "ViewRequirementAgentConnector_ms": 0.22057501566892415}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997, "num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 243.09916685306638, "num_env_steps_trained_throughput_per_sec": 243.09916685306638, "timesteps_total": 44000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 43997, "timers": {"training_iteration_time_ms": 17426.608, "sample_time_ms": 3753.514, "learn_time_ms": 13664.336, "learn_throughput": 292.733, "synch_weights_time_ms": 8.349}, "counters": {"num_env_steps_sampled": 44000, "num_env_steps_trained": 44000, "num_agent_steps_sampled": 43997, "num_agent_steps_trained": 43997}, "done": false, "episodes_total": 2204, "training_iteration": 11, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-24-39", "timestamp": 1711733079, "time_this_iter_s": 20.132137060165405, "time_total_s": 234.63113498687744, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3D120>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3D2D0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C19C1E10>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 234.63113498687744, "iterations_since_restore": 11, "perf": {"cpu_util_percent": 9.85, "ram_util_percent": 90.49642857142855}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.02, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.835, "player_2": 0.9074074074074074, "player_1": 0.75}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [12, 19, 8, 14, 21, 12, 30, 26, 9, 14, 12, 25, 17, 11, 8, 8, 20, 12, 9, 10, 10, 13, 16, 17, 13, 20, 9, 13, 10, 10, 10, 12, 12, 8, 15, 10, 10, 8, 8, 13, 9, 12, 8, 15, 11, 19, 22, 20, 38, 14, 15, 9, 7, 19, 9, 11, 19, 22, 13, 8, 12, 24, 22, 12, 8, 10, 14, 16, 14, 19, 7, 8, 9, 21, 8, 12, 9, 15, 9, 26, 10, 20, 15, 11, 17, 19, 15, 13, 7, 15, 30, 7, 8, 21, 10, 14, 14, 13, 12, 11, 16, 9, 15, 14, 14, 35, 9, 23, 14, 11, 29, 9, 18, 17, 11, 19, 15, 16, 28, 7, 10, 11, 25, 24, 28, 8, 14, 25, 7, 7, 16, 8, 26, 24, 18, 11, 15, 7, 12, 23, 7, 40, 9, 9, 12, 25, 10, 19, 9, 19, 15, 14, 29, 26, 15, 19, 21, 13, 12, 9, 7, 13, 37, 11, 8, 15, 20, 16, 17, 13, 13, 13, 17, 13, 15, 24, 7, 8, 7, 13, 13, 12, 13, 20, 20, 11, 13, 18, 26, 32, 20, 30, 7, 8, 15, 15, 30, 19, 13, 11], "policy_random_reward": [1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0], "policy_player_1_reward": [2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6012511244637345, "mean_inference_ms": 0.9183771408030441, "mean_action_processing_ms": 0.13880878221340967, "mean_env_wait_ms": 0.08969571225033501, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013847053050994873, "ViewRequirementAgentConnector_ms": 0.24275833368301392}, "player_1_winrate": 0.717391304347826, "player_2_winrate": 0.7407407407407407, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.050360151131948, "cur_kl_coeff": 0.025, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.8383939109596552, "policy_loss": -0.011697940325693173, "vf_loss": 1.8499204827289955, "vf_explained_var": 0.3023105140994577, "kl": 0.0068546928762364245, "entropy": 0.4286939577144735, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.11764705882354, "num_grad_updates_lifetime": 5715.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.635844313104948, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7990149170160294, "policy_loss": -0.016728329190421696, "vf_loss": 1.814942308018605, "vf_explained_var": 0.32074569712082546, "kl": 0.00800936235243632, "entropy": 0.4162014654527108, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.3125, "num_grad_updates_lifetime": 5520.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.526829268292683, "episode_media": {}, "episodes_this_iter": 205, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.32682926829268294, "player_2": 0.32682926829268294}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 22, 28, 13, 9, 13, 25, 14, 15, 13, 18, 19, 25, 13, 14, 15, 23, 10, 13, 17, 21, 16, 8, 29, 31, 23, 17, 32, 32, 9, 25, 13, 13, 16, 26, 24, 22, 19, 20, 22, 13, 30, 16, 23, 18, 9, 38, 10, 11, 8, 24, 13, 15, 26, 24, 19, 21, 11, 9, 40, 20, 20, 24, 47, 19, 26, 18, 17, 9, 35, 13, 21, 11, 13, 31, 16, 33, 19, 13, 10, 19, 50, 19, 16, 13, 9, 30, 24, 13, 40, 21, 24, 15, 16, 25, 13, 14, 18, 16, 16, 16, 23, 26, 9, 13, 13, 16, 15, 19, 48, 12, 15, 16, 14, 9, 13, 20, 18, 23, 16, 14, 16, 19, 22, 24, 13, 13, 13, 13, 31, 21, 14, 22, 13, 19, 19, 19, 13, 50, 25, 19, 13, 27, 9, 16, 26, 19, 11, 17, 13, 18, 11, 13, 19, 25, 17, 13, 14, 32, 13, 19, 16, 13, 16, 19, 13, 52, 9, 25, 24, 24, 28, 36, 11, 18, 22, 10, 13, 14, 25, 38, 19, 11, 55, 13, 15, 18, 13, 38, 22, 16, 13, 28, 18, 34, 17, 19, 19, 44, 21, 13, 29, 11, 8, 22], "policy_player_1_reward": [-2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0], "policy_player_2_reward": [2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6094656824007613, "mean_inference_ms": 1.8092318651215171, "mean_action_processing_ms": 0.17336892370727616, "mean_env_wait_ms": 0.11755337700286596, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014628142845339892, "ViewRequirementAgentConnector_ms": 0.23757149533527652}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 19.526829268292683, "episodes_this_iter": 205, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.32682926829268294, "player_2": 0.32682926829268294}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 22, 28, 13, 9, 13, 25, 14, 15, 13, 18, 19, 25, 13, 14, 15, 23, 10, 13, 17, 21, 16, 8, 29, 31, 23, 17, 32, 32, 9, 25, 13, 13, 16, 26, 24, 22, 19, 20, 22, 13, 30, 16, 23, 18, 9, 38, 10, 11, 8, 24, 13, 15, 26, 24, 19, 21, 11, 9, 40, 20, 20, 24, 47, 19, 26, 18, 17, 9, 35, 13, 21, 11, 13, 31, 16, 33, 19, 13, 10, 19, 50, 19, 16, 13, 9, 30, 24, 13, 40, 21, 24, 15, 16, 25, 13, 14, 18, 16, 16, 16, 23, 26, 9, 13, 13, 16, 15, 19, 48, 12, 15, 16, 14, 9, 13, 20, 18, 23, 16, 14, 16, 19, 22, 24, 13, 13, 13, 13, 31, 21, 14, 22, 13, 19, 19, 19, 13, 50, 25, 19, 13, 27, 9, 16, 26, 19, 11, 17, 13, 18, 11, 13, 19, 25, 17, 13, 14, 32, 13, 19, 16, 13, 16, 19, 13, 52, 9, 25, 24, 24, 28, 36, 11, 18, 22, 10, 13, 14, 25, 38, 19, 11, 55, 13, 15, 18, 13, 38, 22, 16, 13, 28, 18, 34, 17, 19, 19, 44, 21, 13, 29, 11, 8, 22], "policy_player_1_reward": [-2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0], "policy_player_2_reward": [2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6094656824007613, "mean_inference_ms": 1.8092318651215171, "mean_action_processing_ms": 0.17336892370727616, "mean_env_wait_ms": 0.11755337700286596, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014628142845339892, "ViewRequirementAgentConnector_ms": 0.23757149533527652}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997, "num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 220.32239013451667, "num_env_steps_trained_throughput_per_sec": 220.32239013451667, "timesteps_total": 48000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 47997, "timers": {"training_iteration_time_ms": 17197.684, "sample_time_ms": 3830.837, "learn_time_ms": 13357.082, "learn_throughput": 299.467, "synch_weights_time_ms": 9.253}, "counters": {"num_env_steps_sampled": 48000, "num_env_steps_trained": 48000, "num_agent_steps_sampled": 47997, "num_agent_steps_trained": 47997}, "done": false, "episodes_total": 2409, "training_iteration": 12, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-25-01", "timestamp": 1711733101, "time_this_iter_s": 21.832059860229492, "time_total_s": 256.46319484710693, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C19C36D0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3D480>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C19C3250>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 256.46319484710693, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 11.929032258064517, "ram_util_percent": 91.68387096774194}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.51, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.845, "player_2": 0.8556701030927835, "player_1": 0.8349514563106796}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 19, 13, 12, 18, 10, 23, 18, 11, 20, 15, 17, 15, 17, 18, 10, 12, 8, 8, 23, 10, 10, 15, 22, 21, 10, 13, 19, 16, 17, 9, 19, 34, 20, 20, 21, 23, 15, 8, 26, 13, 18, 10, 13, 14, 12, 12, 18, 13, 10, 18, 6, 17, 9, 12, 9, 22, 15, 12, 18, 9, 9, 17, 9, 15, 11, 10, 19, 17, 12, 27, 26, 11, 20, 19, 8, 18, 14, 11, 15, 8, 22, 14, 18, 8, 19, 26, 10, 11, 20, 16, 8, 18, 11, 14, 17, 16, 19, 13, 7, 13, 18, 26, 7, 20, 20, 12, 11, 7, 10, 15, 24, 15, 16, 23, 14, 10, 16, 41, 44, 20, 12, 15, 9, 32, 33, 31, 24, 7, 16, 15, 13, 18, 16, 14, 15, 7, 20, 8, 17, 10, 9, 9, 23, 13, 26, 19, 7, 13, 20, 13, 14, 17, 10, 40, 7, 13, 15, 7, 17, 10, 15, 8, 15, 15, 12, 14, 15, 23, 9, 24, 10, 22, 13, 18, 18, 8, 8, 20, 10, 9, 14, 13, 7, 24, 20, 16, 21, 10, 9, 13, 21, 15, 16, 8, 19, 13, 21, 11, 24], "policy_random_reward": [-2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -1.0, 1.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0], "policy_player_2_reward": [2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -1.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0], "policy_player_1_reward": [-2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6026512265802657, "mean_inference_ms": 0.9166757144494446, "mean_action_processing_ms": 0.1385035168220463, "mean_env_wait_ms": 0.0892716266418131, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014372408390045166, "ViewRequirementAgentConnector_ms": 0.24295437335968018}, "player_1_winrate": 0.7378640776699029, "player_2_winrate": 0.7319587628865979, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.283621002646053, "cur_kl_coeff": 0.025, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7407776824691716, "policy_loss": -0.018569545259735747, "vf_loss": 1.759194046580324, "vf_explained_var": 0.34125103716756783, "kl": 0.006126999258554529, "entropy": 0.39986346319463906, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.6470588235294, "num_grad_updates_lifetime": 6225.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.699769172569116, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6579236835241318, "policy_loss": -0.01905476827038607, "vf_loss": 1.6762821597357591, "vf_explained_var": 0.3845246364672979, "kl": 0.006962970850279463, "entropy": 0.41228459974129994, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.8125, "num_grad_updates_lifetime": 6000.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.837696335078533, "episode_media": {}, "episodes_this_iter": 191, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.1256544502617801, "player_2": 0.1256544502617801}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 36, 47, 13, 20, 15, 16, 26, 12, 19, 11, 19, 26, 14, 31, 35, 32, 16, 28, 16, 24, 19, 10, 20, 13, 20, 50, 13, 7, 13, 48, 34, 34, 9, 13, 24, 20, 9, 16, 20, 10, 9, 9, 12, 46, 27, 13, 25, 15, 18, 15, 24, 18, 15, 20, 10, 29, 14, 28, 7, 13, 11, 19, 13, 35, 15, 31, 10, 14, 19, 20, 16, 15, 11, 52, 13, 26, 13, 19, 19, 16, 16, 9, 10, 29, 13, 29, 19, 10, 9, 32, 32, 13, 30, 10, 23, 19, 11, 15, 20, 13, 21, 25, 13, 22, 14, 31, 17, 24, 23, 13, 28, 11, 21, 30, 22, 17, 13, 22, 10, 20, 23, 46, 27, 13, 48, 35, 9, 27, 25, 24, 16, 22, 16, 29, 46, 31, 30, 31, 21, 64, 15, 13, 13, 50, 24, 14, 18, 22, 17, 31, 15, 19, 15, 24, 21, 11, 16, 30, 32, 29, 13, 27, 9, 22, 13, 22, 20, 16, 13, 13, 28, 15, 19, 25, 39, 9, 13, 42, 22, 13, 16, 16, 13, 20, 25, 16, 17, 16, 20, 24], "policy_player_1_reward": [-2.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0], "policy_player_2_reward": [2.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6080849203788686, "mean_inference_ms": 1.8026119753603185, "mean_action_processing_ms": 0.1725720355914619, "mean_env_wait_ms": 0.11746834243460026, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014385877479433389, "ViewRequirementAgentConnector_ms": 0.2166194441430856}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.837696335078533, "episodes_this_iter": 191, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.1256544502617801, "player_2": 0.1256544502617801}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [21, 36, 47, 13, 20, 15, 16, 26, 12, 19, 11, 19, 26, 14, 31, 35, 32, 16, 28, 16, 24, 19, 10, 20, 13, 20, 50, 13, 7, 13, 48, 34, 34, 9, 13, 24, 20, 9, 16, 20, 10, 9, 9, 12, 46, 27, 13, 25, 15, 18, 15, 24, 18, 15, 20, 10, 29, 14, 28, 7, 13, 11, 19, 13, 35, 15, 31, 10, 14, 19, 20, 16, 15, 11, 52, 13, 26, 13, 19, 19, 16, 16, 9, 10, 29, 13, 29, 19, 10, 9, 32, 32, 13, 30, 10, 23, 19, 11, 15, 20, 13, 21, 25, 13, 22, 14, 31, 17, 24, 23, 13, 28, 11, 21, 30, 22, 17, 13, 22, 10, 20, 23, 46, 27, 13, 48, 35, 9, 27, 25, 24, 16, 22, 16, 29, 46, 31, 30, 31, 21, 64, 15, 13, 13, 50, 24, 14, 18, 22, 17, 31, 15, 19, 15, 24, 21, 11, 16, 30, 32, 29, 13, 27, 9, 22, 13, 22, 20, 16, 13, 13, 28, 15, 19, 25, 39, 9, 13, 42, 22, 13, 16, 16, 13, 20, 25, 16, 17, 16, 20, 24], "policy_player_1_reward": [-2.0, 1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0], "policy_player_2_reward": [2.0, -1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6080849203788686, "mean_inference_ms": 1.8026119753603185, "mean_action_processing_ms": 0.1725720355914619, "mean_env_wait_ms": 0.11746834243460026, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014385877479433389, "ViewRequirementAgentConnector_ms": 0.2166194441430856}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997, "num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 248.0382983824909, "num_env_steps_trained_throughput_per_sec": 248.0382983824909, "timesteps_total": 52000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 51997, "timers": {"training_iteration_time_ms": 16873.588, "sample_time_ms": 3749.767, "learn_time_ms": 13113.964, "learn_throughput": 305.018, "synch_weights_time_ms": 9.345}, "counters": {"num_env_steps_sampled": 52000, "num_env_steps_trained": 52000, "num_agent_steps_sampled": 51997, "num_agent_steps_trained": 51997}, "done": false, "episodes_total": 2600, "training_iteration": 13, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-25-21", "timestamp": 1711733121, "time_this_iter_s": 19.86507749557495, "time_total_s": 276.3282723426819, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3F130>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3F490>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3E7A0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 276.3282723426819, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 9.482142857142858, "ram_util_percent": 92.58214285714284}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.565, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 1.0, "random": -0.875, "player_2": 0.7368421052631579}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [8, 18, 20, 11, 21, 15, 11, 33, 19, 10, 16, 15, 18, 7, 13, 14, 10, 16, 13, 14, 24, 20, 18, 8, 18, 32, 11, 11, 9, 16, 18, 27, 16, 14, 20, 29, 9, 8, 7, 18, 10, 10, 8, 26, 14, 12, 15, 7, 22, 9, 11, 35, 19, 8, 10, 20, 30, 9, 27, 12, 10, 18, 19, 12, 7, 8, 15, 8, 28, 13, 15, 15, 12, 12, 12, 7, 19, 18, 19, 14, 10, 18, 20, 33, 11, 8, 11, 18, 25, 12, 16, 8, 20, 26, 21, 15, 7, 14, 16, 24, 10, 22, 9, 25, 17, 8, 11, 10, 16, 21, 13, 18, 12, 8, 23, 19, 7, 17, 7, 14, 14, 13, 42, 20, 8, 8, 9, 9, 18, 21, 8, 24, 19, 16, 15, 12, 10, 17, 14, 29, 18, 20, 18, 9, 20, 22, 13, 18, 20, 8, 10, 11, 31, 8, 16, 7, 13, 9, 15, 15, 15, 16, 10, 16, 20, 7, 7, 9, 36, 10, 9, 16, 13, 11, 14, 15, 9, 20, 14, 33, 13, 10, 18, 12, 27, 13, 20, 15, 8, 15, 20, 16, 21, 25, 12, 25, 7, 10, 34, 15], "policy_player_1_reward": [2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0], "policy_random_reward": [-2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5996106548886259, "mean_inference_ms": 0.909816504494585, "mean_action_processing_ms": 0.13767060861823746, "mean_env_wait_ms": 0.08864173137789516, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010067582130432129, "ViewRequirementAgentConnector_ms": 0.2282085418701172}, "player_1_winrate": 0.7904761904761904, "player_2_winrate": 0.7052631578947368, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.6480098642408847, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6519592540959518, "policy_loss": -0.012400578656161088, "vf_loss": 1.6642524232467015, "vf_explained_var": 0.314546774700284, "kl": 0.004296369035521803, "entropy": 0.39605502461393677, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.6875, "num_grad_updates_lifetime": 6720.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.489924944440523, "cur_kl_coeff": 0.10000000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5791623016198477, "policy_loss": -0.011000845203913439, "vf_loss": 1.5897983151177566, "vf_explained_var": 0.33762695466478665, "kl": 0.0036482601386080243, "entropy": 0.40071910933280985, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.3125, "num_grad_updates_lifetime": 6480.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.126436781609197, "episode_media": {}, "episodes_this_iter": 174, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.005747126436781609, "player_2": -0.005747126436781609}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [30, 37, 46, 22, 14, 8, 13, 19, 38, 25, 22, 10, 28, 24, 32, 17, 20, 8, 34, 21, 55, 17, 35, 25, 52, 14, 32, 16, 48, 16, 18, 14, 58, 21, 9, 65, 25, 25, 42, 19, 21, 19, 30, 9, 16, 19, 16, 30, 42, 13, 11, 24, 16, 22, 13, 18, 36, 16, 10, 17, 9, 13, 17, 13, 25, 14, 24, 22, 9, 13, 18, 48, 36, 13, 19, 19, 20, 19, 22, 15, 31, 25, 25, 19, 22, 76, 16, 10, 46, 18, 19, 17, 44, 19, 26, 13, 12, 30, 34, 28, 8, 17, 13, 25, 20, 34, 16, 20, 17, 30, 21, 25, 26, 34, 13, 24, 48, 44, 13, 31, 17, 9, 11, 9, 16, 9, 20, 16, 15, 32, 9, 85, 31, 23, 14, 19, 14, 15, 13, 27, 17, 36, 23, 28, 24, 22, 22, 13, 16, 18, 46, 23, 11, 8, 9, 9, 31, 10, 13, 12, 16, 13, 10, 22, 28, 13, 31, 13, 46, 67, 24, 33, 21, 23], "policy_player_1_reward": [1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.613299766240668, "mean_inference_ms": 1.822804895540303, "mean_action_processing_ms": 0.17480075158492986, "mean_env_wait_ms": 0.11782135955918292, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015082304505096084, "ViewRequirementAgentConnector_ms": 0.2501915241109914}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.126436781609197, "episodes_this_iter": 174, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.005747126436781609, "player_2": -0.005747126436781609}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [30, 37, 46, 22, 14, 8, 13, 19, 38, 25, 22, 10, 28, 24, 32, 17, 20, 8, 34, 21, 55, 17, 35, 25, 52, 14, 32, 16, 48, 16, 18, 14, 58, 21, 9, 65, 25, 25, 42, 19, 21, 19, 30, 9, 16, 19, 16, 30, 42, 13, 11, 24, 16, 22, 13, 18, 36, 16, 10, 17, 9, 13, 17, 13, 25, 14, 24, 22, 9, 13, 18, 48, 36, 13, 19, 19, 20, 19, 22, 15, 31, 25, 25, 19, 22, 76, 16, 10, 46, 18, 19, 17, 44, 19, 26, 13, 12, 30, 34, 28, 8, 17, 13, 25, 20, 34, 16, 20, 17, 30, 21, 25, 26, 34, 13, 24, 48, 44, 13, 31, 17, 9, 11, 9, 16, 9, 20, 16, 15, 32, 9, 85, 31, 23, 14, 19, 14, 15, 13, 27, 17, 36, 23, 28, 24, 22, 22, 13, 16, 18, 46, 23, 11, 8, 9, 9, 31, 10, 13, 12, 16, 13, 10, 22, 28, 13, 31, 13, 46, 67, 24, 33, 21, 23], "policy_player_1_reward": [1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, 2.0, -1.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.613299766240668, "mean_inference_ms": 1.822804895540303, "mean_action_processing_ms": 0.17480075158492986, "mean_env_wait_ms": 0.11782135955918292, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015082304505096084, "ViewRequirementAgentConnector_ms": 0.2501915241109914}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997, "num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 246.13598112401786, "num_env_steps_trained_throughput_per_sec": 246.13598112401786, "timesteps_total": 56000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 55997, "timers": {"training_iteration_time_ms": 16723.747, "sample_time_ms": 3778.942, "learn_time_ms": 12934.999, "learn_throughput": 309.239, "synch_weights_time_ms": 9.294}, "counters": {"num_env_steps_sampled": 56000, "num_env_steps_trained": 56000, "num_agent_steps_sampled": 55997, "num_agent_steps_trained": 55997}, "done": false, "episodes_total": 2774, "training_iteration": 14, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-25-41", "timestamp": 1711733141, "time_this_iter_s": 19.817915439605713, "time_total_s": 296.1461877822876, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3EEF0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3F1C0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3FD00>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 296.1461877822876, "iterations_since_restore": 14, "perf": {"cpu_util_percent": 11.062962962962963, "ram_util_percent": 90.9}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 14.865, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.78, "player_2": 0.8125, "player_1": 0.75}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [13, 17, 7, 7, 8, 28, 15, 9, 7, 34, 22, 20, 8, 7, 19, 18, 8, 10, 13, 14, 17, 7, 12, 15, 20, 8, 19, 12, 7, 22, 12, 18, 14, 12, 10, 8, 10, 23, 8, 15, 10, 24, 20, 19, 11, 10, 18, 11, 9, 11, 13, 17, 11, 12, 9, 31, 8, 20, 16, 18, 14, 27, 29, 40, 18, 14, 18, 14, 12, 17, 47, 15, 25, 10, 7, 7, 9, 15, 7, 23, 9, 7, 16, 25, 8, 18, 10, 9, 20, 14, 26, 20, 12, 25, 14, 9, 8, 8, 17, 16, 14, 12, 22, 19, 13, 10, 11, 16, 17, 16, 16, 11, 16, 13, 8, 22, 9, 14, 32, 7, 15, 8, 20, 7, 18, 13, 9, 12, 14, 18, 26, 17, 21, 14, 8, 14, 20, 15, 9, 17, 30, 8, 10, 21, 23, 10, 9, 20, 12, 8, 10, 27, 25, 9, 12, 23, 14, 11, 17, 17, 19, 10, 12, 13, 15, 23, 15, 11, 7, 23, 15, 16, 22, 9, 8, 27, 8, 8, 11, 8, 18, 22, 9, 21, 11, 11, 11, 10, 9, 15, 13, 10, 13, 15, 11, 13, 20, 19, 8, 15], "policy_random_reward": [-2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0], "policy_player_2_reward": [2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0], "policy_player_1_reward": [2.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5965333551675073, "mean_inference_ms": 0.9019374421161439, "mean_action_processing_ms": 0.13724447517052504, "mean_env_wait_ms": 0.08805315054066937, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010661005973815918, "ViewRequirementAgentConnector_ms": 0.21928715705871582}, "player_1_winrate": 0.7211538461538461, "player_2_winrate": 0.7291666666666666, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 3.7537240614493688, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6521133879820507, "policy_loss": -0.01121301005356751, "vf_loss": 1.6632760678728422, "vf_explained_var": 0.32806702827413875, "kl": 0.004026071276265557, "entropy": 0.40845470254619914, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.75, "num_grad_updates_lifetime": 7200.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.599734063943227, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6172336330016455, "policy_loss": -0.014162004244402246, "vf_loss": 1.6310884848237037, "vf_explained_var": 0.34006677828729154, "kl": 0.006143040536520631, "entropy": 0.3881259348243475, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.25, "num_grad_updates_lifetime": 6960.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 60000, "num_env_steps_trained": 60000, "num_agent_steps_sampled": 59997, "num_agent_steps_trained": 59997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.98843930635838, "episode_media": {}, "episodes_this_iter": 173, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.12716763005780346, "player_2": 0.12716763005780346}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [24, 10, 10, 11, 35, 8, 26, 10, 13, 19, 20, 11, 13, 22, 36, 9, 9, 30, 9, 8, 19, 16, 43, 13, 35, 27, 19, 13, 83, 20, 19, 45, 64, 10, 23, 31, 8, 16, 25, 24, 13, 27, 62, 14, 14, 26, 14, 20, 13, 21, 13, 72, 13, 22, 32, 17, 16, 30, 16, 16, 17, 32, 13, 56, 33, 29, 9, 28, 13, 13, 13, 10, 9, 26, 16, 13, 25, 20, 17, 25, 25, 19, 16, 12, 13, 13, 23, 16, 23, 26, 25, 12, 16, 31, 10, 37, 15, 54, 13, 19, 29, 34, 13, 39, 22, 30, 19, 12, 19, 9, 38, 13, 19, 15, 22, 9, 40, 16, 33, 30, 10, 65, 11, 18, 38, 13, 80, 19, 33, 16, 18, 38, 42, 18, 13, 19, 18, 23, 25, 22, 10, 37, 14, 24, 32, 13, 31, 13, 39, 21, 35, 22, 30, 37, 36, 21, 24, 36, 25, 9, 25, 27, 13, 18, 13, 13, 16, 28, 34, 24, 10, 31, 24], "policy_player_1_reward": [1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0, 1.0], "policy_player_2_reward": [-1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6056217911363992, "mean_inference_ms": 1.7959592120088985, "mean_action_processing_ms": 0.17260430576506447, "mean_env_wait_ms": 0.1167963506660813, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009553349776075066, "ViewRequirementAgentConnector_ms": 0.1612087894726351}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.98843930635838, "episodes_this_iter": 173, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.12716763005780346, "player_2": 0.12716763005780346}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [24, 10, 10, 11, 35, 8, 26, 10, 13, 19, 20, 11, 13, 22, 36, 9, 9, 30, 9, 8, 19, 16, 43, 13, 35, 27, 19, 13, 83, 20, 19, 45, 64, 10, 23, 31, 8, 16, 25, 24, 13, 27, 62, 14, 14, 26, 14, 20, 13, 21, 13, 72, 13, 22, 32, 17, 16, 30, 16, 16, 17, 32, 13, 56, 33, 29, 9, 28, 13, 13, 13, 10, 9, 26, 16, 13, 25, 20, 17, 25, 25, 19, 16, 12, 13, 13, 23, 16, 23, 26, 25, 12, 16, 31, 10, 37, 15, 54, 13, 19, 29, 34, 13, 39, 22, 30, 19, 12, 19, 9, 38, 13, 19, 15, 22, 9, 40, 16, 33, 30, 10, 65, 11, 18, 38, 13, 80, 19, 33, 16, 18, 38, 42, 18, 13, 19, 18, 23, 25, 22, 10, 37, 14, 24, 32, 13, 31, 13, 39, 21, 35, 22, 30, 37, 36, 21, 24, 36, 25, 9, 25, 27, 13, 18, 13, 13, 16, 28, 34, 24, 10, 31, 24], "policy_player_1_reward": [1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0, 1.0], "policy_player_2_reward": [-1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6056217911363992, "mean_inference_ms": 1.7959592120088985, "mean_action_processing_ms": 0.17260430576506447, "mean_env_wait_ms": 0.1167963506660813, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009553349776075066, "ViewRequirementAgentConnector_ms": 0.1612087894726351}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 59997, "num_agent_steps_trained": 59997, "num_env_steps_sampled": 60000, "num_env_steps_trained": 60000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 271.99773416499335, "num_env_steps_trained_throughput_per_sec": 271.99773416499335, "timesteps_total": 60000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 59997, "timers": {"training_iteration_time_ms": 16500.219, "sample_time_ms": 3648.264, "learn_time_ms": 12842.415, "learn_throughput": 311.468, "synch_weights_time_ms": 9.211}, "counters": {"num_env_steps_sampled": 60000, "num_env_steps_trained": 60000, "num_agent_steps_sampled": 59997, "num_agent_steps_trained": 59997}, "done": false, "episodes_total": 2947, "training_iteration": 15, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-25-59", "timestamp": 1711733159, "time_this_iter_s": 18.00614333152771, "time_total_s": 314.1523311138153, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3F130>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B084C0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3EC20>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 314.1523311138153, "iterations_since_restore": 15, "perf": {"cpu_util_percent": 10.284615384615384, "ram_util_percent": 90.08461538461538}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 14.945, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.6947368421052632, "random": -0.595, "player_2": 0.5047619047619047}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [22, 9, 26, 12, 9, 9, 33, 10, 16, 7, 9, 10, 13, 15, 13, 13, 15, 14, 9, 13, 8, 15, 7, 13, 20, 17, 14, 8, 11, 17, 13, 10, 21, 13, 12, 9, 9, 9, 8, 10, 17, 12, 19, 9, 10, 17, 20, 14, 28, 30, 18, 12, 18, 14, 19, 11, 17, 10, 17, 17, 18, 13, 14, 8, 8, 25, 11, 7, 9, 30, 16, 14, 9, 7, 7, 11, 28, 15, 27, 32, 21, 10, 13, 8, 9, 26, 13, 17, 8, 10, 12, 11, 10, 8, 17, 23, 15, 16, 25, 14, 13, 12, 9, 8, 8, 10, 20, 14, 16, 8, 17, 21, 18, 10, 14, 28, 21, 24, 13, 21, 7, 10, 15, 25, 9, 13, 13, 19, 16, 11, 20, 19, 27, 13, 10, 20, 17, 12, 14, 7, 20, 11, 20, 9, 37, 10, 25, 40, 8, 16, 16, 11, 16, 23, 8, 9, 15, 17, 21, 22, 8, 8, 8, 8, 20, 10, 35, 18, 13, 17, 36, 7, 9, 9, 24, 20, 10, 16, 14, 9, 12, 14, 13, 8, 13, 13, 15, 23, 14, 15, 20, 6, 28, 22, 10, 10, 12, 8, 24, 9], "policy_player_1_reward": [1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0], "policy_random_reward": [-1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 1.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5980615293524394, "mean_inference_ms": 0.902132910386036, "mean_action_processing_ms": 0.13693279334300396, "mean_env_wait_ms": 0.08798223456622967, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009654581546783447, "ViewRequirementAgentConnector_ms": 0.2494601607322693}, "player_1_winrate": 0.6842105263157895, "player_2_winrate": 0.6571428571428571, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.457159736577203, "cur_kl_coeff": 0.00625, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.69751668613331, "policy_loss": -0.016508616882321588, "vf_loss": 1.7139802237936095, "vf_explained_var": 0.3177917439563602, "kl": 0.007211648896048092, "entropy": 0.4099811729774171, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.6470588235294, "num_grad_updates_lifetime": 7695.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.7585612937808035, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6592466520766418, "policy_loss": -0.018260571005521344, "vf_loss": 1.6771054700016976, "vf_explained_var": 0.3427236631512642, "kl": 0.00803511795974665, "entropy": 0.36094945094858605, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.875, "num_grad_updates_lifetime": 7440.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 64000, "num_env_steps_trained": 64000, "num_agent_steps_sampled": 63998, "num_agent_steps_trained": 63998}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.043715846994534, "episode_media": {}, "episodes_this_iter": 183, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.16393442622950818, "player_2": 0.16393442622950818}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [18, 16, 42, 56, 8, 21, 25, 14, 17, 25, 11, 29, 15, 9, 30, 19, 13, 25, 16, 43, 47, 10, 22, 13, 13, 9, 13, 13, 39, 21, 20, 24, 9, 19, 22, 16, 25, 21, 15, 17, 24, 73, 17, 24, 18, 38, 20, 28, 13, 10, 13, 13, 9, 13, 55, 46, 13, 19, 37, 19, 25, 36, 18, 9, 16, 14, 25, 28, 14, 20, 16, 28, 14, 9, 12, 14, 13, 18, 16, 16, 35, 21, 17, 8, 20, 23, 25, 13, 13, 16, 30, 9, 30, 9, 9, 13, 20, 13, 16, 9, 9, 36, 19, 13, 15, 13, 28, 56, 64, 38, 92, 64, 16, 13, 13, 21, 36, 25, 17, 40, 35, 19, 13, 42, 25, 26, 15, 25, 19, 18, 19, 13, 20, 14, 13, 23, 14, 16, 15, 22, 24, 13, 53, 13, 24, 24, 23, 24, 16, 53, 21, 10, 48, 28, 16, 9, 16, 16, 14, 64, 13, 10, 19, 20, 11, 36, 22, 21, 15, 18, 13, 29, 8, 21, 9, 14, 17, 24, 19, 19, 25, 17, 39], "policy_player_1_reward": [2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0], "policy_player_2_reward": [-2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6017423622153637, "mean_inference_ms": 1.7787593071772965, "mean_action_processing_ms": 0.1711940319341452, "mean_env_wait_ms": 0.11547575269085522, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01304501392802254, "ViewRequirementAgentConnector_ms": 0.19546414985031377}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.043715846994534, "episodes_this_iter": 183, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.16393442622950818, "player_2": 0.16393442622950818}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [18, 16, 42, 56, 8, 21, 25, 14, 17, 25, 11, 29, 15, 9, 30, 19, 13, 25, 16, 43, 47, 10, 22, 13, 13, 9, 13, 13, 39, 21, 20, 24, 9, 19, 22, 16, 25, 21, 15, 17, 24, 73, 17, 24, 18, 38, 20, 28, 13, 10, 13, 13, 9, 13, 55, 46, 13, 19, 37, 19, 25, 36, 18, 9, 16, 14, 25, 28, 14, 20, 16, 28, 14, 9, 12, 14, 13, 18, 16, 16, 35, 21, 17, 8, 20, 23, 25, 13, 13, 16, 30, 9, 30, 9, 9, 13, 20, 13, 16, 9, 9, 36, 19, 13, 15, 13, 28, 56, 64, 38, 92, 64, 16, 13, 13, 21, 36, 25, 17, 40, 35, 19, 13, 42, 25, 26, 15, 25, 19, 18, 19, 13, 20, 14, 13, 23, 14, 16, 15, 22, 24, 13, 53, 13, 24, 24, 23, 24, 16, 53, 21, 10, 48, 28, 16, 9, 16, 16, 14, 64, 13, 10, 19, 20, 11, 36, 22, 21, 15, 18, 13, 29, 8, 21, 9, 14, 17, 24, 19, 19, 25, 17, 39], "policy_player_1_reward": [2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0], "policy_player_2_reward": [-2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6017423622153637, "mean_inference_ms": 1.7787593071772965, "mean_action_processing_ms": 0.1711940319341452, "mean_env_wait_ms": 0.11547575269085522, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01304501392802254, "ViewRequirementAgentConnector_ms": 0.19546414985031377}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 63998, "num_agent_steps_trained": 63998, "num_env_steps_sampled": 64000, "num_env_steps_trained": 64000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 237.77309380592555, "num_env_steps_trained_throughput_per_sec": 237.77309380592555, "timesteps_total": 64000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 63998, "timers": {"training_iteration_time_ms": 16420.989, "sample_time_ms": 3609.269, "learn_time_ms": 12802.417, "learn_throughput": 312.441, "synch_weights_time_ms": 9.0}, "counters": {"num_env_steps_sampled": 64000, "num_env_steps_trained": 64000, "num_agent_steps_sampled": 63998, "num_agent_steps_trained": 63998}, "done": false, "episodes_total": 3130, "training_iteration": 16, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-26-19", "timestamp": 1711733179, "time_this_iter_s": 20.506336212158203, "time_total_s": 334.6586673259735, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3E4D0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3D000>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3ED40>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 334.6586673259735, "iterations_since_restore": 16, "perf": {"cpu_util_percent": 11.765517241379312, "ram_util_percent": 90.4862068965517}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.42, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.6568627450980392, "random": -0.8, "player_2": 0.9489795918367347}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [30, 14, 9, 14, 21, 19, 19, 8, 22, 15, 15, 13, 17, 10, 10, 13, 21, 11, 24, 31, 30, 15, 14, 18, 11, 13, 12, 13, 21, 15, 14, 21, 15, 10, 10, 8, 7, 9, 11, 14, 22, 15, 7, 9, 11, 13, 28, 13, 15, 46, 10, 8, 16, 17, 31, 10, 23, 8, 16, 14, 33, 10, 10, 17, 18, 21, 15, 23, 7, 28, 17, 10, 17, 10, 25, 14, 17, 13, 16, 14, 22, 44, 17, 40, 19, 22, 22, 32, 16, 7, 8, 24, 14, 17, 9, 7, 24, 19, 17, 8, 14, 13, 13, 22, 7, 11, 17, 16, 12, 16, 16, 21, 25, 23, 18, 16, 24, 10, 9, 8, 9, 13, 11, 10, 21, 20, 10, 15, 17, 19, 8, 10, 17, 14, 27, 16, 29, 9, 11, 17, 13, 9, 11, 13, 7, 12, 22, 25, 18, 8, 11, 7, 22, 9, 24, 11, 7, 10, 12, 8, 19, 7, 20, 32, 20, 9, 14, 13, 7, 9, 11, 8, 9, 10, 12, 15, 16, 36, 17, 7, 11, 11, 7, 13, 20, 11, 13, 16, 10, 16, 9, 15, 16, 10, 11, 11, 14, 11, 12, 11], "policy_player_1_reward": [1.0, 1.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0], "policy_random_reward": [-1.0, 1.0, -2.0, -1.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6034861384970234, "mean_inference_ms": 0.9085142574940712, "mean_action_processing_ms": 0.13708935080715404, "mean_env_wait_ms": 0.08889012078155384, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009967029094696045, "ViewRequirementAgentConnector_ms": 0.29414480924606323}, "player_1_winrate": 0.696078431372549, "player_2_winrate": 0.7857142857142857, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.997214957779529, "cur_kl_coeff": 0.00625, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.630496475334261, "policy_loss": -0.014301166239687624, "vf_loss": 1.6447715723631429, "vf_explained_var": 0.3266432915248123, "kl": 0.004171028783400843, "entropy": 0.40911057037027443, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 120.76470588235294, "num_grad_updates_lifetime": 8205.5, "diff_num_grad_updates_vs_sampler_policy": 254.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.7790181875228885, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5535690061748029, "policy_loss": -0.014447920037976776, "vf_loss": 1.5677332383890947, "vf_explained_var": 0.36017561741173265, "kl": 0.005673904240937471, "entropy": 0.3389677729457617, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 121.625, "num_grad_updates_lifetime": 7920.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 68000, "num_env_steps_trained": 68000, "num_agent_steps_sampled": 67997, "num_agent_steps_trained": 67997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.76439790575916, "episode_media": {}, "episodes_this_iter": 191, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.2356020942408377, "player_2": 0.2356020942408377}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [25, 9, 14, 16, 31, 25, 9, 23, 22, 26, 17, 31, 23, 19, 24, 22, 31, 16, 9, 10, 16, 36, 19, 16, 9, 15, 30, 34, 7, 12, 23, 19, 60, 18, 16, 19, 76, 16, 27, 16, 19, 96, 25, 18, 30, 10, 17, 16, 7, 13, 25, 24, 37, 9, 29, 9, 16, 13, 8, 10, 15, 18, 21, 16, 24, 37, 19, 23, 16, 16, 16, 10, 11, 19, 13, 25, 13, 21, 16, 19, 13, 24, 13, 16, 13, 13, 35, 13, 43, 9, 17, 13, 13, 30, 18, 8, 13, 10, 22, 12, 26, 9, 30, 24, 10, 13, 33, 22, 13, 25, 48, 9, 16, 7, 14, 15, 13, 24, 23, 9, 13, 23, 9, 13, 20, 18, 14, 38, 13, 13, 19, 19, 13, 33, 14, 19, 11, 31, 18, 24, 44, 25, 10, 39, 16, 9, 13, 52, 22, 20, 30, 9, 23, 13, 66, 15, 9, 10, 12, 36, 31, 19, 22, 10, 25, 13, 41, 13, 25, 16, 25, 27, 24, 24, 12, 17, 17, 27, 7, 12, 13, 16, 13, 72, 10, 18, 64, 26, 15, 13, 35], "policy_player_1_reward": [-1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599697998161065, "mean_inference_ms": 1.771145885936115, "mean_action_processing_ms": 0.17020689050108365, "mean_env_wait_ms": 0.11505926837806518, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012423043475725265, "ViewRequirementAgentConnector_ms": 0.18675626884580282}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.76439790575916, "episodes_this_iter": 191, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.2356020942408377, "player_2": 0.2356020942408377}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [25, 9, 14, 16, 31, 25, 9, 23, 22, 26, 17, 31, 23, 19, 24, 22, 31, 16, 9, 10, 16, 36, 19, 16, 9, 15, 30, 34, 7, 12, 23, 19, 60, 18, 16, 19, 76, 16, 27, 16, 19, 96, 25, 18, 30, 10, 17, 16, 7, 13, 25, 24, 37, 9, 29, 9, 16, 13, 8, 10, 15, 18, 21, 16, 24, 37, 19, 23, 16, 16, 16, 10, 11, 19, 13, 25, 13, 21, 16, 19, 13, 24, 13, 16, 13, 13, 35, 13, 43, 9, 17, 13, 13, 30, 18, 8, 13, 10, 22, 12, 26, 9, 30, 24, 10, 13, 33, 22, 13, 25, 48, 9, 16, 7, 14, 15, 13, 24, 23, 9, 13, 23, 9, 13, 20, 18, 14, 38, 13, 13, 19, 19, 13, 33, 14, 19, 11, 31, 18, 24, 44, 25, 10, 39, 16, 9, 13, 52, 22, 20, 30, 9, 23, 13, 66, 15, 9, 10, 12, 36, 31, 19, 22, 10, 25, 13, 41, 13, 25, 16, 25, 27, 24, 24, 12, 17, 17, 27, 7, 12, 13, 16, 13, 72, 10, 18, 64, 26, 15, 13, 35], "policy_player_1_reward": [-1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0], "policy_player_2_reward": [1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599697998161065, "mean_inference_ms": 1.771145885936115, "mean_action_processing_ms": 0.17020689050108365, "mean_env_wait_ms": 0.11505926837806518, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012423043475725265, "ViewRequirementAgentConnector_ms": 0.18675626884580282}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 67997, "num_agent_steps_trained": 67997, "num_env_steps_sampled": 68000, "num_env_steps_trained": 68000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 248.86593312212807, "num_env_steps_trained_throughput_per_sec": 248.86593312212807, "timesteps_total": 68000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 67997, "timers": {"training_iteration_time_ms": 16352.681, "sample_time_ms": 3546.271, "learn_time_ms": 12797.121, "learn_throughput": 312.57, "synch_weights_time_ms": 8.885}, "counters": {"num_env_steps_sampled": 68000, "num_env_steps_trained": 68000, "num_agent_steps_sampled": 67997, "num_agent_steps_trained": 67997}, "done": false, "episodes_total": 3321, "training_iteration": 17, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-26-40", "timestamp": 1711733200, "time_this_iter_s": 20.309407472610474, "time_total_s": 354.968074798584, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B09870>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B09BD0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B08EE0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 354.968074798584, "iterations_since_restore": 17, "perf": {"cpu_util_percent": 10.885714285714286, "ram_util_percent": 91.17499999999997}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.645, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.9, "random": -0.86, "player_2": 0.8111111111111111}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [32, 17, 34, 19, 17, 13, 10, 22, 23, 25, 16, 16, 12, 7, 14, 15, 9, 11, 16, 23, 16, 10, 8, 17, 19, 13, 28, 18, 18, 30, 25, 30, 24, 10, 12, 18, 15, 16, 11, 20, 13, 7, 13, 12, 24, 7, 12, 12, 15, 17, 10, 11, 8, 7, 14, 22, 27, 11, 12, 16, 9, 18, 15, 20, 12, 35, 50, 15, 13, 17, 16, 8, 14, 12, 23, 25, 8, 23, 13, 26, 10, 12, 9, 12, 10, 17, 17, 10, 13, 7, 12, 16, 16, 10, 10, 10, 27, 19, 18, 16, 10, 9, 8, 9, 8, 10, 18, 10, 13, 16, 12, 15, 9, 22, 9, 27, 9, 8, 19, 10, 18, 8, 7, 8, 18, 12, 21, 14, 22, 16, 23, 34, 15, 13, 9, 10, 15, 11, 15, 34, 13, 18, 44, 25, 16, 8, 22, 10, 9, 14, 12, 27, 12, 20, 17, 9, 27, 13, 27, 7, 16, 10, 8, 8, 8, 28, 25, 17, 16, 15, 16, 18, 10, 9, 12, 12, 15, 21, 12, 15, 17, 19, 23, 10, 28, 9, 19, 13, 15, 15, 10, 11, 13, 16, 13, 13, 7, 11, 13, 17], "policy_player_1_reward": [1.0, 1.0, -1.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0], "policy_random_reward": [-1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -1.0, -1.0, -1.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0], "policy_player_2_reward": [1.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6039712962153351, "mean_inference_ms": 0.9106537478412011, "mean_action_processing_ms": 0.13781366484886562, "mean_env_wait_ms": 0.08964553147636746, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010608196258544922, "ViewRequirementAgentConnector_ms": 0.24308276176452637}, "player_1_winrate": 0.7454545454545455, "player_2_winrate": 0.7444444444444445, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.055831557512283, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.7655777297914028, "policy_loss": -0.016265343187721253, "vf_loss": 1.7818262182176112, "vf_explained_var": 0.2861068348089854, "kl": 0.005390635637599394, "entropy": 0.40709302928298713, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.875, "num_grad_updates_lifetime": 8700.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.802352714538574, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5967650807152192, "policy_loss": -0.013189114432316273, "vf_loss": 1.6096135590225458, "vf_explained_var": 0.3467345996449391, "kl": 0.006812779786699728, "entropy": 0.3195391789699594, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.1875, "num_grad_updates_lifetime": 8400.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 72000, "num_env_steps_trained": 72000, "num_agent_steps_sampled": 71998, "num_agent_steps_trained": 71998}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.673575129533678, "episode_media": {}, "episodes_this_iter": 193, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.031088082901554404, "player_2": -0.031088082901554404}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [44, 10, 16, 22, 9, 13, 9, 30, 37, 43, 10, 22, 9, 19, 9, 13, 24, 10, 13, 22, 13, 30, 24, 29, 16, 9, 20, 9, 9, 17, 17, 12, 19, 11, 28, 16, 16, 16, 13, 26, 64, 16, 17, 13, 9, 20, 17, 9, 44, 16, 25, 16, 28, 29, 35, 20, 17, 19, 16, 16, 17, 16, 9, 19, 16, 13, 19, 8, 23, 13, 19, 30, 16, 23, 55, 13, 47, 16, 10, 19, 17, 24, 16, 35, 16, 19, 25, 28, 24, 25, 47, 25, 10, 16, 28, 22, 16, 26, 22, 32, 18, 18, 13, 23, 12, 9, 36, 16, 21, 13, 8, 13, 37, 26, 20, 16, 19, 18, 46, 7, 28, 13, 16, 14, 26, 14, 43, 9, 21, 23, 24, 20, 15, 36, 22, 24, 13, 50, 21, 16, 25, 17, 31, 7, 16, 9, 19, 16, 22, 31, 9, 13, 15, 16, 20, 30, 11, 13, 9, 13, 16, 13, 13, 18, 52, 14, 10, 10, 14, 76, 16, 14, 29, 23, 16, 43, 12, 15, 10, 26, 100, 34, 22, 11, 13, 13, 28, 16, 10, 20, 9, 16, 25], "policy_player_1_reward": [1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 0.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0], "policy_player_2_reward": [-1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 0.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599457411890382, "mean_inference_ms": 1.7687177745367615, "mean_action_processing_ms": 0.17013323390230045, "mean_env_wait_ms": 0.115082761429324, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011252430436524703, "ViewRequirementAgentConnector_ms": 0.2243381707779484}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 20.673575129533678, "episodes_this_iter": 193, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.031088082901554404, "player_2": -0.031088082901554404}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [44, 10, 16, 22, 9, 13, 9, 30, 37, 43, 10, 22, 9, 19, 9, 13, 24, 10, 13, 22, 13, 30, 24, 29, 16, 9, 20, 9, 9, 17, 17, 12, 19, 11, 28, 16, 16, 16, 13, 26, 64, 16, 17, 13, 9, 20, 17, 9, 44, 16, 25, 16, 28, 29, 35, 20, 17, 19, 16, 16, 17, 16, 9, 19, 16, 13, 19, 8, 23, 13, 19, 30, 16, 23, 55, 13, 47, 16, 10, 19, 17, 24, 16, 35, 16, 19, 25, 28, 24, 25, 47, 25, 10, 16, 28, 22, 16, 26, 22, 32, 18, 18, 13, 23, 12, 9, 36, 16, 21, 13, 8, 13, 37, 26, 20, 16, 19, 18, 46, 7, 28, 13, 16, 14, 26, 14, 43, 9, 21, 23, 24, 20, 15, 36, 22, 24, 13, 50, 21, 16, 25, 17, 31, 7, 16, 9, 19, 16, 22, 31, 9, 13, 15, 16, 20, 30, 11, 13, 9, 13, 16, 13, 13, 18, 52, 14, 10, 10, 14, 76, 16, 14, 29, 23, 16, 43, 12, 15, 10, 26, 100, 34, 22, 11, 13, 13, 28, 16, 10, 20, 9, 16, 25], "policy_player_1_reward": [1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 0.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0], "policy_player_2_reward": [-1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 0.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599457411890382, "mean_inference_ms": 1.7687177745367615, "mean_action_processing_ms": 0.17013323390230045, "mean_env_wait_ms": 0.115082761429324, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011252430436524703, "ViewRequirementAgentConnector_ms": 0.2243381707779484}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 71998, "num_agent_steps_trained": 71998, "num_env_steps_sampled": 72000, "num_env_steps_trained": 72000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 235.02491565436023, "num_env_steps_trained_throughput_per_sec": 235.02491565436023, "timesteps_total": 72000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 71998, "timers": {"training_iteration_time_ms": 16527.486, "sample_time_ms": 3564.094, "learn_time_ms": 12953.707, "learn_throughput": 308.792, "synch_weights_time_ms": 9.181}, "counters": {"num_env_steps_sampled": 72000, "num_env_steps_trained": 72000, "num_agent_steps_sampled": 71998, "num_agent_steps_trained": 71998}, "done": false, "episodes_total": 3514, "training_iteration": 18, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-27-01", "timestamp": 1711733221, "time_this_iter_s": 21.00137758255005, "time_total_s": 375.96945238113403, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B09480>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B09240>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3EC20>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 375.96945238113403, "iterations_since_restore": 18, "perf": {"cpu_util_percent": 11.666666666666664, "ram_util_percent": 89.02666666666667}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.79, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -1.085, "player_2": 1.1376146788990826, "player_1": 1.021978021978022}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [10, 44, 17, 25, 9, 7, 7, 13, 26, 8, 42, 8, 34, 16, 7, 12, 10, 34, 10, 32, 15, 10, 17, 22, 11, 13, 29, 12, 17, 7, 7, 10, 9, 14, 21, 19, 21, 22, 24, 23, 14, 23, 13, 17, 16, 9, 7, 22, 9, 11, 17, 13, 14, 17, 16, 25, 8, 15, 13, 21, 15, 9, 16, 15, 18, 16, 17, 17, 29, 48, 19, 8, 20, 10, 12, 30, 25, 17, 8, 14, 10, 13, 17, 10, 12, 16, 36, 9, 27, 22, 20, 35, 8, 19, 42, 19, 11, 13, 9, 8, 8, 26, 7, 13, 23, 23, 19, 13, 14, 6, 15, 12, 10, 13, 9, 7, 12, 10, 17, 9, 9, 20, 7, 9, 9, 10, 30, 18, 13, 15, 9, 14, 13, 14, 7, 17, 16, 20, 16, 13, 13, 36, 9, 10, 12, 9, 13, 18, 21, 8, 8, 7, 21, 9, 17, 20, 24, 12, 26, 12, 13, 10, 12, 17, 20, 7, 23, 7, 17, 16, 13, 13, 9, 29, 7, 9, 20, 12, 11, 21, 21, 14, 14, 17, 8, 14, 9, 10, 9, 9, 28, 16, 18, 11, 27, 26, 9, 22, 20, 7], "policy_random_reward": [1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0], "policy_player_2_reward": [-1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0], "policy_player_1_reward": [2.0, -1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6066426409282073, "mean_inference_ms": 0.9142516199106748, "mean_action_processing_ms": 0.13846561436556512, "mean_env_wait_ms": 0.09001235783287488, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.013165652751922607, "ViewRequirementAgentConnector_ms": 0.27566444873809814}, "player_1_winrate": 0.7912087912087912, "player_2_winrate": 0.8165137614678899, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.030016578733921, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.6236962420245011, "policy_loss": -0.01623960810150796, "vf_loss": 1.6399160998562972, "vf_explained_var": 0.30177081016202767, "kl": 0.00632117947645966, "entropy": 0.3825457299128175, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.5, "num_grad_updates_lifetime": 9180.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.760200478633245, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.55932208498319, "policy_loss": -0.01614308335823201, "vf_loss": 1.575163191060225, "vf_explained_var": 0.3137947720785936, "kl": 0.006039418111459166, "entropy": 0.30596799024691185, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.4375, "num_grad_updates_lifetime": 8880.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 76000, "num_env_steps_trained": 76000, "num_agent_steps_sampled": 75997, "num_agent_steps_trained": 75997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.693181818181817, "episode_media": {}, "episodes_this_iter": 176, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13636363636363635, "player_2": -0.13636363636363635}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [28, 13, 8, 13, 10, 10, 29, 26, 42, 19, 10, 44, 9, 13, 8, 22, 16, 29, 16, 12, 8, 13, 35, 41, 9, 15, 34, 16, 30, 33, 29, 13, 23, 33, 19, 12, 35, 9, 16, 36, 36, 12, 22, 52, 22, 10, 9, 27, 33, 52, 44, 16, 9, 8, 37, 13, 8, 14, 9, 58, 62, 10, 16, 13, 35, 9, 10, 10, 40, 24, 100, 46, 9, 35, 23, 13, 18, 24, 16, 51, 38, 88, 34, 17, 22, 24, 25, 25, 30, 17, 13, 19, 17, 16, 8, 37, 12, 22, 10, 13, 10, 19, 25, 22, 56, 19, 16, 27, 16, 22, 16, 12, 10, 14, 10, 13, 100, 25, 11, 23, 17, 9, 22, 19, 16, 13, 13, 29, 31, 13, 24, 13, 9, 10, 8, 47, 29, 10, 9, 38, 19, 56, 37, 22, 10, 31, 17, 9, 24, 16, 10, 46, 21, 10, 25, 13, 50, 15, 13, 32, 13, 16, 24, 9, 30, 12, 16, 13, 13, 12, 18, 17, 8, 13, 31, 50], "policy_player_1_reward": [2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 0.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 0.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0], "policy_player_2_reward": [-2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 0.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599206728342494, "mean_inference_ms": 1.7700432931902663, "mean_action_processing_ms": 0.170185095247013, "mean_env_wait_ms": 0.11532007786859665, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011233579028736462, "ViewRequirementAgentConnector_ms": 0.21993402730334888}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.693181818181817, "episodes_this_iter": 176, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13636363636363635, "player_2": -0.13636363636363635}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [28, 13, 8, 13, 10, 10, 29, 26, 42, 19, 10, 44, 9, 13, 8, 22, 16, 29, 16, 12, 8, 13, 35, 41, 9, 15, 34, 16, 30, 33, 29, 13, 23, 33, 19, 12, 35, 9, 16, 36, 36, 12, 22, 52, 22, 10, 9, 27, 33, 52, 44, 16, 9, 8, 37, 13, 8, 14, 9, 58, 62, 10, 16, 13, 35, 9, 10, 10, 40, 24, 100, 46, 9, 35, 23, 13, 18, 24, 16, 51, 38, 88, 34, 17, 22, 24, 25, 25, 30, 17, 13, 19, 17, 16, 8, 37, 12, 22, 10, 13, 10, 19, 25, 22, 56, 19, 16, 27, 16, 22, 16, 12, 10, 14, 10, 13, 100, 25, 11, 23, 17, 9, 22, 19, 16, 13, 13, 29, 31, 13, 24, 13, 9, 10, 8, 47, 29, 10, 9, 38, 19, 56, 37, 22, 10, 31, 17, 9, 24, 16, 10, 46, 21, 10, 25, 13, 50, 15, 13, 32, 13, 16, 24, 9, 30, 12, 16, 13, 13, 12, 18, 17, 8, 13, 31, 50], "policy_player_1_reward": [2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 0.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 0.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0], "policy_player_2_reward": [-2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, 0.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.599206728342494, "mean_inference_ms": 1.7700432931902663, "mean_action_processing_ms": 0.170185095247013, "mean_env_wait_ms": 0.11532007786859665, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.011233579028736462, "ViewRequirementAgentConnector_ms": 0.21993402730334888}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 75997, "num_agent_steps_trained": 75997, "num_env_steps_sampled": 76000, "num_env_steps_trained": 76000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 257.637722996499, "num_env_steps_trained_throughput_per_sec": 257.637722996499, "timesteps_total": 76000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 75997, "timers": {"training_iteration_time_ms": 16370.412, "sample_time_ms": 3554.364, "learn_time_ms": 12806.036, "learn_throughput": 312.353, "synch_weights_time_ms": 9.463}, "counters": {"num_env_steps_sampled": 76000, "num_env_steps_trained": 76000, "num_agent_steps_sampled": 75997, "num_agent_steps_trained": 75997}, "done": false, "episodes_total": 3690, "training_iteration": 19, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-27-21", "timestamp": 1711733241, "time_this_iter_s": 19.729113578796387, "time_total_s": 395.6985659599304, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3D2D0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B095A0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3FD00>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 395.6985659599304, "iterations_since_restore": 19, "perf": {"cpu_util_percent": 9.953571428571427, "ram_util_percent": 89.58928571428574}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 15.735, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.93, "player_2": 1.1235955056179776, "player_1": 0.7747747747747747}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [8, 8, 19, 10, 12, 14, 17, 14, 18, 14, 26, 24, 10, 13, 25, 11, 23, 9, 13, 14, 11, 22, 32, 12, 13, 15, 10, 10, 26, 33, 11, 10, 13, 20, 17, 16, 13, 17, 7, 21, 15, 26, 7, 12, 8, 7, 13, 24, 8, 18, 15, 19, 7, 22, 9, 22, 9, 12, 17, 8, 12, 13, 24, 9, 9, 15, 23, 15, 10, 18, 9, 10, 18, 16, 14, 15, 7, 19, 11, 10, 11, 18, 17, 9, 13, 17, 22, 10, 27, 9, 15, 16, 22, 12, 14, 20, 16, 10, 31, 16, 23, 29, 29, 38, 12, 26, 9, 11, 10, 20, 11, 13, 7, 16, 18, 25, 24, 16, 11, 28, 28, 13, 7, 13, 23, 15, 17, 8, 19, 23, 14, 7, 9, 28, 11, 8, 11, 23, 7, 9, 8, 14, 19, 10, 13, 16, 16, 16, 14, 15, 11, 20, 19, 13, 18, 10, 10, 24, 8, 33, 16, 8, 13, 43, 16, 22, 18, 18, 9, 34, 23, 11, 11, 17, 22, 13, 11, 7, 17, 9, 12, 20, 23, 31, 17, 17, 9, 24, 7, 12, 19, 24, 16, 16, 15, 7, 19, 8, 14, 12], "policy_random_reward": [2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0], "policy_player_2_reward": [-2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0], "policy_player_1_reward": [2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, 1.0, -1.0, -1.0, 1.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6060505563173016, "mean_inference_ms": 0.9139158046539413, "mean_action_processing_ms": 0.13822586065019157, "mean_env_wait_ms": 0.09021519885999611, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00936746597290039, "ViewRequirementAgentConnector_ms": 0.23166948556900024}, "player_1_winrate": 0.7297297297297297, "player_2_winrate": 0.8202247191011236, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.4215006644527115, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5513047489027183, "policy_loss": -0.021133876834452774, "vf_loss": 1.5724081819256146, "vf_explained_var": 0.34224138110876084, "kl": 0.00974227472111064, "entropy": 0.3678317324568828, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.6875, "num_grad_updates_lifetime": 9660.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.0230163012941675, "cur_kl_coeff": 0.05000000000000001, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3884735473742087, "policy_loss": -0.012075644730066415, "vf_loss": 1.400349111109972, "vf_explained_var": 0.40924658589065077, "kl": 0.004001559274094082, "entropy": 0.29314966447030505, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.3125, "num_grad_updates_lifetime": 9360.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 80000, "num_env_steps_trained": 80000, "num_agent_steps_sampled": 79997, "num_agent_steps_trained": 79997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.540372670807454, "episode_media": {}, "episodes_this_iter": 161, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.15527950310559005, "player_2": 0.15527950310559005}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 28, 16, 13, 13, 21, 19, 13, 18, 17, 16, 42, 24, 25, 16, 22, 31, 25, 45, 13, 26, 7, 52, 16, 51, 12, 36, 31, 18, 20, 9, 9, 9, 12, 13, 10, 16, 17, 13, 58, 34, 13, 19, 9, 9, 16, 16, 13, 80, 32, 16, 12, 9, 13, 21, 24, 19, 13, 17, 40, 24, 50, 7, 79, 22, 37, 22, 30, 9, 46, 16, 9, 13, 10, 13, 13, 13, 16, 18, 13, 62, 10, 16, 17, 13, 24, 70, 41, 8, 22, 9, 11, 9, 23, 18, 13, 26, 18, 19, 9, 10, 55, 37, 9, 41, 9, 13, 19, 100, 17, 25, 20, 9, 20, 16, 15, 53, 30, 16, 11, 54, 16, 23, 30, 48, 16, 13, 18, 9, 43, 9, 10, 58, 12, 16, 13, 19, 41, 15, 16, 17, 20, 89, 22, 46, 32, 35, 39, 16, 25, 16, 54, 31, 21, 16, 25, 27, 13, 99, 16, 100], "policy_player_1_reward": [-2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 0.0], "policy_player_2_reward": [2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5990086200968907, "mean_inference_ms": 1.766457628305999, "mean_action_processing_ms": 0.1694079341264788, "mean_env_wait_ms": 0.115651538174828, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01154575288665961, "ViewRequirementAgentConnector_ms": 0.20927331462409926}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.540372670807454, "episodes_this_iter": 161, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.15527950310559005, "player_2": 0.15527950310559005}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [13, 28, 16, 13, 13, 21, 19, 13, 18, 17, 16, 42, 24, 25, 16, 22, 31, 25, 45, 13, 26, 7, 52, 16, 51, 12, 36, 31, 18, 20, 9, 9, 9, 12, 13, 10, 16, 17, 13, 58, 34, 13, 19, 9, 9, 16, 16, 13, 80, 32, 16, 12, 9, 13, 21, 24, 19, 13, 17, 40, 24, 50, 7, 79, 22, 37, 22, 30, 9, 46, 16, 9, 13, 10, 13, 13, 13, 16, 18, 13, 62, 10, 16, 17, 13, 24, 70, 41, 8, 22, 9, 11, 9, 23, 18, 13, 26, 18, 19, 9, 10, 55, 37, 9, 41, 9, 13, 19, 100, 17, 25, 20, 9, 20, 16, 15, 53, 30, 16, 11, 54, 16, 23, 30, 48, 16, 13, 18, 9, 43, 9, 10, 58, 12, 16, 13, 19, 41, 15, 16, 17, 20, 89, 22, 46, 32, 35, 39, 16, 25, 16, 54, 31, 21, 16, 25, 27, 13, 99, 16, 100], "policy_player_1_reward": [-2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 1.0, -1.0, -1.0, 2.0, -1.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 0.0], "policy_player_2_reward": [2.0, -1.0, -2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -1.0, 1.0, 1.0, -2.0, 1.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5990086200968907, "mean_inference_ms": 1.766457628305999, "mean_action_processing_ms": 0.1694079341264788, "mean_env_wait_ms": 0.115651538174828, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01154575288665961, "ViewRequirementAgentConnector_ms": 0.20927331462409926}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 79997, "num_agent_steps_trained": 79997, "num_env_steps_sampled": 80000, "num_env_steps_trained": 80000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 240.59842598999737, "num_env_steps_trained_throughput_per_sec": 240.59842598999737, "timesteps_total": 80000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 79997, "timers": {"training_iteration_time_ms": 16375.916, "sample_time_ms": 3571.016, "learn_time_ms": 12794.89, "learn_throughput": 312.625, "synch_weights_time_ms": 9.358}, "counters": {"num_env_steps_sampled": 80000, "num_env_steps_trained": 80000, "num_agent_steps_sampled": 79997, "num_agent_steps_trained": 79997}, "done": false, "episodes_total": 3851, "training_iteration": 20, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-27-41", "timestamp": 1711733261, "time_this_iter_s": 20.466078281402588, "time_total_s": 416.164644241333, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B0B640>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0B5B0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B09FC0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 416.164644241333, "iterations_since_restore": 20, "perf": {"cpu_util_percent": 10.903571428571427, "ram_util_percent": 88.73214285714286}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.175, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.5257731958762887, "random": -0.725, "player_2": 0.912621359223301}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [22, 26, 25, 20, 14, 20, 24, 28, 19, 18, 9, 6, 7, 11, 22, 18, 7, 18, 11, 16, 11, 18, 25, 8, 8, 15, 13, 7, 8, 9, 19, 13, 13, 23, 20, 29, 11, 15, 15, 9, 16, 9, 9, 15, 15, 39, 12, 43, 13, 13, 12, 14, 10, 13, 50, 9, 15, 27, 24, 16, 8, 19, 10, 25, 7, 15, 10, 13, 21, 20, 11, 8, 13, 8, 26, 14, 21, 25, 20, 18, 9, 22, 10, 7, 38, 25, 8, 11, 7, 7, 11, 16, 8, 19, 20, 16, 11, 8, 23, 19, 14, 16, 10, 21, 19, 23, 15, 36, 19, 9, 26, 7, 7, 23, 14, 7, 17, 14, 9, 11, 13, 13, 13, 15, 11, 26, 12, 34, 7, 7, 13, 12, 22, 10, 29, 21, 21, 25, 19, 13, 16, 17, 16, 18, 16, 13, 12, 20, 11, 18, 10, 19, 17, 19, 16, 21, 17, 52, 23, 8, 8, 9, 7, 15, 14, 17, 11, 17, 17, 22, 13, 26, 20, 9, 13, 13, 8, 13, 23, 15, 9, 18, 12, 14, 19, 11, 10, 17, 12, 28, 11, 12, 10, 34, 18, 12, 9, 22, 20, 24], "policy_player_1_reward": [1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -1.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0], "policy_random_reward": [-1.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0], "policy_player_2_reward": [1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6070185911720762, "mean_inference_ms": 0.9146742361237233, "mean_action_processing_ms": 0.13883931303558147, "mean_env_wait_ms": 0.09041783004422461, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.00858616828918457, "ViewRequirementAgentConnector_ms": 0.24939030408859253}, "player_1_winrate": 0.6597938144329897, "player_2_winrate": 0.7572815533980582, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.535671187937259, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.639631668229898, "policy_loss": -0.01462574907927774, "vf_loss": 1.6542379952967168, "vf_explained_var": 0.30439953990280627, "kl": 0.006217086008790102, "entropy": 0.3798732919618487, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.9375, "num_grad_updates_lifetime": 10140.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.377229742705822, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5065486185252666, "policy_loss": -0.01499568930521491, "vf_loss": 1.5214122193555037, "vf_explained_var": 0.3509372298916181, "kl": 0.00528367900433069, "entropy": 0.30919805485755203, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.0625, "num_grad_updates_lifetime": 9840.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 84000, "num_env_steps_trained": 84000, "num_agent_steps_sampled": 83997, "num_agent_steps_trained": 83997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.486187845303867, "episode_media": {}, "episodes_this_iter": 181, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.06629834254143646, "player_2": 0.06629834254143646}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [100, 31, 15, 13, 70, 17, 56, 19, 13, 68, 13, 29, 7, 16, 47, 10, 31, 16, 23, 16, 8, 9, 13, 17, 13, 16, 25, 13, 7, 19, 16, 22, 16, 73, 17, 17, 30, 9, 41, 13, 24, 13, 23, 19, 17, 16, 19, 22, 25, 46, 13, 30, 22, 33, 13, 9, 22, 9, 13, 13, 16, 38, 15, 18, 13, 13, 28, 10, 94, 22, 13, 9, 33, 16, 15, 30, 7, 46, 100, 7, 33, 13, 15, 16, 8, 23, 32, 17, 10, 16, 16, 14, 13, 16, 13, 13, 13, 23, 16, 16, 13, 16, 19, 17, 16, 9, 30, 58, 34, 8, 32, 16, 18, 13, 13, 15, 10, 27, 16, 13, 28, 20, 16, 20, 20, 37, 35, 16, 9, 17, 17, 39, 13, 60, 22, 13, 13, 17, 22, 13, 38, 19, 16, 32, 13, 38, 19, 22, 20, 17, 8, 10, 16, 16, 30, 16, 16, 20, 17, 13, 10, 14, 10, 48, 52, 36, 46, 17, 25, 62, 13, 16, 16, 13, 33, 35, 29, 16, 15, 28, 16], "policy_player_1_reward": [0.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 0.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0], "policy_player_2_reward": [0.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 0.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5985598143410016, "mean_inference_ms": 1.7626185864549841, "mean_action_processing_ms": 0.1692738081321675, "mean_env_wait_ms": 0.11518418783509006, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0153320270348649, "ViewRequirementAgentConnector_ms": 0.20691176145774884}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.486187845303867, "episodes_this_iter": 181, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.06629834254143646, "player_2": 0.06629834254143646}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [100, 31, 15, 13, 70, 17, 56, 19, 13, 68, 13, 29, 7, 16, 47, 10, 31, 16, 23, 16, 8, 9, 13, 17, 13, 16, 25, 13, 7, 19, 16, 22, 16, 73, 17, 17, 30, 9, 41, 13, 24, 13, 23, 19, 17, 16, 19, 22, 25, 46, 13, 30, 22, 33, 13, 9, 22, 9, 13, 13, 16, 38, 15, 18, 13, 13, 28, 10, 94, 22, 13, 9, 33, 16, 15, 30, 7, 46, 100, 7, 33, 13, 15, 16, 8, 23, 32, 17, 10, 16, 16, 14, 13, 16, 13, 13, 13, 23, 16, 16, 13, 16, 19, 17, 16, 9, 30, 58, 34, 8, 32, 16, 18, 13, 13, 15, 10, 27, 16, 13, 28, 20, 16, 20, 20, 37, 35, 16, 9, 17, 17, 39, 13, 60, 22, 13, 13, 17, 22, 13, 38, 19, 16, 32, 13, 38, 19, 22, 20, 17, 8, 10, 16, 16, 30, 16, 16, 20, 17, 13, 10, 14, 10, 48, 52, 36, 46, 17, 25, 62, 13, 16, 16, 13, 33, 35, 29, 16, 15, 28, 16], "policy_player_1_reward": [0.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 0.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0], "policy_player_2_reward": [0.0, 2.0, 2.0, 2.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 0.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5985598143410016, "mean_inference_ms": 1.7626185864549841, "mean_action_processing_ms": 0.1692738081321675, "mean_env_wait_ms": 0.11518418783509006, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0153320270348649, "ViewRequirementAgentConnector_ms": 0.20691176145774884}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 83997, "num_agent_steps_trained": 83997, "num_env_steps_sampled": 84000, "num_env_steps_trained": 84000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 232.1696116876583, "num_env_steps_trained_throughput_per_sec": 232.1696116876583, "timesteps_total": 84000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 83997, "timers": {"training_iteration_time_ms": 16453.376, "sample_time_ms": 3540.904, "learn_time_ms": 12902.059, "learn_throughput": 310.028, "synch_weights_time_ms": 9.556}, "counters": {"num_env_steps_sampled": 84000, "num_env_steps_trained": 84000, "num_agent_steps_sampled": 83997, "num_agent_steps_trained": 83997}, "done": false, "episodes_total": 4032, "training_iteration": 21, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-28-03", "timestamp": 1711733283, "time_this_iter_s": 21.253907918930054, "time_total_s": 437.41855216026306, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B0AE60>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0AEF0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B0B7F0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 437.41855216026306, "iterations_since_restore": 21, "perf": {"cpu_util_percent": 11.353333333333332, "ram_util_percent": 90.36666666666666}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.03, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.72, "player_2": 0.8256880733944955, "player_1": 0.5934065934065934}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 30, 7, 21, 21, 7, 13, 21, 9, 17, 10, 21, 16, 13, 29, 20, 13, 11, 19, 13, 12, 22, 13, 21, 12, 9, 15, 32, 17, 19, 19, 13, 7, 17, 34, 22, 16, 8, 27, 12, 19, 15, 9, 22, 13, 28, 7, 15, 11, 20, 14, 21, 11, 9, 11, 14, 9, 21, 16, 23, 9, 13, 28, 20, 23, 24, 21, 26, 13, 16, 20, 10, 16, 20, 7, 18, 13, 23, 26, 10, 28, 18, 20, 12, 12, 23, 17, 21, 13, 18, 15, 10, 18, 13, 9, 12, 9, 12, 12, 30, 12, 15, 27, 14, 22, 9, 20, 9, 24, 11, 11, 13, 17, 22, 25, 12, 8, 23, 7, 31, 15, 13, 10, 10, 14, 15, 21, 18, 15, 13, 11, 14, 20, 22, 18, 16, 20, 10, 16, 26, 9, 14, 12, 31, 24, 22, 8, 19, 27, 12, 13, 14, 9, 9, 11, 26, 18, 10, 17, 21, 20, 7, 23, 34, 7, 9, 12, 20, 21, 12, 9, 9, 16, 7, 13, 8, 7, 15, 7, 13, 13, 25, 12, 13, 17, 18, 8, 10, 14, 10, 10, 12, 17, 16, 28, 8, 7, 17, 23, 33], "policy_random_reward": [-2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, 2.0], "policy_player_1_reward": [1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6083569756367275, "mean_inference_ms": 0.9167357841013998, "mean_action_processing_ms": 0.13907367668073486, "mean_env_wait_ms": 0.090401563349705, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012879431247711182, "ViewRequirementAgentConnector_ms": 0.2469691038131714}, "player_1_winrate": 0.6703296703296703, "player_2_winrate": 0.7155963302752294, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.2382621283332504, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5230209324508905, "policy_loss": -0.01887206836108817, "vf_loss": 1.5418705336749554, "vf_explained_var": 0.2692000284790993, "kl": 0.007191498031685682, "entropy": 0.36794018826136987, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.1875, "num_grad_updates_lifetime": 10620.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.929338815808296, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.4669238686561585, "policy_loss": -0.013842346796445781, "vf_loss": 1.4806399472057818, "vf_explained_var": 0.2730785650511583, "kl": 0.005050859819797956, "entropy": 0.2979354262662431, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.8125, "num_grad_updates_lifetime": 10320.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 88000, "num_env_steps_trained": 88000, "num_agent_steps_sampled": 87997, "num_agent_steps_trained": 87997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.39873417721519, "episode_media": {}, "episodes_this_iter": 158, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13924050632911392, "player_2": -0.13924050632911392}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [63, 23, 22, 24, 38, 29, 22, 13, 9, 13, 16, 16, 19, 24, 9, 10, 13, 9, 13, 21, 54, 13, 17, 90, 24, 47, 8, 13, 16, 42, 82, 9, 13, 23, 12, 19, 10, 13, 58, 20, 25, 22, 42, 20, 13, 10, 18, 12, 8, 33, 31, 10, 29, 48, 22, 15, 52, 7, 21, 7, 9, 23, 20, 9, 7, 12, 7, 20, 70, 32, 16, 18, 32, 24, 30, 13, 16, 16, 16, 68, 23, 10, 56, 12, 80, 15, 44, 16, 15, 13, 9, 13, 27, 13, 16, 17, 22, 16, 17, 16, 10, 16, 13, 25, 25, 12, 16, 22, 100, 16, 35, 9, 35, 16, 67, 22, 48, 13, 26, 39, 33, 21, 7, 13, 17, 16, 22, 16, 13, 68, 13, 16, 46, 16, 31, 22, 25, 16, 13, 34, 13, 29, 9, 78, 100, 16, 56, 9, 25, 36, 16, 27, 100, 10, 18, 12, 18, 76], "policy_player_1_reward": [-1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 0.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 0.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0], "policy_player_2_reward": [1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 0.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 0.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 0.0, -2.0, -1.0, -2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6005245253106826, "mean_inference_ms": 1.7695565272671019, "mean_action_processing_ms": 0.1720241177696399, "mean_env_wait_ms": 0.11545041293008904, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015173682683630835, "ViewRequirementAgentConnector_ms": 0.23209558257573767}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.39873417721519, "episodes_this_iter": 158, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13924050632911392, "player_2": -0.13924050632911392}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [63, 23, 22, 24, 38, 29, 22, 13, 9, 13, 16, 16, 19, 24, 9, 10, 13, 9, 13, 21, 54, 13, 17, 90, 24, 47, 8, 13, 16, 42, 82, 9, 13, 23, 12, 19, 10, 13, 58, 20, 25, 22, 42, 20, 13, 10, 18, 12, 8, 33, 31, 10, 29, 48, 22, 15, 52, 7, 21, 7, 9, 23, 20, 9, 7, 12, 7, 20, 70, 32, 16, 18, 32, 24, 30, 13, 16, 16, 16, 68, 23, 10, 56, 12, 80, 15, 44, 16, 15, 13, 9, 13, 27, 13, 16, 17, 22, 16, 17, 16, 10, 16, 13, 25, 25, 12, 16, 22, 100, 16, 35, 9, 35, 16, 67, 22, 48, 13, 26, 39, 33, 21, 7, 13, 17, 16, 22, 16, 13, 68, 13, 16, 46, 16, 31, 22, 25, 16, 13, 34, 13, 29, 9, 78, 100, 16, 56, 9, 25, 36, 16, 27, 100, 10, 18, 12, 18, 76], "policy_player_1_reward": [-1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 0.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 0.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0], "policy_player_2_reward": [1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 0.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 0.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 0.0, -2.0, -1.0, -2.0, -1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6005245253106826, "mean_inference_ms": 1.7695565272671019, "mean_action_processing_ms": 0.1720241177696399, "mean_env_wait_ms": 0.11545041293008904, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.015173682683630835, "ViewRequirementAgentConnector_ms": 0.23209558257573767}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 87997, "num_agent_steps_trained": 87997, "num_env_steps_sampled": 88000, "num_env_steps_trained": 88000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 238.35997265708036, "num_env_steps_trained_throughput_per_sec": 238.35997265708036, "timesteps_total": 88000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 87997, "timers": {"training_iteration_time_ms": 16315.988, "sample_time_ms": 3551.356, "learn_time_ms": 12755.135, "learn_throughput": 313.599, "synch_weights_time_ms": 8.643}, "counters": {"num_env_steps_sampled": 88000, "num_env_steps_trained": 88000, "num_agent_steps_sampled": 87997, "num_agent_steps_trained": 87997}, "done": false, "episodes_total": 4190, "training_iteration": 22, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-28-24", "timestamp": 1711733304, "time_this_iter_s": 20.839972734451294, "time_total_s": 458.25852489471436, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B09A20>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0BD00>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B0A320>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 458.25852489471436, "iterations_since_restore": 22, "perf": {"cpu_util_percent": 10.613793103448275, "ram_util_percent": 91.84482758620686}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.52, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.935, "player_2": 0.86, "player_1": 1.01}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [9, 32, 26, 16, 13, 22, 24, 13, 16, 17, 35, 11, 21, 23, 17, 11, 31, 26, 8, 9, 19, 16, 13, 7, 8, 16, 20, 16, 15, 23, 17, 16, 19, 9, 19, 13, 21, 7, 13, 13, 20, 21, 10, 28, 34, 9, 10, 8, 22, 14, 24, 30, 9, 11, 17, 23, 20, 9, 24, 36, 24, 18, 7, 14, 28, 18, 26, 22, 9, 18, 9, 13, 16, 22, 12, 7, 18, 13, 17, 13, 12, 17, 8, 13, 19, 8, 13, 16, 9, 20, 13, 19, 16, 8, 18, 12, 18, 18, 27, 21, 16, 16, 12, 15, 18, 21, 21, 7, 10, 28, 13, 38, 13, 15, 19, 18, 10, 7, 10, 16, 15, 24, 17, 38, 14, 11, 18, 16, 10, 14, 10, 18, 27, 19, 8, 14, 13, 12, 10, 19, 16, 17, 33, 21, 10, 23, 20, 15, 16, 16, 12, 18, 16, 22, 13, 8, 10, 7, 15, 21, 13, 15, 13, 9, 9, 11, 14, 16, 14, 9, 10, 23, 20, 7, 14, 21, 12, 22, 20, 21, 10, 12, 24, 19, 17, 28, 18, 13, 16, 12, 11, 14, 16, 56, 18, 13, 11, 9, 14, 13], "policy_random_reward": [-1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, 2.0, 2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0], "policy_player_2_reward": [1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0], "policy_player_1_reward": [1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6091937426273701, "mean_inference_ms": 0.9207832699142107, "mean_action_processing_ms": 0.13921013716722752, "mean_env_wait_ms": 0.09088996501113357, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.010020077228546143, "ViewRequirementAgentConnector_ms": 0.2500760555267334}, "player_1_winrate": 0.81, "player_2_winrate": 0.75, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.4182705074548725, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.5095940493047237, "policy_loss": -0.017239723211393235, "vf_loss": 1.5268072093526521, "vf_explained_var": 0.33327926074465114, "kl": 0.008501032260401368, "entropy": 0.37861348539590833, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.6875, "num_grad_updates_lifetime": 11100.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 5.032161373396715, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.398507915313045, "policy_loss": -0.01825271190367251, "vf_loss": 1.4165947955101728, "vf_explained_var": 0.36673208276430763, "kl": 0.006633278370932754, "entropy": 0.30158438192059595, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.3125, "num_grad_updates_lifetime": 10800.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 92000, "num_env_steps_trained": 92000, "num_agent_steps_sampled": 91997, "num_agent_steps_trained": 91997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.650887573964496, "episode_media": {}, "episodes_this_iter": 169, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.10059171597633136, "player_2": 0.10059171597633136}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [46, 13, 18, 29, 12, 9, 17, 13, 25, 20, 27, 28, 12, 13, 23, 7, 32, 18, 20, 19, 16, 13, 46, 82, 9, 16, 45, 18, 40, 25, 22, 26, 33, 21, 35, 10, 13, 22, 56, 18, 17, 35, 7, 22, 13, 15, 25, 14, 35, 13, 13, 19, 25, 25, 20, 19, 20, 16, 16, 20, 13, 21, 7, 27, 25, 32, 13, 18, 22, 9, 32, 13, 28, 13, 16, 21, 42, 17, 8, 22, 24, 35, 44, 36, 10, 28, 88, 8, 52, 26, 16, 13, 38, 8, 13, 12, 13, 16, 22, 8, 12, 23, 92, 30, 35, 17, 51, 21, 13, 13, 41, 7, 30, 16, 100, 16, 7, 16, 19, 17, 26, 12, 30, 7, 30, 13, 35, 7, 27, 27, 13, 13, 16, 13, 7, 16, 17, 17, 22, 25, 12, 17, 13, 13, 24, 30, 13, 9, 16, 16, 100, 15, 27, 13, 35, 16, 23, 27, 8, 51, 13, 16, 86, 21, 54, 7, 58, 16, 34], "policy_player_1_reward": [2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 0.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 0.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 2.0, 1.0], "policy_player_2_reward": [-2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 0.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 0.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5983522204223822, "mean_inference_ms": 1.764108026834014, "mean_action_processing_ms": 0.1714854044651045, "mean_env_wait_ms": 0.11507583165032194, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008181495779364773, "ViewRequirementAgentConnector_ms": 0.19484321041220037}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.650887573964496, "episodes_this_iter": 169, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": -0.10059171597633136, "player_2": 0.10059171597633136}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [46, 13, 18, 29, 12, 9, 17, 13, 25, 20, 27, 28, 12, 13, 23, 7, 32, 18, 20, 19, 16, 13, 46, 82, 9, 16, 45, 18, 40, 25, 22, 26, 33, 21, 35, 10, 13, 22, 56, 18, 17, 35, 7, 22, 13, 15, 25, 14, 35, 13, 13, 19, 25, 25, 20, 19, 20, 16, 16, 20, 13, 21, 7, 27, 25, 32, 13, 18, 22, 9, 32, 13, 28, 13, 16, 21, 42, 17, 8, 22, 24, 35, 44, 36, 10, 28, 88, 8, 52, 26, 16, 13, 38, 8, 13, 12, 13, 16, 22, 8, 12, 23, 92, 30, 35, 17, 51, 21, 13, 13, 41, 7, 30, 16, 100, 16, 7, 16, 19, 17, 26, 12, 30, 7, 30, 13, 35, 7, 27, 27, 13, 13, 16, 13, 7, 16, 17, 17, 22, 25, 12, 17, 13, 13, 24, 30, 13, 9, 16, 16, 100, 15, 27, 13, 35, 16, 23, 27, 8, 51, 13, 16, 86, 21, 54, 7, 58, 16, 34], "policy_player_1_reward": [2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 0.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 0.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, 1.0, 2.0, 1.0], "policy_player_2_reward": [-2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 0.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 0.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, 2.0, -1.0, -2.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5983522204223822, "mean_inference_ms": 1.764108026834014, "mean_action_processing_ms": 0.1714854044651045, "mean_env_wait_ms": 0.11507583165032194, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008181495779364773, "ViewRequirementAgentConnector_ms": 0.19484321041220037}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 91997, "num_agent_steps_trained": 91997, "num_env_steps_sampled": 92000, "num_env_steps_trained": 92000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 247.8230897095479, "num_env_steps_trained_throughput_per_sec": 247.8230897095479, "timesteps_total": 92000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 91997, "timers": {"training_iteration_time_ms": 16317.389, "sample_time_ms": 3539.112, "learn_time_ms": 12768.148, "learn_throughput": 313.28, "synch_weights_time_ms": 9.274}, "counters": {"num_env_steps_sampled": 92000, "num_env_steps_trained": 92000, "num_agent_steps_sampled": 91997, "num_agent_steps_trained": 91997}, "done": false, "episodes_total": 4359, "training_iteration": 23, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-28-45", "timestamp": 1711733325, "time_this_iter_s": 20.603526830673218, "time_total_s": 478.8620517253876, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1BBD1B0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1BBD2D0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3EC20>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 478.8620517253876, "iterations_since_restore": 23, "perf": {"cpu_util_percent": 10.558620689655173, "ram_util_percent": 90.73103448275863}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.66, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 1.0, "random": -0.95, "player_2": 0.9}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [10, 13, 22, 11, 11, 21, 18, 7, 16, 15, 18, 11, 9, 30, 23, 27, 15, 23, 11, 15, 16, 8, 25, 22, 10, 28, 14, 13, 34, 18, 8, 17, 30, 15, 18, 18, 10, 18, 16, 52, 12, 11, 19, 14, 13, 13, 12, 12, 8, 16, 11, 12, 7, 16, 9, 23, 14, 17, 21, 19, 19, 32, 16, 18, 17, 13, 15, 11, 11, 10, 19, 12, 19, 14, 18, 15, 14, 9, 34, 7, 18, 17, 18, 17, 15, 16, 34, 10, 21, 24, 17, 19, 13, 16, 12, 24, 22, 22, 8, 12, 37, 10, 19, 7, 21, 9, 27, 13, 11, 20, 9, 8, 10, 7, 10, 13, 22, 10, 30, 13, 20, 9, 9, 21, 20, 12, 30, 17, 30, 10, 10, 16, 18, 20, 21, 17, 13, 20, 10, 29, 17, 19, 16, 18, 26, 23, 9, 16, 30, 20, 24, 10, 16, 17, 26, 14, 16, 22, 15, 12, 13, 12, 9, 19, 36, 21, 7, 15, 16, 15, 7, 21, 13, 15, 20, 13, 21, 7, 10, 16, 33, 14, 10, 17, 17, 9, 7, 18, 15, 10, 28, 10, 15, 11, 24, 7, 12, 14, 18, 40], "policy_player_1_reward": [2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 1.0, -1.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0], "policy_random_reward": [-2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -1.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6097723348314352, "mean_inference_ms": 0.9216547592532371, "mean_action_processing_ms": 0.13927960296691336, "mean_env_wait_ms": 0.09087156593666477, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009077906608581543, "ViewRequirementAgentConnector_ms": 0.25451385974884033}, "player_1_winrate": 0.79, "player_2_winrate": 0.75, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.118642018238703, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.4367010147621235, "policy_loss": -0.018815159205648038, "vf_loss": 1.4554895306626956, "vf_explained_var": 0.19766658134758472, "kl": 0.008525901835817775, "entropy": 0.3590397389605641, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.1875, "num_grad_updates_lifetime": 11580.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.5676127701997755, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.4059137595196565, "policy_loss": -0.015918644649597507, "vf_loss": 1.4216972433030606, "vf_explained_var": 0.19746972993016243, "kl": 0.005406303112880072, "entropy": 0.2986827063684662, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.8125, "num_grad_updates_lifetime": 11280.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 96000, "num_env_steps_trained": 96000, "num_agent_steps_sampled": 95997, "num_agent_steps_trained": 95997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 26.091503267973856, "episode_media": {}, "episodes_this_iter": 153, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13725490196078433, "player_2": -0.13725490196078433}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [100, 16, 37, 7, 24, 16, 23, 9, 13, 13, 16, 24, 19, 10, 8, 100, 16, 14, 16, 23, 9, 8, 20, 17, 60, 35, 34, 13, 100, 100, 13, 70, 16, 25, 13, 16, 7, 39, 21, 16, 39, 20, 52, 21, 62, 8, 17, 35, 16, 18, 16, 13, 24, 8, 60, 26, 27, 16, 18, 14, 17, 13, 13, 18, 16, 16, 9, 13, 19, 16, 16, 13, 32, 48, 13, 32, 17, 35, 58, 18, 49, 8, 8, 30, 56, 17, 16, 100, 65, 13, 36, 7, 17, 36, 24, 25, 32, 9, 31, 13, 8, 13, 28, 44, 24, 8, 38, 16, 13, 13, 13, 72, 17, 16, 23, 36, 22, 31, 12, 24, 29, 7, 17, 17, 39, 16, 7, 78, 29, 81, 7, 9, 13, 22, 52, 16, 16, 68, 55, 13, 23, 16, 19, 24, 25, 7, 22, 13, 18, 30, 16, 30, 17], "policy_player_1_reward": [0.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 0.0, 0.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 0.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0], "policy_player_2_reward": [0.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 0.0, 0.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 0.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5988104039422065, "mean_inference_ms": 1.7704914593810668, "mean_action_processing_ms": 0.17281930142066013, "mean_env_wait_ms": 0.11549023564173387, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014096615361232384, "ViewRequirementAgentConnector_ms": 0.21394023708268708}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 26.091503267973856, "episodes_this_iter": 153, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.13725490196078433, "player_2": -0.13725490196078433}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [100, 16, 37, 7, 24, 16, 23, 9, 13, 13, 16, 24, 19, 10, 8, 100, 16, 14, 16, 23, 9, 8, 20, 17, 60, 35, 34, 13, 100, 100, 13, 70, 16, 25, 13, 16, 7, 39, 21, 16, 39, 20, 52, 21, 62, 8, 17, 35, 16, 18, 16, 13, 24, 8, 60, 26, 27, 16, 18, 14, 17, 13, 13, 18, 16, 16, 9, 13, 19, 16, 16, 13, 32, 48, 13, 32, 17, 35, 58, 18, 49, 8, 8, 30, 56, 17, 16, 100, 65, 13, 36, 7, 17, 36, 24, 25, 32, 9, 31, 13, 8, 13, 28, 44, 24, 8, 38, 16, 13, 13, 13, 72, 17, 16, 23, 36, 22, 31, 12, 24, 29, 7, 17, 17, 39, 16, 7, 78, 29, 81, 7, 9, 13, 22, 52, 16, 16, 68, 55, 13, 23, 16, 19, 24, 25, 7, 22, 13, 18, 30, 16, 30, 17], "policy_player_1_reward": [0.0, 2.0, -1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 2.0, -2.0, 0.0, 0.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 0.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -1.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, -2.0], "policy_player_2_reward": [0.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 0.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -2.0, 2.0, 0.0, 0.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 0.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -1.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5988104039422065, "mean_inference_ms": 1.7704914593810668, "mean_action_processing_ms": 0.17281930142066013, "mean_env_wait_ms": 0.11549023564173387, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014096615361232384, "ViewRequirementAgentConnector_ms": 0.21394023708268708}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 95997, "num_agent_steps_trained": 95997, "num_env_steps_sampled": 96000, "num_env_steps_trained": 96000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 232.6628277678768, "num_env_steps_trained_throughput_per_sec": 232.6628277678768, "timesteps_total": 96000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 95997, "timers": {"training_iteration_time_ms": 16411.497, "sample_time_ms": 3506.591, "learn_time_ms": 12894.586, "learn_throughput": 310.208, "synch_weights_time_ms": 9.465}, "counters": {"num_env_steps_sampled": 96000, "num_env_steps_trained": 96000, "num_agent_steps_sampled": 95997, "num_agent_steps_trained": 95997}, "done": false, "episodes_total": 4512, "training_iteration": 24, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-29-06", "timestamp": 1711733346, "time_this_iter_s": 21.379050731658936, "time_total_s": 500.2411024570465, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B095A0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0B370>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B084C0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 500.2411024570465, "iterations_since_restore": 24, "perf": {"cpu_util_percent": 10.690000000000001, "ram_util_percent": 93.67333333333333}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.695, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.98, "player_2": 0.9901960784313726, "player_1": 0.9693877551020408}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [14, 9, 8, 11, 16, 17, 10, 11, 26, 27, 23, 9, 12, 32, 12, 30, 20, 12, 13, 8, 28, 13, 32, 20, 11, 19, 13, 13, 16, 19, 11, 23, 17, 16, 15, 13, 10, 8, 15, 28, 9, 30, 11, 20, 13, 12, 15, 17, 20, 10, 18, 35, 15, 33, 30, 18, 13, 15, 23, 8, 36, 13, 29, 22, 26, 17, 13, 31, 8, 9, 13, 15, 15, 9, 12, 19, 10, 24, 15, 10, 14, 9, 7, 33, 25, 32, 8, 10, 13, 17, 13, 20, 16, 16, 24, 16, 15, 19, 18, 10, 9, 7, 22, 12, 13, 14, 22, 19, 15, 20, 18, 16, 14, 10, 20, 18, 12, 64, 17, 10, 13, 15, 9, 29, 17, 14, 14, 34, 17, 24, 10, 11, 19, 16, 29, 10, 22, 8, 26, 23, 15, 9, 15, 21, 33, 16, 21, 28, 11, 10, 36, 9, 15, 7, 11, 14, 38, 9, 22, 10, 15, 9, 10, 18, 18, 12, 16, 23, 15, 13, 12, 10, 23, 15, 14, 13, 34, 9, 7, 13, 7, 13, 14, 22, 22, 12, 17, 9, 48, 8, 13, 8, 13, 9, 14, 9, 7, 7, 12, 8], "policy_random_reward": [2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 1.0, -2.0, 2.0, -1.0, -1.0, -1.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0], "policy_player_2_reward": [-2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0], "policy_player_1_reward": [2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 1.0, -2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6114512358795882, "mean_inference_ms": 0.9245267047368506, "mean_action_processing_ms": 0.13963215479599736, "mean_env_wait_ms": 0.09126239508229253, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.012679636478424072, "ViewRequirementAgentConnector_ms": 0.27394354343414307}, "player_1_winrate": 0.7857142857142857, "player_2_winrate": 0.7843137254901961, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.4781795054674145, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.387789368381103, "policy_loss": -0.019683258259222687, "vf_loss": 1.4074455539385478, "vf_explained_var": 0.4011197621623675, "kl": 0.008660895123066706, "entropy": 0.3742673186585307, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.25, "num_grad_updates_lifetime": 12060.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.5301632588108385, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2199125123520693, "policy_loss": -0.016959009032386046, "vf_loss": 1.2367348606387774, "vf_explained_var": 0.46514730180303254, "kl": 0.005466761280418203, "entropy": 0.29351043337956073, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.75, "num_grad_updates_lifetime": 11760.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 100000, "num_env_steps_trained": 100000, "num_agent_steps_sampled": 99997, "num_agent_steps_trained": 99997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.017241379310345, "episode_media": {}, "episodes_this_iter": 174, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.29310344827586204, "player_2": -0.29310344827586204}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 17, 8, 16, 12, 17, 24, 13, 13, 24, 27, 35, 38, 17, 40, 56, 15, 44, 16, 25, 7, 19, 44, 16, 13, 16, 16, 8, 8, 16, 24, 16, 22, 7, 86, 24, 30, 100, 48, 13, 12, 8, 13, 40, 12, 16, 16, 18, 12, 23, 22, 25, 18, 19, 27, 40, 54, 16, 13, 12, 31, 16, 22, 41, 21, 9, 24, 100, 19, 16, 8, 16, 9, 13, 13, 27, 16, 24, 13, 13, 25, 17, 18, 16, 31, 12, 37, 7, 13, 12, 22, 10, 24, 16, 17, 17, 24, 12, 22, 22, 17, 29, 12, 16, 16, 17, 17, 33, 16, 16, 44, 17, 27, 43, 7, 10, 16, 10, 16, 19, 16, 16, 13, 13, 100, 16, 35, 91, 17, 36, 12, 8, 21, 22, 35, 8, 53, 16, 16, 12, 40, 18, 20, 74, 8, 24, 24, 19, 35, 16, 28, 13, 9, 23, 18, 45, 43, 30, 16, 16, 13, 29, 42, 7, 16, 22, 19, 20, 12, 17, 17, 16, 37, 18], "policy_player_1_reward": [2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 0.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 0.0, 1.0, -2.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0], "policy_player_2_reward": [-2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 0.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 0.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5988501807282605, "mean_inference_ms": 1.7698637601131473, "mean_action_processing_ms": 0.17230108567800115, "mean_env_wait_ms": 0.11589344126467781, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0114978044882588, "ViewRequirementAgentConnector_ms": 0.21435570442813567}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.017241379310345, "episodes_this_iter": 174, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.29310344827586204, "player_2": -0.29310344827586204}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 17, 8, 16, 12, 17, 24, 13, 13, 24, 27, 35, 38, 17, 40, 56, 15, 44, 16, 25, 7, 19, 44, 16, 13, 16, 16, 8, 8, 16, 24, 16, 22, 7, 86, 24, 30, 100, 48, 13, 12, 8, 13, 40, 12, 16, 16, 18, 12, 23, 22, 25, 18, 19, 27, 40, 54, 16, 13, 12, 31, 16, 22, 41, 21, 9, 24, 100, 19, 16, 8, 16, 9, 13, 13, 27, 16, 24, 13, 13, 25, 17, 18, 16, 31, 12, 37, 7, 13, 12, 22, 10, 24, 16, 17, 17, 24, 12, 22, 22, 17, 29, 12, 16, 16, 17, 17, 33, 16, 16, 44, 17, 27, 43, 7, 10, 16, 10, 16, 19, 16, 16, 13, 13, 100, 16, 35, 91, 17, 36, 12, 8, 21, 22, 35, 8, 53, 16, 16, 12, 40, 18, 20, 74, 8, 24, 24, 19, 35, 16, 28, 13, 9, 23, 18, 45, 43, 30, 16, 16, 13, 29, 42, 7, 16, 22, 19, 20, 12, 17, 17, 16, 37, 18], "policy_player_1_reward": [2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 0.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 0.0, 1.0, -2.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0], "policy_player_2_reward": [-2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -1.0, -1.0, 0.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 0.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5988501807282605, "mean_inference_ms": 1.7698637601131473, "mean_action_processing_ms": 0.17230108567800115, "mean_env_wait_ms": 0.11589344126467781, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.0114978044882588, "ViewRequirementAgentConnector_ms": 0.21435570442813567}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 99997, "num_agent_steps_trained": 99997, "num_env_steps_sampled": 100000, "num_env_steps_trained": 100000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 244.2665260731468, "num_env_steps_trained_throughput_per_sec": 244.2665260731468, "timesteps_total": 100000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 99997, "timers": {"training_iteration_time_ms": 16578.452, "sample_time_ms": 3571.637, "learn_time_ms": 12996.3, "learn_throughput": 307.78, "synch_weights_time_ms": 9.561}, "counters": {"num_env_steps_sampled": 100000, "num_env_steps_trained": 100000, "num_agent_steps_sampled": 99997, "num_agent_steps_trained": 99997}, "done": false, "episodes_total": 4686, "training_iteration": 25, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-29-27", "timestamp": 1711733367, "time_this_iter_s": 20.792662858963013, "time_total_s": 521.0337653160095, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B09A20>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0B640>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B0AF80>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 521.0337653160095, "iterations_since_restore": 25, "perf": {"cpu_util_percent": 10.43, "ram_util_percent": 91.57666666666667}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.93, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 1.1157894736842104, "random": -0.99, "player_2": 0.8761904761904762}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 13, 18, 17, 9, 27, 21, 8, 15, 8, 20, 18, 8, 15, 26, 25, 23, 17, 19, 16, 16, 60, 15, 10, 27, 13, 14, 17, 22, 9, 13, 28, 17, 7, 20, 7, 23, 20, 10, 6, 13, 24, 28, 12, 13, 20, 14, 9, 40, 15, 13, 7, 27, 24, 9, 13, 15, 11, 36, 7, 16, 13, 27, 15, 8, 14, 15, 14, 22, 38, 58, 12, 19, 14, 14, 22, 22, 9, 17, 18, 18, 30, 24, 9, 15, 13, 26, 8, 13, 10, 34, 9, 22, 7, 28, 13, 12, 13, 16, 12, 9, 15, 13, 20, 11, 12, 10, 19, 21, 12, 8, 7, 11, 8, 20, 46, 12, 32, 9, 16, 17, 9, 23, 29, 14, 8, 13, 28, 18, 13, 19, 12, 11, 11, 14, 18, 24, 13, 10, 21, 13, 17, 9, 11, 40, 12, 16, 14, 14, 20, 14, 7, 7, 7, 18, 27, 10, 16, 34, 46, 14, 13, 24, 17, 22, 14, 13, 8, 11, 12, 18, 8, 15, 16, 11, 15, 24, 8, 12, 27, 16, 20, 20, 7, 9, 10, 18, 17, 15, 12, 14, 20, 15, 16, 9, 7, 13, 35, 36, 34], "policy_player_1_reward": [-2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, 2.0], "policy_random_reward": [2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0], "policy_player_2_reward": [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6119496487903995, "mean_inference_ms": 0.9250261507852915, "mean_action_processing_ms": 0.13891733923463254, "mean_env_wait_ms": 0.09098459496772673, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014457643032073975, "ViewRequirementAgentConnector_ms": 0.2556301951408386}, "player_1_winrate": 0.8315789473684211, "player_2_winrate": 0.7428571428571429, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.453647497296333, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.431444134687384, "policy_loss": -0.018844358761270996, "vf_loss": 1.4502699388811986, "vf_explained_var": 0.2257527453203996, "kl": 0.005937297416608089, "entropy": 0.352190045081079, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 126.75, "num_grad_updates_lifetime": 12540.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.540145387748877, "cur_kl_coeff": 0.025000000000000005, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3915501055618127, "policy_loss": -0.013355677968259745, "vf_loss": 1.4047817928095658, "vf_explained_var": 0.2247575379908085, "kl": 0.0049594611326701475, "entropy": 0.28689110598837336, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 123.25, "num_grad_updates_lifetime": 12240.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 104000, "num_env_steps_trained": 104000, "num_agent_steps_sampled": 103997, "num_agent_steps_trained": 103997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.666666666666668, "episode_media": {}, "episodes_this_iter": 159, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.48427672955974843, "player_2": -0.48427672955974843}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 52, 27, 16, 100, 18, 23, 13, 12, 14, 8, 23, 9, 16, 28, 17, 35, 13, 39, 18, 22, 22, 16, 22, 17, 16, 17, 21, 16, 16, 68, 23, 38, 28, 12, 7, 16, 29, 16, 7, 17, 30, 8, 23, 52, 100, 22, 16, 19, 20, 13, 35, 13, 13, 22, 28, 16, 9, 22, 16, 23, 34, 18, 13, 18, 88, 80, 7, 13, 29, 84, 22, 20, 16, 16, 13, 8, 25, 29, 31, 10, 8, 12, 8, 10, 18, 22, 12, 16, 16, 12, 8, 14, 22, 13, 7, 100, 31, 23, 27, 30, 38, 8, 32, 8, 40, 13, 31, 40, 13, 16, 22, 18, 38, 8, 21, 52, 7, 42, 18, 17, 34, 12, 35, 17, 10, 74, 86, 44, 13, 13, 16, 46, 21, 27, 30, 30, 8, 13, 52, 70, 16, 16, 84, 8, 40, 17, 13, 8, 13, 19, 16, 10, 18, 16, 21, 18, 13, 17], "policy_player_1_reward": [2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 0.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0], "policy_player_2_reward": [-2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6008563676160357, "mean_inference_ms": 1.7770449995182054, "mean_action_processing_ms": 0.17283784400136054, "mean_env_wait_ms": 0.11632370247371972, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014309688184246327, "ViewRequirementAgentConnector_ms": 0.23091206760526453}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.666666666666668, "episodes_this_iter": 159, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.48427672955974843, "player_2": -0.48427672955974843}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [16, 52, 27, 16, 100, 18, 23, 13, 12, 14, 8, 23, 9, 16, 28, 17, 35, 13, 39, 18, 22, 22, 16, 22, 17, 16, 17, 21, 16, 16, 68, 23, 38, 28, 12, 7, 16, 29, 16, 7, 17, 30, 8, 23, 52, 100, 22, 16, 19, 20, 13, 35, 13, 13, 22, 28, 16, 9, 22, 16, 23, 34, 18, 13, 18, 88, 80, 7, 13, 29, 84, 22, 20, 16, 16, 13, 8, 25, 29, 31, 10, 8, 12, 8, 10, 18, 22, 12, 16, 16, 12, 8, 14, 22, 13, 7, 100, 31, 23, 27, 30, 38, 8, 32, 8, 40, 13, 31, 40, 13, 16, 22, 18, 38, 8, 21, 52, 7, 42, 18, 17, 34, 12, 35, 17, 10, 74, 86, 44, 13, 13, 16, 46, 21, 27, 30, 30, 8, 13, 52, 70, 16, 16, 84, 8, 40, 17, 13, 8, 13, 19, 16, 10, 18, 16, 21, 18, 13, 17], "policy_player_1_reward": [2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, 2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 0.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -1.0], "policy_player_2_reward": [-2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 1.0, -2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, 1.0, 2.0, -1.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6008563676160357, "mean_inference_ms": 1.7770449995182054, "mean_action_processing_ms": 0.17283784400136054, "mean_env_wait_ms": 0.11632370247371972, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014309688184246327, "ViewRequirementAgentConnector_ms": 0.23091206760526453}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 103997, "num_agent_steps_trained": 103997, "num_env_steps_sampled": 104000, "num_env_steps_trained": 104000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 222.49131606103114, "num_env_steps_trained_throughput_per_sec": 222.49131606103114, "timesteps_total": 104000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 103997, "timers": {"training_iteration_time_ms": 16693.999, "sample_time_ms": 3656.408, "learn_time_ms": 13026.955, "learn_throughput": 307.056, "synch_weights_time_ms": 9.782}, "counters": {"num_env_steps_sampled": 104000, "num_env_steps_trained": 104000, "num_agent_steps_sampled": 103997, "num_agent_steps_trained": 103997}, "done": false, "episodes_total": 4845, "training_iteration": 26, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-29-49", "timestamp": 1711733389, "time_this_iter_s": 22.16775369644165, "time_total_s": 543.2015190124512, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3FF40>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3E050>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B0BD90>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 543.2015190124512, "iterations_since_restore": 26, "perf": {"cpu_util_percent": 11.393548387096773, "ram_util_percent": 89.94516129032257}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.31, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.67, "player_2": 0.5306122448979592, "player_1": 0.803921568627451}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 16, 16, 19, 9, 19, 13, 16, 8, 8, 28, 15, 22, 12, 13, 18, 18, 18, 18, 20, 11, 17, 21, 9, 64, 23, 13, 14, 8, 36, 18, 9, 13, 15, 26, 7, 11, 16, 18, 13, 11, 11, 14, 22, 8, 18, 20, 22, 9, 17, 11, 34, 21, 8, 21, 32, 24, 24, 22, 13, 12, 9, 24, 10, 16, 16, 13, 17, 18, 15, 16, 20, 6, 11, 11, 7, 9, 18, 12, 37, 23, 44, 39, 22, 14, 13, 40, 18, 26, 13, 24, 13, 9, 16, 29, 17, 41, 16, 31, 27, 7, 23, 8, 9, 13, 13, 12, 22, 18, 18, 9, 40, 9, 30, 32, 9, 8, 12, 13, 17, 11, 13, 7, 13, 12, 10, 9, 16, 16, 9, 16, 20, 20, 13, 21, 16, 18, 28, 29, 10, 15, 21, 13, 24, 12, 27, 16, 23, 10, 16, 7, 26, 13, 10, 16, 18, 16, 17, 9, 15, 20, 44, 14, 13, 52, 14, 11, 16, 11, 15, 28, 13, 8, 8, 10, 7, 10, 15, 36, 6, 29, 18, 22, 14, 16, 21, 24, 7, 17, 19, 15, 18, 18, 23, 15, 17, 10, 12, 14, 20], "policy_random_reward": [-2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -1.0, 2.0, -1.0, -2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -1.0, 1.0, -1.0, 2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -1.0, 1.0, -2.0, 1.0, 1.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -2.0, 1.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0], "policy_player_2_reward": [2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -1.0], "policy_player_1_reward": [2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 2.0, -1.0, 1.0, -1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6125121080827881, "mean_inference_ms": 0.9282127008353394, "mean_action_processing_ms": 0.1392421653798681, "mean_env_wait_ms": 0.09126239110100774, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014646351337432861, "ViewRequirementAgentConnector_ms": 0.26572322845458984}, "player_1_winrate": 0.7352941176470589, "player_2_winrate": 0.6632653061224489, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.359433860580126, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3490965048472086, "policy_loss": -0.022830407017318068, "vf_loss": 1.3718990998963514, "vf_explained_var": 0.2819579238692919, "kl": 0.008897131834956218, "entropy": 0.34013777747750285, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.375, "num_grad_updates_lifetime": 13020.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.3370898276567456, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2799019500613213, "policy_loss": -0.01976031281859226, "vf_loss": 1.2995588033149639, "vf_explained_var": 0.31011385036011535, "kl": 0.008276913003824483, "entropy": 0.2757507655148705, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.625, "num_grad_updates_lifetime": 12720.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 108000, "num_env_steps_trained": 108000, "num_agent_steps_sampled": 107997, "num_agent_steps_trained": 107997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.898089171974522, "episode_media": {}, "episodes_this_iter": 157, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.050955414012738856, "player_2": -0.050955414012738856}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [40, 72, 82, 8, 45, 100, 30, 24, 62, 43, 19, 17, 46, 29, 34, 17, 8, 18, 16, 33, 22, 12, 41, 17, 22, 12, 16, 23, 17, 11, 36, 13, 48, 19, 7, 52, 22, 18, 7, 16, 17, 10, 58, 13, 58, 13, 22, 16, 19, 16, 16, 10, 19, 8, 13, 9, 17, 23, 40, 13, 13, 8, 13, 27, 8, 7, 17, 31, 20, 13, 17, 9, 34, 8, 38, 25, 33, 9, 12, 38, 94, 94, 22, 13, 16, 13, 17, 32, 13, 39, 24, 13, 13, 16, 8, 33, 74, 17, 16, 22, 9, 34, 35, 17, 12, 89, 54, 7, 41, 36, 16, 15, 18, 9, 13, 10, 16, 8, 21, 34, 8, 22, 16, 17, 100, 8, 19, 18, 21, 12, 31, 17, 17, 24, 9, 23, 53, 13, 17, 23, 16, 7, 21, 32, 16, 58, 81, 13, 10, 16, 8, 100, 100, 12, 32, 16, 13], "policy_player_1_reward": [2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 2.0, 1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 0.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, -1.0], "policy_player_2_reward": [-2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 0.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 0.0, 0.0, -2.0, -2.0, -2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6038748342555804, "mean_inference_ms": 1.787216387642737, "mean_action_processing_ms": 0.17391270252470864, "mean_env_wait_ms": 0.11692446323395089, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014868283727366453, "ViewRequirementAgentConnector_ms": 0.2605195258073746}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.898089171974522, "episodes_this_iter": 157, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.050955414012738856, "player_2": -0.050955414012738856}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [40, 72, 82, 8, 45, 100, 30, 24, 62, 43, 19, 17, 46, 29, 34, 17, 8, 18, 16, 33, 22, 12, 41, 17, 22, 12, 16, 23, 17, 11, 36, 13, 48, 19, 7, 52, 22, 18, 7, 16, 17, 10, 58, 13, 58, 13, 22, 16, 19, 16, 16, 10, 19, 8, 13, 9, 17, 23, 40, 13, 13, 8, 13, 27, 8, 7, 17, 31, 20, 13, 17, 9, 34, 8, 38, 25, 33, 9, 12, 38, 94, 94, 22, 13, 16, 13, 17, 32, 13, 39, 24, 13, 13, 16, 8, 33, 74, 17, 16, 22, 9, 34, 35, 17, 12, 89, 54, 7, 41, 36, 16, 15, 18, 9, 13, 10, 16, 8, 21, 34, 8, 22, 16, 17, 100, 8, 19, 18, 21, 12, 31, 17, 17, 24, 9, 23, 53, 13, 17, 23, 16, 7, 21, 32, 16, 58, 81, 13, 10, 16, 8, 100, 100, 12, 32, 16, 13], "policy_player_1_reward": [2.0, 1.0, 1.0, 2.0, -2.0, 0.0, 2.0, 1.0, 1.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 1.0, -2.0, 0.0, 2.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -1.0, 2.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, -1.0], "policy_player_2_reward": [-2.0, -1.0, -1.0, -2.0, 2.0, 0.0, -2.0, -1.0, -1.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, 1.0, -2.0, -1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 2.0, -2.0, -1.0, -1.0, -1.0, -2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -1.0, 2.0, 0.0, -2.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 0.0, 0.0, -2.0, -2.0, -2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6038748342555804, "mean_inference_ms": 1.787216387642737, "mean_action_processing_ms": 0.17391270252470864, "mean_env_wait_ms": 0.11692446323395089, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014868283727366453, "ViewRequirementAgentConnector_ms": 0.2605195258073746}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 107997, "num_agent_steps_trained": 107997, "num_env_steps_sampled": 108000, "num_env_steps_trained": 108000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 237.31225722467335, "num_env_steps_trained_throughput_per_sec": 237.31225722467335, "timesteps_total": 108000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 107997, "timers": {"training_iteration_time_ms": 16772.251, "sample_time_ms": 3756.12, "learn_time_ms": 13005.485, "learn_throughput": 307.563, "synch_weights_time_ms": 9.892}, "counters": {"num_env_steps_sampled": 108000, "num_env_steps_trained": 108000, "num_agent_steps_sampled": 107997, "num_agent_steps_trained": 107997}, "done": false, "episodes_total": 5002, "training_iteration": 27, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-30-11", "timestamp": 1711733411, "time_this_iter_s": 21.43398690223694, "time_total_s": 564.6355059146881, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3E680>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0BBE0>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C187E170>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 564.6355059146881, "iterations_since_restore": 27, "perf": {"cpu_util_percent": 11.696666666666665, "ram_util_percent": 92.85999999999999}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 16.56, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 1.0, "random": -1.05, "player_2": 1.099009900990099}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [12, 13, 8, 12, 11, 16, 12, 13, 12, 7, 12, 14, 8, 7, 18, 22, 7, 20, 35, 18, 17, 16, 17, 7, 25, 15, 7, 25, 32, 13, 15, 13, 13, 24, 24, 6, 16, 13, 10, 7, 22, 7, 9, 19, 10, 20, 11, 11, 12, 22, 10, 10, 46, 12, 21, 13, 13, 10, 10, 9, 13, 8, 11, 7, 12, 31, 17, 11, 9, 11, 22, 23, 7, 11, 29, 7, 10, 11, 11, 22, 24, 21, 22, 16, 15, 25, 13, 10, 17, 8, 28, 34, 10, 22, 9, 13, 21, 7, 20, 24, 11, 30, 15, 17, 33, 19, 30, 14, 16, 12, 22, 12, 28, 16, 18, 9, 15, 13, 10, 11, 15, 15, 17, 22, 23, 23, 10, 12, 16, 21, 16, 15, 9, 25, 36, 12, 18, 23, 37, 12, 13, 7, 12, 12, 27, 29, 23, 23, 26, 7, 10, 17, 9, 19, 12, 35, 25, 46, 14, 28, 10, 20, 11, 16, 24, 22, 10, 16, 14, 30, 12, 18, 19, 34, 12, 7, 12, 9, 22, 13, 10, 7, 22, 7, 20, 13, 21, 8, 8, 15, 11, 33, 28, 17, 11, 9, 14, 10, 16, 48], "policy_player_1_reward": [2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 2.0], "policy_random_reward": [-2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -1.0, 1.0, -2.0, 1.0, -1.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -1.0, 1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, -1.0, 2.0, -1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6098193194746506, "mean_inference_ms": 0.9231207300417523, "mean_action_processing_ms": 0.13871160942436953, "mean_env_wait_ms": 0.09061241925583903, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009602069854736328, "ViewRequirementAgentConnector_ms": 0.2267695665359497}, "player_1_winrate": 0.8080808080808081, "player_2_winrate": 0.801980198019802, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.159537373483181, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3497217083970705, "policy_loss": -0.01997847237265281, "vf_loss": 1.3696711013714473, "vf_explained_var": 0.17450507134199142, "kl": 0.009305590561267141, "entropy": 0.33664789612715446, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 126.8125, "num_grad_updates_lifetime": 13500.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.1493596081932385, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2841726381331682, "policy_loss": -0.01507156903874905, "vf_loss": 1.2991763183226188, "vf_explained_var": 0.18484040920933087, "kl": 0.0054309877981164306, "entropy": 0.28323613296573363, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 123.1875, "num_grad_updates_lifetime": 13200.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 112000, "num_env_steps_trained": 112000, "num_agent_steps_sampled": 111997, "num_agent_steps_trained": 111997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.746987951807228, "episode_media": {}, "episodes_this_iter": 166, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.4578313253012048, "player_2": -0.4578313253012048}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [34, 16, 11, 16, 8, 8, 12, 12, 17, 13, 40, 16, 32, 22, 8, 39, 8, 35, 12, 8, 13, 11, 13, 10, 94, 16, 16, 7, 22, 13, 17, 16, 12, 100, 9, 13, 13, 42, 54, 12, 16, 27, 24, 17, 37, 28, 17, 24, 42, 8, 100, 8, 20, 16, 24, 10, 33, 10, 43, 9, 12, 22, 12, 100, 23, 22, 16, 16, 35, 16, 12, 44, 62, 8, 13, 10, 40, 24, 27, 31, 17, 7, 36, 38, 12, 7, 12, 32, 7, 12, 35, 7, 13, 100, 13, 16, 26, 13, 16, 16, 16, 19, 16, 50, 7, 22, 62, 48, 22, 10, 26, 17, 28, 53, 16, 13, 23, 16, 10, 21, 18, 12, 48, 7, 18, 18, 8, 33, 10, 22, 25, 22, 22, 12, 12, 13, 16, 16, 44, 32, 17, 19, 21, 23, 80, 31, 9, 13, 28, 8, 32, 39, 39, 67, 22, 16, 50, 17, 13, 22, 16, 25, 12, 31, 12, 14], "policy_player_1_reward": [1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 0.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 0.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 0.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0], "policy_player_2_reward": [-1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 0.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -2.0, 0.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 0.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 0.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.603864811962619, "mean_inference_ms": 1.7904661901447765, "mean_action_processing_ms": 0.1738929915720848, "mean_env_wait_ms": 0.11712015475088569, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014909635107201266, "ViewRequirementAgentConnector_ms": 0.20545038832239357}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 23.746987951807228, "episodes_this_iter": 166, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.4578313253012048, "player_2": -0.4578313253012048}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [34, 16, 11, 16, 8, 8, 12, 12, 17, 13, 40, 16, 32, 22, 8, 39, 8, 35, 12, 8, 13, 11, 13, 10, 94, 16, 16, 7, 22, 13, 17, 16, 12, 100, 9, 13, 13, 42, 54, 12, 16, 27, 24, 17, 37, 28, 17, 24, 42, 8, 100, 8, 20, 16, 24, 10, 33, 10, 43, 9, 12, 22, 12, 100, 23, 22, 16, 16, 35, 16, 12, 44, 62, 8, 13, 10, 40, 24, 27, 31, 17, 7, 36, 38, 12, 7, 12, 32, 7, 12, 35, 7, 13, 100, 13, 16, 26, 13, 16, 16, 16, 19, 16, 50, 7, 22, 62, 48, 22, 10, 26, 17, 28, 53, 16, 13, 23, 16, 10, 21, 18, 12, 48, 7, 18, 18, 8, 33, 10, 22, 25, 22, 22, 12, 12, 13, 16, 16, 44, 32, 17, 19, 21, 23, 80, 31, 9, 13, 28, 8, 32, 39, 39, 67, 22, 16, 50, 17, 13, 22, 16, 25, 12, 31, 12, 14], "policy_player_1_reward": [1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 0.0, -2.0, -2.0, -2.0, 1.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, -2.0, 2.0, 1.0, 2.0, 0.0, -1.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 0.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, -1.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, -2.0, 1.0, -1.0, -2.0, -2.0, 1.0, 2.0, 1.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0], "policy_player_2_reward": [-1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -1.0, -1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 0.0, 2.0, 2.0, 2.0, -1.0, -1.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -2.0, 0.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -1.0, -2.0, 0.0, 1.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 0.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -1.0, 1.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -1.0, -1.0, 1.0, 2.0, 1.0, 2.0, -1.0, 1.0, 2.0, 2.0, -1.0, -2.0, -1.0, 1.0, 1.0, 1.0, -1.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.603864811962619, "mean_inference_ms": 1.7904661901447765, "mean_action_processing_ms": 0.1738929915720848, "mean_env_wait_ms": 0.11712015475088569, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014909635107201266, "ViewRequirementAgentConnector_ms": 0.20545038832239357}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 111997, "num_agent_steps_trained": 111997, "num_env_steps_sampled": 112000, "num_env_steps_trained": 112000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 251.6291982160879, "num_env_steps_trained_throughput_per_sec": 251.6291982160879, "timesteps_total": 112000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 111997, "timers": {"training_iteration_time_ms": 16659.944, "sample_time_ms": 3783.357, "learn_time_ms": 12866.356, "learn_throughput": 310.888, "synch_weights_time_ms": 9.578}, "counters": {"num_env_steps_sampled": 112000, "num_env_steps_trained": 112000, "num_agent_steps_sampled": 111997, "num_agent_steps_trained": 111997}, "done": false, "episodes_total": 5168, "training_iteration": 28, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-30-31", "timestamp": 1711733431, "time_this_iter_s": 19.542060613632202, "time_total_s": 584.1775665283203, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B0AF80>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3FC70>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1B0AEF0>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 584.1775665283203, "iterations_since_restore": 28, "perf": {"cpu_util_percent": 10.55925925925926, "ram_util_percent": 89.4222222222222}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.325, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"player_1": -2.0, "random": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "random": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.5638297872340425, "random": -0.895, "player_2": 1.1886792452830188}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [20, 7, 13, 17, 8, 18, 16, 26, 40, 30, 37, 10, 34, 21, 15, 11, 32, 13, 20, 10, 30, 11, 11, 11, 29, 15, 13, 27, 9, 15, 11, 30, 18, 13, 15, 9, 30, 9, 27, 30, 7, 25, 16, 13, 13, 38, 13, 29, 11, 12, 24, 11, 23, 25, 13, 32, 8, 34, 17, 17, 16, 26, 13, 17, 13, 15, 17, 17, 15, 7, 16, 34, 9, 15, 9, 10, 13, 41, 11, 25, 7, 17, 10, 23, 13, 13, 19, 35, 8, 9, 18, 24, 9, 13, 23, 34, 10, 12, 39, 11, 16, 20, 17, 7, 15, 10, 16, 7, 12, 16, 22, 11, 10, 13, 17, 17, 24, 17, 14, 14, 23, 17, 23, 17, 13, 8, 16, 27, 16, 8, 9, 18, 11, 27, 7, 13, 8, 16, 12, 13, 8, 13, 23, 9, 22, 10, 7, 13, 7, 28, 9, 15, 9, 40, 25, 24, 28, 14, 19, 24, 15, 14, 18, 13, 16, 22, 18, 18, 15, 15, 22, 22, 10, 33, 19, 27, 19, 11, 34, 10, 7, 18, 9, 23, 11, 21, 16, 13, 8, 20, 30, 7, 13, 9, 16, 11, 17, 21, 31, 21], "policy_player_1_reward": [1.0, 2.0, 1.0, 1.0, 2.0, 1.0, -1.0, 1.0, 2.0, 1.0, -2.0, -2.0, 1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 1.0, 2.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0], "policy_random_reward": [-1.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -1.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -1.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -1.0], "policy_player_2_reward": [2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6085744801721401, "mean_inference_ms": 0.9221941728975994, "mean_action_processing_ms": 0.13877946278445022, "mean_env_wait_ms": 0.09057427627916521, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008752942085266113, "ViewRequirementAgentConnector_ms": 0.24441474676132202}, "player_1_winrate": 0.6595744680851063, "player_2_winrate": 0.8207547169811321, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.15118191242218, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2267207257449626, "policy_loss": -0.015206577139421522, "vf_loss": 1.2419001637647549, "vf_explained_var": 0.3477842086305221, "kl": 0.008684057201372664, "entropy": 0.3217608434769014, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 126.6875, "num_grad_updates_lifetime": 13980.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.059062105913957, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.100555476670464, "policy_loss": -0.016980642025009728, "vf_loss": 1.1174520301322142, "vf_explained_var": 0.39726875349879265, "kl": 0.006726906639180472, "entropy": 0.25471207648515704, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 123.3125, "num_grad_updates_lifetime": 13680.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 116000, "num_env_steps_trained": 116000, "num_agent_steps_sampled": 115997, "num_agent_steps_trained": 115997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.808641975308642, "episode_media": {}, "episodes_this_iter": 162, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.5679012345679012, "player_2": -0.5679012345679012}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [58, 13, 58, 32, 12, 38, 15, 45, 12, 8, 16, 10, 13, 12, 18, 12, 43, 24, 18, 22, 20, 17, 23, 9, 35, 36, 12, 10, 16, 70, 16, 12, 12, 10, 94, 24, 13, 13, 7, 17, 22, 22, 15, 7, 12, 18, 34, 7, 12, 10, 8, 34, 32, 12, 35, 13, 12, 35, 82, 14, 17, 40, 13, 67, 38, 12, 8, 17, 8, 16, 16, 7, 12, 100, 19, 13, 12, 35, 16, 10, 23, 29, 8, 16, 17, 30, 32, 13, 13, 13, 16, 58, 17, 12, 8, 18, 22, 12, 13, 100, 86, 7, 10, 54, 16, 12, 12, 47, 35, 16, 16, 92, 9, 30, 10, 16, 28, 16, 7, 35, 40, 12, 16, 37, 14, 100, 16, 7, 12, 12, 100, 12, 18, 31, 16, 13, 9, 20, 10, 13, 50, 22, 12, 19, 10, 60, 15, 22, 14, 12, 12, 43, 54, 22, 8, 16, 16, 79, 100, 10, 17, 17], "policy_player_1_reward": [1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 0.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 0.0, 2.0, -2.0, 2.0, 2.0, 0.0, 1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 0.0, 2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 0.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 0.0, -2.0, 2.0, -2.0, -2.0, 0.0, -1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 0.0, -2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.600505357664722, "mean_inference_ms": 1.7792913496122496, "mean_action_processing_ms": 0.17330045838595898, "mean_env_wait_ms": 0.11656691060243062, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008251784760275004, "ViewRequirementAgentConnector_ms": 0.17571706830719372}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 24.808641975308642, "episodes_this_iter": 162, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.5679012345679012, "player_2": -0.5679012345679012}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [58, 13, 58, 32, 12, 38, 15, 45, 12, 8, 16, 10, 13, 12, 18, 12, 43, 24, 18, 22, 20, 17, 23, 9, 35, 36, 12, 10, 16, 70, 16, 12, 12, 10, 94, 24, 13, 13, 7, 17, 22, 22, 15, 7, 12, 18, 34, 7, 12, 10, 8, 34, 32, 12, 35, 13, 12, 35, 82, 14, 17, 40, 13, 67, 38, 12, 8, 17, 8, 16, 16, 7, 12, 100, 19, 13, 12, 35, 16, 10, 23, 29, 8, 16, 17, 30, 32, 13, 13, 13, 16, 58, 17, 12, 8, 18, 22, 12, 13, 100, 86, 7, 10, 54, 16, 12, 12, 47, 35, 16, 16, 92, 9, 30, 10, 16, 28, 16, 7, 35, 40, 12, 16, 37, 14, 100, 16, 7, 12, 12, 100, 12, 18, 31, 16, 13, 9, 20, 10, 13, 50, 22, 12, 19, 10, 60, 15, 22, 14, 12, 12, 43, 54, 22, 8, 16, 16, 79, 100, 10, 17, 17], "policy_player_1_reward": [1.0, -2.0, 1.0, 1.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, -2.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 2.0, -2.0, 1.0, 2.0, -1.0, 2.0, -2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, 2.0, 0.0, -2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 0.0, 1.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 0.0, 2.0, -2.0, 2.0, 2.0, 0.0, 1.0, 1.0, -1.0, 2.0, -2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -1.0, 0.0, 2.0, -1.0, -2.0], "policy_player_2_reward": [-1.0, 2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, 1.0, 1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, 2.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 2.0, -2.0, 0.0, 2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 0.0, -1.0, 2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 0.0, -2.0, 2.0, -2.0, -2.0, 0.0, -1.0, -1.0, 1.0, -2.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 0.0, -2.0, 1.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.600505357664722, "mean_inference_ms": 1.7792913496122496, "mean_action_processing_ms": 0.17330045838595898, "mean_env_wait_ms": 0.11656691060243062, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.008251784760275004, "ViewRequirementAgentConnector_ms": 0.17571706830719372}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 115997, "num_agent_steps_trained": 115997, "num_env_steps_sampled": 116000, "num_env_steps_trained": 116000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 251.62736783823993, "num_env_steps_trained_throughput_per_sec": 251.62736783823993, "timesteps_total": 116000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 115997, "timers": {"training_iteration_time_ms": 16697.029, "sample_time_ms": 3726.34, "learn_time_ms": 12960.729, "learn_throughput": 308.625, "synch_weights_time_ms": 9.454}, "counters": {"num_env_steps_sampled": 116000, "num_env_steps_trained": 116000, "num_agent_steps_sampled": 115997, "num_agent_steps_trained": 115997}, "done": false, "episodes_total": 5330, "training_iteration": 29, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-30-51", "timestamp": 1711733451, "time_this_iter_s": 20.091557502746582, "time_total_s": 604.2691240310669, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1A3E050>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0B640>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3C040>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 604.2691240310669, "iterations_since_restore": 29, "perf": {"cpu_util_percent": 11.096428571428573, "ram_util_percent": 88.22142857142858}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 18.11, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -1.035, "player_2": 0.97, "player_1": 1.1}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 28, 24, 15, 24, 22, 23, 15, 30, 12, 23, 20, 30, 19, 10, 17, 15, 10, 23, 30, 22, 21, 15, 12, 13, 14, 7, 10, 15, 12, 27, 13, 17, 14, 9, 13, 7, 9, 18, 54, 12, 11, 9, 18, 7, 17, 7, 16, 25, 13, 16, 14, 7, 21, 17, 70, 31, 7, 40, 9, 24, 23, 12, 16, 31, 16, 23, 12, 18, 13, 7, 27, 10, 7, 15, 26, 29, 16, 13, 20, 13, 10, 10, 14, 8, 7, 10, 18, 9, 8, 10, 72, 13, 11, 37, 12, 22, 44, 10, 12, 16, 30, 15, 15, 23, 19, 13, 13, 16, 29, 30, 14, 19, 10, 48, 13, 12, 20, 13, 15, 16, 14, 15, 10, 18, 14, 18, 18, 18, 43, 8, 15, 16, 10, 7, 52, 27, 25, 16, 9, 7, 16, 16, 19, 9, 25, 10, 22, 21, 13, 17, 17, 11, 22, 14, 17, 15, 21, 16, 9, 13, 20, 24, 16, 32, 21, 18, 7, 7, 8, 13, 16, 17, 24, 46, 18, 15, 52, 13, 9, 12, 19, 22, 15, 17, 18, 30, 19, 25, 10, 13, 27, 20, 22, 25, 10, 8, 16, 20, 22], "policy_random_reward": [-2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, -1.0, -2.0, 1.0, 1.0, -1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -1.0, 1.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, 2.0, -2.0, -2.0, -1.0, -2.0, -1.0, 1.0, -2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 1.0, -1.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, -2.0, 1.0, 2.0, 1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, -2.0, -1.0, 1.0, 2.0, 2.0, -2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -1.0], "policy_player_1_reward": [2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, -1.0, 2.0, 2.0, -1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, -1.0, -1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 1.0, 2.0, -1.0, 1.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6094470233561051, "mean_inference_ms": 0.9262223627705539, "mean_action_processing_ms": 0.13954341960209146, "mean_env_wait_ms": 0.09073844878738174, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.01414787769317627, "ViewRequirementAgentConnector_ms": 0.26427334547042847}, "player_1_winrate": 0.8, "player_2_winrate": 0.78, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.678205643097559, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.385757614672184, "policy_loss": -0.020283481092580283, "vf_loss": 1.4060151471445959, "vf_explained_var": 0.3669909526904424, "kl": 0.00830228656615037, "entropy": 0.33077026897420486, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.4375, "num_grad_updates_lifetime": 14460.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.887281724810601, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2993637351940075, "policy_loss": -0.015844656828752097, "vf_loss": 1.3151239700615407, "vf_explained_var": 0.3982417993247509, "kl": 0.006753863920218943, "entropy": 0.24120211076612275, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 122.5625, "num_grad_updates_lifetime": 14160.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 120000, "num_env_steps_trained": 120000, "num_agent_steps_sampled": 119997, "num_agent_steps_trained": 119997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.735955056179776, "episode_media": {}, "episodes_this_iter": 178, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.25842696629213485, "player_2": -0.25842696629213485}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [60, 50, 13, 17, 16, 22, 40, 8, 13, 100, 24, 35, 8, 8, 23, 14, 8, 24, 9, 24, 13, 45, 7, 16, 16, 36, 12, 30, 16, 20, 22, 28, 12, 7, 17, 13, 8, 31, 16, 35, 35, 14, 16, 46, 37, 41, 13, 8, 30, 36, 13, 51, 8, 10, 12, 100, 14, 34, 66, 33, 23, 12, 12, 9, 25, 13, 13, 13, 12, 8, 17, 39, 42, 13, 16, 30, 35, 17, 46, 16, 16, 16, 26, 13, 22, 17, 31, 13, 31, 16, 76, 9, 91, 10, 13, 16, 17, 31, 35, 13, 89, 18, 41, 10, 17, 25, 13, 13, 9, 16, 8, 66, 13, 7, 7, 46, 16, 19, 33, 75, 13, 16, 10, 25, 8, 41, 8, 13, 12, 14, 16, 17, 16, 17, 8, 16, 13, 9, 36, 18, 17, 16, 8, 9, 19, 8, 12, 16, 37, 16, 22, 8, 34, 17, 35, 100, 16, 16, 10, 16, 16, 9, 17, 19, 12, 13, 14, 17, 17, 16, 10, 18, 16, 12, 8, 44, 26, 8], "policy_player_1_reward": [1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 0.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0], "policy_player_2_reward": [-1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 0.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 0.0, -1.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 0.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5993451464348916, "mean_inference_ms": 1.7771227027228627, "mean_action_processing_ms": 0.17360422269833828, "mean_env_wait_ms": 0.11616690803153334, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009684750203336224, "ViewRequirementAgentConnector_ms": 0.19256285067354695}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 22.735955056179776, "episodes_this_iter": 178, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.25842696629213485, "player_2": -0.25842696629213485}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [60, 50, 13, 17, 16, 22, 40, 8, 13, 100, 24, 35, 8, 8, 23, 14, 8, 24, 9, 24, 13, 45, 7, 16, 16, 36, 12, 30, 16, 20, 22, 28, 12, 7, 17, 13, 8, 31, 16, 35, 35, 14, 16, 46, 37, 41, 13, 8, 30, 36, 13, 51, 8, 10, 12, 100, 14, 34, 66, 33, 23, 12, 12, 9, 25, 13, 13, 13, 12, 8, 17, 39, 42, 13, 16, 30, 35, 17, 46, 16, 16, 16, 26, 13, 22, 17, 31, 13, 31, 16, 76, 9, 91, 10, 13, 16, 17, 31, 35, 13, 89, 18, 41, 10, 17, 25, 13, 13, 9, 16, 8, 66, 13, 7, 7, 46, 16, 19, 33, 75, 13, 16, 10, 25, 8, 41, 8, 13, 12, 14, 16, 17, 16, 17, 8, 16, 13, 9, 36, 18, 17, 16, 8, 9, 19, 8, 12, 16, 37, 16, 22, 8, 34, 17, 35, 100, 16, 16, 10, 16, 16, 9, 17, 19, 12, 13, 14, 17, 17, 16, 10, 18, 16, 12, 8, 44, 26, 8], "policy_player_1_reward": [1.0, 1.0, -2.0, -1.0, 2.0, 2.0, 1.0, 2.0, -2.0, 0.0, 2.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, -1.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -1.0, -1.0, -2.0, -1.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, -2.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, -2.0, 2.0, -2.0, 2.0, -1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0], "policy_player_2_reward": [-1.0, -1.0, 2.0, 1.0, -2.0, -2.0, -1.0, -2.0, 2.0, 0.0, -2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, 1.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 0.0, -1.0, -1.0, -1.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -1.0, 2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -1.0, 2.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 1.0, -1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, 2.0, 2.0, -1.0, -1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, 2.0, 0.0, -2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 1.0, 2.0, -2.0, 2.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5993451464348916, "mean_inference_ms": 1.7771227027228627, "mean_action_processing_ms": 0.17360422269833828, "mean_env_wait_ms": 0.11616690803153334, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.009684750203336224, "ViewRequirementAgentConnector_ms": 0.19256285067354695}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 119997, "num_agent_steps_trained": 119997, "num_env_steps_sampled": 120000, "num_env_steps_trained": 120000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 252.23567809314508, "num_env_steps_trained_throughput_per_sec": 252.23567809314508, "timesteps_total": 120000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 119997, "timers": {"training_iteration_time_ms": 16620.326, "sample_time_ms": 3711.928, "learn_time_ms": 12898.379, "learn_throughput": 310.116, "synch_weights_time_ms": 9.508}, "counters": {"num_env_steps_sampled": 120000, "num_env_steps_trained": 120000, "num_agent_steps_sampled": 119997, "num_agent_steps_trained": 119997}, "done": false, "episodes_total": 5508, "training_iteration": 30, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-31-12", "timestamp": 1711733472, "time_this_iter_s": 20.741843700408936, "time_total_s": 625.0109677314758, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B0BBE0>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1A3DC60>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1A3CD30>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 625.0109677314758, "iterations_since_restore": 30, "perf": {"cpu_util_percent": 9.7, "ram_util_percent": 89.28666666666668}}
{"evaluation": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 17.93, "episode_media": {}, "episodes_this_iter": 200, "policy_reward_min": {"random": -2.0, "player_2": -2.0, "player_1": -2.0}, "policy_reward_max": {"random": 2.0, "player_2": 2.0, "player_1": 2.0}, "policy_reward_mean": {"random": -0.99, "player_2": 1.0198019801980198, "player_1": 0.9595959595959596}, "custom_metrics": {}, "hist_stats": {"episode_lengths": [7, 11, 7, 10, 15, 13, 19, 11, 7, 27, 14, 47, 10, 20, 33, 18, 7, 8, 17, 23, 13, 8, 21, 19, 13, 37, 24, 36, 12, 14, 30, 30, 11, 9, 25, 20, 18, 31, 12, 30, 16, 34, 26, 9, 46, 13, 22, 14, 14, 13, 15, 19, 13, 22, 13, 20, 18, 9, 25, 9, 13, 11, 11, 9, 22, 28, 17, 26, 11, 7, 24, 11, 18, 9, 12, 23, 13, 9, 19, 19, 18, 12, 24, 18, 18, 8, 8, 22, 13, 12, 8, 8, 17, 23, 12, 17, 9, 16, 7, 14, 19, 29, 13, 11, 13, 19, 7, 12, 18, 21, 13, 7, 9, 20, 7, 10, 19, 32, 7, 35, 13, 7, 13, 12, 14, 64, 17, 28, 8, 19, 22, 22, 23, 72, 15, 17, 26, 14, 25, 15, 24, 14, 18, 48, 40, 23, 12, 31, 52, 14, 7, 13, 12, 16, 19, 16, 10, 12, 10, 14, 13, 33, 18, 19, 18, 21, 22, 13, 16, 35, 42, 7, 7, 12, 12, 12, 23, 14, 19, 24, 13, 24, 15, 16, 16, 19, 18, 18, 17, 7, 30, 13, 12, 22, 7, 17, 14, 11, 18, 24], "policy_random_reward": [-2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, 2.0, -2.0, -2.0, -2.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -2.0, -2.0, 1.0, -2.0, -1.0, -2.0, -1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 1.0, -2.0, -2.0, 2.0, 1.0, -1.0, 1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -1.0, -2.0, 2.0, -2.0, 1.0, 1.0, 1.0, 2.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, 2.0, -1.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 2.0, -1.0, -1.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, 2.0, -1.0, -2.0, -2.0, -1.0, -1.0, -1.0, -2.0, -1.0, -2.0, -1.0, -2.0, 1.0, -2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, -2.0, 1.0, -1.0, -2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, -2.0, -1.0, -2.0, -2.0, -2.0, 2.0, 2.0, -2.0, -2.0, -1.0], "policy_player_2_reward": [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, -2.0, -2.0, 2.0, -2.0, 2.0, -1.0, 1.0, 2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, -1.0, -2.0, 1.0, 1.0, 2.0, 1.0, -2.0, 1.0, 2.0, 1.0, 1.0, -1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 1.0, 2.0, 1.0, 2.0, -2.0, 2.0], "policy_player_1_reward": [2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, -1.0, 2.0, 1.0, -1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -1.0, -2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, -2.0, -1.0, -1.0, -2.0, -2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 1.0, 2.0, 2.0, 1.0, -2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 2.0, 1.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6107219778229847, "mean_inference_ms": 0.9285156283536621, "mean_action_processing_ms": 0.14024058723999644, "mean_env_wait_ms": 0.09130379901048859, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014152228832244873, "ViewRequirementAgentConnector_ms": 0.25440794229507446}, "player_1_winrate": 0.7575757575757576, "player_2_winrate": 0.7722772277227723, "strg_rewards": [], "num_healthy_workers": 2, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0}, "custom_metrics": {}, "episode_media": {}, "info": {"learner": {"player_1": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.597224231561025, "cur_kl_coeff": 0.0031250000000000006, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.3392775394022465, "policy_loss": -0.018837228684181657, "vf_loss": 1.3580875178178151, "vf_explained_var": 0.31280359203616775, "kl": 0.008719649269533702, "entropy": 0.3166480690551301, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 127.0, "num_grad_updates_lifetime": 14940.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}, "player_2": {"learner_stats": {"allreduce_latency": 0.0, "grad_gnorm": 4.75639310280482, "cur_kl_coeff": 0.012500000000000002, "cur_lr": 5.0000000000000016e-05, "total_loss": 1.2252589051922163, "policy_loss": -0.01266367592907045, "vf_loss": 1.2378698983540137, "vf_explained_var": 0.3639563423891862, "kl": 0.004214619895740251, "entropy": 0.2446386833054324, "entropy_coeff": 0.0}, "model": {}, "custom_metrics": {}, "num_agent_steps_trained": 123.0, "num_grad_updates_lifetime": 14640.5, "diff_num_grad_updates_vs_sampler_policy": 239.5}}, "num_env_steps_sampled": 124000, "num_env_steps_trained": 124000, "num_agent_steps_sampled": 123997, "num_agent_steps_trained": 123997}, "sampler_results": {"episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.471337579617835, "episode_media": {}, "episodes_this_iter": 157, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.29936305732484075, "player_2": -0.29936305732484075}, "custom_metrics": {}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 58, 16, 12, 38, 25, 13, 12, 28, 22, 19, 34, 22, 27, 16, 16, 16, 64, 16, 13, 22, 16, 100, 100, 16, 33, 47, 16, 17, 12, 12, 82, 7, 24, 13, 16, 11, 16, 16, 92, 56, 9, 34, 90, 10, 23, 12, 17, 18, 12, 36, 30, 13, 16, 30, 30, 16, 11, 17, 17, 13, 18, 86, 13, 12, 18, 17, 16, 10, 12, 30, 13, 17, 27, 13, 22, 9, 33, 40, 12, 40, 29, 34, 19, 89, 18, 13, 18, 95, 45, 20, 100, 18, 30, 9, 43, 13, 56, 17, 66, 12, 16, 10, 27, 27, 10, 68, 13, 12, 17, 24, 19, 16, 18, 12, 16, 29, 19, 13, 31, 23, 17, 16, 8, 13, 13, 24, 36, 13, 13, 10, 10, 52, 13, 17, 16, 33, 13, 13, 22, 15, 10, 24, 16, 13, 13, 14, 40, 8, 12, 7, 16, 41, 16, 13, 13, 100], "policy_player_1_reward": [-2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 0.0, 0.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 0.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 0.0], "policy_player_2_reward": [2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 0.0, 0.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 0.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6006099141906324, "mean_inference_ms": 1.782375758475459, "mean_action_processing_ms": 0.1740276312017952, "mean_env_wait_ms": 0.11687909274850339, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014574740343033129, "ViewRequirementAgentConnector_ms": 0.23424147040980636}}, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "episode_reward_mean": 0.0, "episode_len_mean": 25.471337579617835, "episodes_this_iter": 157, "policy_reward_min": {"player_1": -2.0, "player_2": -2.0}, "policy_reward_max": {"player_1": 2.0, "player_2": 2.0}, "policy_reward_mean": {"player_1": 0.29936305732484075, "player_2": -0.29936305732484075}, "hist_stats": {"episode_reward": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "episode_lengths": [9, 58, 16, 12, 38, 25, 13, 12, 28, 22, 19, 34, 22, 27, 16, 16, 16, 64, 16, 13, 22, 16, 100, 100, 16, 33, 47, 16, 17, 12, 12, 82, 7, 24, 13, 16, 11, 16, 16, 92, 56, 9, 34, 90, 10, 23, 12, 17, 18, 12, 36, 30, 13, 16, 30, 30, 16, 11, 17, 17, 13, 18, 86, 13, 12, 18, 17, 16, 10, 12, 30, 13, 17, 27, 13, 22, 9, 33, 40, 12, 40, 29, 34, 19, 89, 18, 13, 18, 95, 45, 20, 100, 18, 30, 9, 43, 13, 56, 17, 66, 12, 16, 10, 27, 27, 10, 68, 13, 12, 17, 24, 19, 16, 18, 12, 16, 29, 19, 13, 31, 23, 17, 16, 8, 13, 13, 24, 36, 13, 13, 10, 10, 52, 13, 17, 16, 33, 13, 13, 22, 15, 10, 24, 16, 13, 13, 14, 40, 8, 12, 7, 16, 41, 16, 13, 13, 100], "policy_player_1_reward": [-2.0, 2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 2.0, -2.0, 2.0, 2.0, 0.0, 0.0, 2.0, -1.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 2.0, 2.0, -1.0, 2.0, -1.0, 2.0, 2.0, 1.0, 1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 1.0, 1.0, -2.0, 2.0, 2.0, -1.0, 2.0, 2.0, 2.0, 1.0, -2.0, -1.0, -1.0, -2.0, 2.0, -2.0, -1.0, 2.0, 2.0, 1.0, -2.0, 2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, -1.0, 1.0, 0.0, 2.0, 2.0, -2.0, -1.0, -1.0, 1.0, -2.0, 1.0, 2.0, 2.0, 2.0, -1.0, -1.0, 2.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -1.0, -1.0, 2.0, 2.0, -2.0, -2.0, 2.0, 1.0, -2.0, -2.0, 2.0, 2.0, 1.0, -2.0, -2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, 1.0, 2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, 0.0], "policy_player_2_reward": [2.0, -2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -2.0, 2.0, -1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -1.0, -2.0, 2.0, -2.0, -2.0, 0.0, 0.0, -2.0, 1.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, -1.0, 2.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -2.0, -2.0, 1.0, -2.0, 1.0, -2.0, -2.0, -1.0, -1.0, 2.0, -2.0, -1.0, -1.0, -2.0, 2.0, 1.0, 1.0, 2.0, -1.0, -1.0, 2.0, -2.0, -2.0, 1.0, -2.0, -2.0, -2.0, -1.0, 2.0, 1.0, 1.0, 2.0, -2.0, 2.0, 1.0, -2.0, -2.0, -1.0, 2.0, -2.0, 2.0, 1.0, -2.0, 2.0, -2.0, 1.0, 1.0, -1.0, 0.0, -2.0, -2.0, 2.0, 1.0, 1.0, -1.0, 2.0, -1.0, -2.0, -2.0, -2.0, 1.0, 1.0, -2.0, -2.0, 2.0, -2.0, 1.0, -2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, -2.0, -2.0, 2.0, 2.0, -2.0, -1.0, 2.0, 2.0, -2.0, -2.0, -1.0, 2.0, 2.0, -2.0, 2.0, 2.0, 2.0, -2.0, 1.0, -2.0, -1.0, -2.0, 2.0, 2.0, -2.0, -2.0, -2.0, -2.0, 2.0, -1.0, 2.0, -2.0, 2.0, 2.0, 0.0]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6006099141906324, "mean_inference_ms": 1.782375758475459, "mean_action_processing_ms": 0.1740276312017952, "mean_env_wait_ms": 0.11687909274850339, "mean_env_render_ms": 0.0}, "num_faulty_episodes": 0, "connector_metrics": {"StateBufferConnector_ms": 0.014574740343033129, "ViewRequirementAgentConnector_ms": 0.23424147040980636}, "num_healthy_workers": 3, "num_in_flight_async_reqs": 0, "num_remote_worker_restarts": 0, "num_agent_steps_sampled": 123997, "num_agent_steps_trained": 123997, "num_env_steps_sampled": 124000, "num_env_steps_trained": 124000, "num_env_steps_sampled_this_iter": 4000, "num_env_steps_trained_this_iter": 4000, "num_env_steps_sampled_throughput_per_sec": 232.78633342449305, "num_env_steps_trained_throughput_per_sec": 232.78633342449305, "timesteps_total": 124000, "num_steps_trained_this_iter": 4000, "agent_timesteps_total": 123997, "timers": {"training_iteration_time_ms": 16615.761, "sample_time_ms": 3758.359, "learn_time_ms": 12847.522, "learn_throughput": 311.344, "synch_weights_time_ms": 9.574}, "counters": {"num_env_steps_sampled": 124000, "num_env_steps_trained": 124000, "num_agent_steps_sampled": 123997, "num_agent_steps_trained": 123997}, "done": false, "episodes_total": 5665, "training_iteration": 31, "trial_id": "9ca8f_00000", "date": "2024-03-29_17-31-34", "timestamp": 1711733494, "time_this_iter_s": 21.949310302734375, "time_total_s": 646.9602780342102, "pid": 1756, "hostname": "DESKTOP-RNN7NJG", "node_ip": "127.0.0.1", "config": {"extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "num_cpus_per_worker": 2, "num_gpus_per_worker": 0, "_fake_gpus": false, "num_learner_workers": 0, "num_gpus_per_learner_worker": 0, "num_cpus_per_learner_worker": 1, "local_gpu_idx": 0, "custom_resources_per_worker": {}, "placement_strategy": "PACK", "eager_tracing": true, "eager_max_retraces": 20, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "torch_compile_learner": false, "torch_compile_learner_what_to_compile": "forward_train", "torch_compile_learner_dynamo_backend": "inductor", "torch_compile_learner_dynamo_mode": null, "torch_compile_worker": false, "torch_compile_worker_dynamo_backend": "onnxrt", "torch_compile_worker_dynamo_mode": null, "env": "Coup", "env_config": {"action_space": "Discrete(13)", "observation_space": "MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13])"}, "observation_space": null, "action_space": null, "env_task_fn": null, "render_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "disable_env_checking": false, "auto_wrap_old_gym_envs": true, "action_mask_key": "action_mask", "_is_atari": null, "env_runner_cls": null, "num_envs_per_worker": 1, "enable_connectors": true, "_env_to_module_connector": null, "_module_to_env_connector": null, "add_default_connectors_to_env_to_module_pipeline": true, "add_default_connectors_to_module_to_env_pipeline": true, "episode_lookback_horizon": 1, "rollout_fragment_length": "auto", "batch_mode": "truncate_episodes", "validate_workers_after_construction": true, "compress_observations": false, "sampler_perf_stats_ema_coef": null, "sample_async": -1, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "enable_tf1_exec_eagerly": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "preprocessor_pref": "deepmind", "observation_filter": "NoFilter", "update_worker_filter_stats": true, "use_worker_filter_stats": true, "gamma": 0.99, "lr": 5e-05, "grad_clip": null, "grad_clip_by": "global_norm", "train_batch_size": 4000, "train_batch_size_per_learner": null, "model": {"_disable_preprocessor_api": true, "_disable_action_flattening": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "fcnet_weights_initializer": null, "fcnet_weights_initializer_config": null, "fcnet_bias_initializer": null, "fcnet_bias_initializer_config": null, "conv_filters": null, "conv_activation": "relu", "conv_kernel_initializer": null, "conv_kernel_initializer_config": null, "conv_bias_initializer": null, "conv_bias_initializer_config": null, "conv_transpose_kernel_initializer": null, "conv_transpose_kernel_initializer_config": null, "conv_transpose_bias_initializer": null, "conv_transpose_bias_initializer_config": null, "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "post_fcnet_weights_initializer": null, "post_fcnet_weights_initializer_config": null, "post_fcnet_bias_initializer": null, "post_fcnet_bias_initializer_config": null, "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "lstm_weights_initializer": null, "lstm_weights_initializer_config": null, "lstm_bias_initializer": null, "lstm_bias_initializer_config": null, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "am_model", "custom_model_config": {}, "custom_action_dist": null, "custom_preprocessor": null, "encoder_latent_dim": null, "always_check_shapes": false, "lstm_use_prev_action_reward": -1, "_use_default_native_models": -1}, "_learner_connector": null, "add_default_connectors_to_learner_pipeline": true, "optimizer": {}, "max_requests_in_flight_per_sampler_worker": 2, "_learner_class": null, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "algorithm_config_overrides_per_module": {}, "_per_module_overrides": {}, "count_steps_by": "env_steps", "policy_map_capacity": 100, "policy_mapping_fn": "<function <lambda> at 0x00000170C1B09A20>", "policies_to_train": null, "policy_states_are_swappable": false, "observation_fn": null, "input_config": {}, "actions_in_input_normalized": false, "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_config": {}, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "offline_sampling": false, "evaluation_interval": 1, "evaluation_duration": 10, "evaluation_duration_unit": "episodes", "evaluation_sample_timeout_s": 180.0, "evaluation_parallel_to_training": false, "evaluation_config": {"multiagent": {"policy_mapping_fn": "<function policy_mapping_fn at 0x00000170C1B0BD90>"}}, "off_policy_estimation_methods": {}, "ope_split_batch_by_episode": true, "evaluation_num_workers": 2, "custom_async_evaluation_function": null, "always_attach_evaluation_results": false, "enable_async_evaluation": false, "in_evaluation": false, "sync_filters_on_rollout_workers_timeout_s": 60.0, "keep_per_episode_custom_metrics": false, "metrics_episode_collection_timeout_s": 60.0, "metrics_num_episodes_for_smoothing": 100, "min_time_s_per_iteration": null, "min_train_timesteps_per_iteration": 0, "min_sample_timesteps_per_iteration": 0, "export_native_model_files": false, "checkpoint_trainable_policies_only": false, "logger_creator": null, "logger_config": null, "log_level": "WARN", "log_sys_usage": true, "fake_sampler": false, "seed": null, "ignore_worker_failures": false, "recreate_failed_workers": false, "max_num_worker_restarts": 1000, "delay_between_worker_restarts_s": 60.0, "restart_failed_sub_environments": false, "num_consecutive_worker_failures_tolerance": 100, "worker_health_probe_timeout_s": 60, "worker_restore_timeout_s": 1800, "_rl_module_spec": null, "_AlgorithmConfig__prior_exploration_config": null, "_enable_new_api_stack": false, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": true, "_disable_action_flattening": false, "_disable_initialize_loss_from_dummy_batch": false, "simple_optimizer": true, "policy_map_cache": -1, "worker_cls": -1, "synchronize_filters": -1, "replay_sequence_length": null, "_disable_execution_plan_api": -1, "lr_schedule": null, "use_critic": true, "use_gae": true, "use_kl_loss": true, "kl_coeff": 0.2, "kl_target": 0.01, "sgd_minibatch_size": 128, "mini_batch_size_per_learner": null, "num_sgd_iter": 30, "shuffle_sequences": true, "vf_loss_coeff": 1.0, "entropy_coeff": 0.0, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "vf_share_layers": -1, "__stdout_file__": null, "__stderr_file__": null, "lambda": 1.0, "input": "sampler", "policies": {"player_1": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "player_2": [null, "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}], "random": ["<class '__main__.RandomPolicyActionMask'>", "Dict('action_mask': MultiBinary(13), 'observations': MultiDiscrete([ 5  5  2  2 14  6  6 14 13 13 13 13 13 13 13 13 13 13]))", "Discrete(13)", {}]}, "callbacks": "<class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>", "create_env_on_driver": false, "custom_eval_function": "<function custom_eval_function at 0x00000170C1BBEA70>", "framework": "torch", "num_cpus_for_driver": 1, "num_workers": 3}, "time_since_restore": 646.9602780342102, "iterations_since_restore": 31, "perf": {"cpu_util_percent": 12.043333333333331, "ram_util_percent": 83.85000000000001}}
