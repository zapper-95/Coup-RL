Failure # 1 (occurred at 2024-03-24_23-10-05)
The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=28092, ip=127.0.0.1, actor_id=b1cebddedb245f5056ccea9701000000, repr=PPO)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\evaluation\worker_set.py", line 227, in _setup
    self.add_workers(
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\evaluation\worker_set.py", line 593, in add_workers
    raise result.get()
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\actor_manager.py", line 481, in __fetch_result
    result = ray.get(r)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\_private\auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\_private\worker.py", line 2549, in get
    raise value
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=24696, ip=127.0.0.1, actor_id=1c795ffcdc3648e098de3ada01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028606828A00>)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 338, in check_multiagent_environments
    raise ValueError(error)
ValueError: The observation collected from env.reset was not contained within your env's observation space. Its possible that there was a typemismatch (for example observations of np.float32 and a space ofnp.float64 observations), or that one of the sub-observations wasout of bounds

 reset_obs: {'player_1': {'observations': array([ 2,  4,  1,  1,  1,  2,  5,  5, 10, 10, 10, 10, 10, 10, 10, 10, 10,
       10]), 'action_mask': array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], dtype=int8)}}

 env.observation_space_sample(): {'player_1': OrderedDict([('action_mask', array([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], dtype=int8)), ('observations', array([0, 3, 0, 0, 6, 4, 2, 9, 8, 8, 2, 0, 0], dtype=int64))]), 'player_2': OrderedDict([('action_mask', array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int8)), ('observations', array([ 4,  3,  0,  0, 10,  2,  4,  0,  2,  3,  0,  3,  0], dtype=int64))])}

 

During handling of the above exception, another exception occurred:

[36mray::RolloutWorker.__init__()[39m (pid=24696, ip=127.0.0.1, actor_id=1c795ffcdc3648e098de3ada01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000028606828A00>)
  File "python\ray\_raylet.pyx", line 1610, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1704, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1616, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1556, in ray._raylet.execute_task.function_executor
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 404, in __init__
    check_env(self.env, self.config)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 96, in check_env
    raise ValueError(
ValueError: Traceback (most recent call last):
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 338, in check_multiagent_environments
    raise ValueError(error)
ValueError: The observation collected from env.reset was not contained within your env's observation space. Its possible that there was a typemismatch (for example observations of np.float32 and a space ofnp.float64 observations), or that one of the sub-observations wasout of bounds

 reset_obs: {'player_1': {'observations': array([ 2,  4,  1,  1,  1,  2,  5,  5, 10, 10, 10, 10, 10, 10, 10, 10, 10,
       10]), 'action_mask': array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], dtype=int8)}}

 env.observation_space_sample(): {'player_1': OrderedDict([('action_mask', array([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], dtype=int8)), ('observations', array([0, 3, 0, 0, 6, 4, 2, 9, 8, 8, 2, 0, 0], dtype=int64))]), 'player_2': OrderedDict([('action_mask', array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int8)), ('observations', array([ 4,  3,  0,  0, 10,  2,  4,  0,  2,  3,  0,  3,  0], dtype=int64))])}

 

The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).

During handling of the above exception, another exception occurred:

[36mray::PPO.__init__()[39m (pid=28092, ip=127.0.0.1, actor_id=b1cebddedb245f5056ccea9701000000, repr=PPO)
  File "python\ray\_raylet.pyx", line 1610, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1704, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1616, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1556, in ray._raylet.execute_task.function_executor
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\_private\function_manager.py", line 726, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 517, in __init__
    super().__init__(
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\tune\trainable\trainable.py", line 185, in __init__
    self.setup(copy.deepcopy(self.config))
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\algorithms\algorithm.py", line 639, in setup
    self.workers = WorkerSet(
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\evaluation\worker_set.py", line 179, in __init__
    raise e.args[0].args[2]
ValueError: Traceback (most recent call last):
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "C:\Users\josep\Documents\Coup-RL\env\lib\site-packages\ray\rllib\utils\pre_checks\env.py", line 338, in check_multiagent_environments
    raise ValueError(error)
ValueError: The observation collected from env.reset was not contained within your env's observation space. Its possible that there was a typemismatch (for example observations of np.float32 and a space ofnp.float64 observations), or that one of the sub-observations wasout of bounds

 reset_obs: {'player_1': {'observations': array([ 2,  4,  1,  1,  1,  2,  5,  5, 10, 10, 10, 10, 10, 10, 10, 10, 10,
       10]), 'action_mask': array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0], dtype=int8)}}

 env.observation_space_sample(): {'player_1': OrderedDict([('action_mask', array([1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], dtype=int8)), ('observations', array([0, 3, 0, 0, 6, 4, 2, 9, 8, 8, 2, 0, 0], dtype=int64))]), 'player_2': OrderedDict([('action_mask', array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int8)), ('observations', array([ 4,  3,  0,  0, 10,  2,  4,  0,  2,  3,  0,  3,  0], dtype=int64))])}

 

The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).
